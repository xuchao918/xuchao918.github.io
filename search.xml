<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用cert-manager实现Ingress https]]></title>
    <url>%2F2019%2F03%2F14%2F%E4%BD%BF%E7%94%A8cert-manager%E5%AE%9E%E7%8E%B0Ingress-https%2F</url>
    <content type="text"><![CDATA[什么是https超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。 为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS，为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 什么是cert-managercert-manager 是一个云原生证书管理开源项目，用于在 Kubernetes 集群中提供 HTTPS 证书并自动续期，支持 Let’s Encrypt, HashiCorp Vault 这些免费证书的签发。在Kubernetes集群中，我们可以通过 Kubernetes Ingress 和 Let’s Encrypt 实现外部服务的自动化 HTTPS。 在Kubernetes集群中使用 HTTPS 协议，需要一个证书管理器、一个证书自动签发服务，主要通过 Ingress 来发布 HTTPS 服务，因此需要Ingress Controller并进行配置，启用 HTTPS 及其路由。 环境依赖 本文使用 Helm 安装，所以请确保 Helm 已安装，且版本最好&gt;2.10 集群必须已经装有 Ingress Controller 外部客户端配置hosts，IP 指向 Ingress Controller 对外暴露的地址（如果IP是公网地址并做了域名解析，则无需配置） 部署cert-manager使用helm安装cert-manager安装 CustomResourceDefinition资源1kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/00-crds.yaml 创建cert-manager namespace1kubectl create namespace cert-manager 标记cert-Manager命名空间以禁用资源验证1kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true 添加 Jetstack Helm repository1helm repo add jetstack https://charts.jetstack.io 更新本地Helm chart repository1helm repo update 使用Helm chart安装cert-manager12345helm install \ --name cert-manager \ --namespace cert-manager \ --version v0.7.0 \ jetstack/cert-manager 查看cert-manager部署结果12345# kubectl get pods --namespace cert-managerNAME READY STATUS RESTARTS AGEcert-manager-5658b7db79-824lt 1/1 Running 0 11hcert-manager-cainjector-768fd47f68-ls6zh 1/1 Running 0 11hcert-manager-webhook-5b4bc6b547-8qk2v 1/1 Running 0 11h 创建ClusterIssuer我们需要先创建一个签发机构，cert-manager 给我们提供了 Issuer 和 ClusterIssuer 这两种用于创建签发机构的自定义资源对象，Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书，这里以 ClusterIssuer 为例创建一个签发机构：123456789101112# cat issuer.yaml apiVersion: certmanager.k8s.io/v1alpha1kind: ClusterIssuermetadata: name: letsencrypt-prodspec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: xxxx@126.com privateKeySecretRef: name: letsencrypt-prod http01: &#123;&#125; 说明： metadata.name 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它 spec.acme.email 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期 spec.acme.server 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行 spec.acme.privateKeySecretRef 指示此签发机构的私钥将要存储到哪个 Secret 对象中，名称不重要 spec.acme.http01 这里指示签发机构使用 HTTP-01 的方式进行 acme 协议 (还可以用 DNS 方式，acme 协议的目的是证明这台机器和域名都是属于你的，然后才准许给你颁发证书) 部署1# kubectl apply -f issuer.yaml 查看clusterissuer创建结果123# kubectl get clusterissuerNAME AGEletsencrypt-prod 11m 创建Certificate有了签发机构，接下来我们就可以生成免费证书了，cert-manager 给我们提供了 Certificate 这个用于生成证书的自定义资源对象，它必须局限在某一个 namespace 下，证书最终会在这个 namespace 下以 Secret 的资源对象存储，创建一个 Certificate 对象：12345678910111213141516171819# cat cert.yaml apiVersion: certmanager.k8s.io/v1alpha1kind: Certificatemetadata: name: dashboard-imroc-io namespace: istio-systemspec: secretName: dashboard-imroc-io issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - istio.kiali.com acme: config: - http01: ingressClass: traefik domains: - istio.kiali.com 说明： spec.secretName 指示证书最终存到哪个 Secret 中 spec.issuerRef.kind 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局 spec.issuerRef.name 我们创建的签发机构的名称 (ClusterIssuer.metadata.name) spec.dnsNames 指示该证书的可以用于哪些域名 spec.acme.config.http01.ingressClass 使用 HTTP-01 方式校验该域名和机器时，cert-manager 会尝试创建Ingress 对象来实现该校验，如果指定该值，会给创建的 Ingress 加上 kubernetes.io/ingress.class 这个 annotation，如果我们的 Ingress Controller 是 Nginx Ingress Controller，指定这个字段可以让创建的 Ingress 被 Nginx Ingress Controller 处理。 spec.acme.config.http01.domains 指示该证书的可以用于哪些域名 执行部署命令1# kubectl apply -f cert.yaml 查看certificate创建结果123# kubectl get certificate -n istio-systemNAME AGEdashboard-imroc-io 54s 执行上述步骤，如有问题，可使用如下命令排查原因12# kubectl describe -n istio-system certificate dashboard-imroc-io# kubectl describe clusterissuer letsencrypt-prod 查看生成的secret 结果12# kubectl get secret -n istio-system | grep dashboard-imroc-iodashboard-imroc-io kubernetes.io/tls 3 2m32s 测试Ingress使用https创建一个nginx1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# cat test-nginx.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 1 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 443---apiVersion: v1kind: Servicemetadata: name: my-nginx labels: app: my-nginxspec: ports: - port: 443 protocol: TCP name: https selector: run: my-nginx---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: my-nginx annotations: kubernetes.io/ingress.class: "traefik" kubernetes.io/tls-acme: "true" certmanager.k8s.io/cluster-issuer: "letsencrypt-prod"spec: rules: - host: test.nginx.com http: paths: - backend: serviceName: my-nginx servicePort: 443 path: / tls: - secretName: dashboard-imroc-io hosts: - test.nginx.com 需要注意的是上面我们添加的两个annotations非常重要，这个将告诉 Cert Manager 去生成证书，然后由于我们这里要使用 HTTPS，所以我们需要添加一个 tls 证书，而证书就是通过k8sui-tls这个 Secret 对象来提供的，要注意的是这个 Secret 对象并不是我们手动创建的，而是 Cert Manager 自动创建的证书对应的对应。然后直接创建这个资源对象即可：1# kubectl apply -f test-nginx.yaml 创建完成后隔一会儿我们可以看到会多出现一个随机名称的 Ingress 对象，这个 Ingress 对象就是用来专门验证证书的：123# kubectl get ingress -n istio-systemNAME HOSTS ADDRESS PORTS AGEcm-acme-http-solver-z562f test.nginx.com 80 62s 我们可以通过 Traefik 的 Dashboard 可以观察到这一变化，验证成功后，这个 Ingress 对象也自动删除了： 这个时候我们可以去describe下我们的 Ingress 对象，查看有无报错1# kubectl describe ingress my-nginx 同样我们可以去查看 Cert manager 的 Pod 日志信息：1# kubectl logs -f cert-manager-5658b7db79-824lt --namespace cert-manager 最后，我们来打开浏览器使用https访问服务 到这里我们就完成了使用Let’s Encrypt实现Kubernetes Ingress自动化 HTTPS。 参考资料项目地址：https://github.com/jetstack/cert-manager文档地址：https://cert-manager.readthedocs.io]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>cert-manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Kubernetes生产架构]]></title>
    <url>%2F2019%2F03%2F02%2F%E6%B5%85%E8%B0%88Kubernetes%E7%94%9F%E4%BA%A7%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[注意本文，只是笔者针对Kubernetes生产环境运行的一些关于架构设计和实现方案的总结，内容很粗糙，同时也会不断完善。 首先，我们来看下Kubernetes生产架构，如下图所示。 在该架构中，我将其分为了四层，如下： Client层：即Kubernetes集群外部用户、客户端等； 服务访问层：即由Traefik ingress实现服务发现、负载均衡和路由规则定义等； 业务应用层：即基于Kubernetes平台构建和运行企业业务应用，如CI/CD持续集成、微服务项目、监控告警和日志管理、私有镜像仓库等服务； 基础设施层：即由Kubernetes容器管理平台和Ceph数据持久化存储等系统组成的基础设施服务。 下面，我们分别来谈谈各层的具体实现方案。 基础设施层Kubernetes平台 部署管理：Kubernetes平台除了直接使用公有云如阿里云、AWS等云服务提供商的K8s服务外，我们还可以自己部署和管理等，如使用Kubespray工具。 网络通信：在容器和容器之间、容器和主机网络方面，可以使用Calico或Flannel等方案。 HA高可用：Kubernetes节点分为Master和Node两种类型节点，前者负责运行集群相关的控制管理服务，而后者负责运行Pod容器。在多Node节点模式下，由于Kubernetes Pod具有天然的容灾冗余HA高可用实现，因此，我们并不需要关心Node节点的HA高可用，而只需关心Master节点的HA即可，Master节点的HA高可用，通过多Master节点+HAProxy方案实现即可。 Docker和操作系统：在生产环境中，Docker和操作系统版本应当使用较新的release版本。并且，主机操作系统应当做一定程度的优化配置，如关闭swap内存交换分区，预留一定的CPU核数和内存资源给宿主机使用等。 Ceph/NFS数据存储Kubernetes平台的数据持久化存储，可以使用Ceph、NFS等存储方案。其中，Ceph适用于有其技术背景或大容量存储的公司；而NFS适用于存储容量相对较小，存储技术较低的公司。 业务应用层 镜像管理：使用Harbor私有镜像仓库服务； 日志管理：使用Elasticsearch、Fluentd 和 Kibana技术栈； 监控告警管理：使用Cadvisor、Prometheus和Grafana技术栈； 微服务架构：使用Service Mesh服务网格中的Istio实现方案； Devops：使用Gitlab、Jenkins等持续集成工具； 服务访问层K8s集群内的服务访问、负载均衡和路由规则定义使用Traefik Ingress实现，此外，可以根据自身业务需求，选择是否在Ingress前面部署Nginix软负载均衡或硬件负载均衡。 —未完待续]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Istio微服务架构部署和使用]]></title>
    <url>%2F2019%2F03%2F01%2FKubernetes-Istio%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是IstioIstio是Service Mesh（服务网格）的主流实现方案。该方案降低了与微服务架构相关的复杂性，并提供了负载均衡、服务发现、流量管理、断路器、监控、故障注入和智能路由等功能特性。 其中，Sidecar模式是一种将应用功能从应用本身剥离出来作为单独进程的方式。该模式允许我们向应用无侵入添加多种功能，避免了为满足第三方组件需求而向应用添加额外的配置代码。从某种意义上来说，服务对于网络是无感知的，只知道所附加的sidecar代理，它将网络依赖抽象成了Sidecar。 在Service Mesh中，我们需要了解Data Plane和Control Plane两个概念： Data Plane：作用是处理网格内服务间的通信，并完成服务发现、负载均衡、流量管理、健康检查等功能； Control Plane：作用是管理和配置策略用于路由流量，同时也在运行期执行策略。 Istio核心组件 Envoy：Istio 使用 Envoy调解服务网格中所有服务的入站和出站流量。属于数据平面。 Mixer：负责在服务网格上执行访问控制和使用策略，以及收集从Envoy和其他服务自动监控到的数据。 Pilot：为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。属于控制平面。 Citadel：提供访问控制和用户身份认证功能。 Istio可视化管理组件 Vistio：用于近乎实时地监控应用程序和集群之间的网络流量。可以参考：https://www.yangcs.net/posts/vistio-visualize-your-istio-mesh-using-netflixs-vizceral/ Kiali：提供可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。可以参考：https://jimmysong.io/istio-handbook/setup/istio-observability-tool-kiali.html jaeger：用于展示istio微服务调用链关系，以及微服务工作状态监测。注意，在生产环境中，你应当使用Elasticsearch或cassandra持久化存储jaeger数据。可以参考：https://blog.csdn.net/ywq935/article/details/80599297https://mathspanda.github.io/2018/09/19/jaeger-deploy/https://blog.frognew.com/2017/12/opentracing-jaeger-3.html 其中，Kiali、Jaeger、prometheus、grafana管理工具，将和Istio一并部署。 使用Helm部署Istio依赖环境Helm &gt; 2.10Kubernetes &gt; 1.9 下载并解压缩istio的发布包123wget https://github.com/istio/istio/releases/download/1.0.6/istio-1.0.6-linux.tar.gztar -zxvf istio-1.0.6-linux.tar.gzcd istio-1.0.6 Istio的Chart在istio-1.0.6/install/kubernetes/helm目录中，这个Chart包含了下面的代码文件1234# tree install/kubernetes/helm/istio............31 directories, 139 files 如果安装的Helm版本高于2.10，就不再需要手动使用kubectl安装Istio的CRD。反之，则需要执行如下命令安装1kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml 查看安装的CRD1kubectl get CustomResourceDefinition 通过各个组件在vaule file的enabled flag启用或禁用，下面创建名称为istio.yaml的vaule file，将几个默认禁用的组件也启用12345678tracing: enabled: trueservicegraph: enabled: truekiali: enabled: truegrafana: enabled: true 首先，创建名称为kiali的secret。12# echo -n 'admin' | base64YWRtaW4= 12# echo -n '1f2d1e2e67df' | base64MWYyZDFlMmU2N2Rm 12345678910111213# cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Secretmetadata: name: kiali namespace: istio-system labels: app: kialitype: Opaquedata: username: YWRtaW4= passphrase: MWYyZDFlMmU2N2RmEOF 执行helm安装命令1helm install install/kubernetes/helm/istio --name istio --namespace istio-system -f istio.yaml 安装完成后确认各个组件的Pod正常运行12345678910111213141516# kubectl get pod -n istio-systemNAME READY STATUS RESTARTS AGEgrafana-59b8896965-5f9j2 1/1 Running 0 23mistio-citadel-6f444d9999-s9jrc 1/1 Running 0 23mistio-egressgateway-6d79447874-ssbc4 1/1 Running 0 23mistio-galley-685bb48846-mvf5w 1/1 Running 0 23mistio-grafana-post-install-6m256 0/1 Completed 0 23mistio-ingressgateway-5b64fffc9f-mrl9t 1/1 Running 0 23mistio-pilot-8645f5655b-k6fcz 2/2 Running 0 23mistio-policy-547d64b8d7-6dgkp 2/2 Running 0 23mistio-sidecar-injector-5d8dd9448d-zfdsb 1/1 Running 0 23mistio-telemetry-c5488fc49-qwwcv 2/2 Running 0 23mistio-tracing-6b994895fd-4vjfx 1/1 Running 0 23mkiali-5f9ffff7cf-jqk8p 1/1 Running 0 23mprometheus-76b7745b64-xjzmm 1/1 Running 0 23mservicegraph-cb9b94c-mlhjm 1/1 Running 0 23m Istio 以一个项目的形式部署到 Kubernetes 集群中。我们可以看到，部署好的 pods 中，除了有 istio-citadel、istio-egressgateway、istio-ingressgateway、istio-pilot 等 Istio 本身的功能组件，还集成了微服务相关的监控工具，如：grafana、jaeger-agent、kiali、prometheus。正是这些功能丰富且强大的监控工具，帮助 Istio实现了微服务的可视化管理。 运行示例Bookinfo您可以部署自己的应用或者示例应用程序如 Bookinfo。 注意：应用程序必须使用 HTTP/1.1 或 HTTP/2.0 协议来传递 HTTP 流量，因为 HTTP/1.0 已经不再支持。 如果运行 Pod 的 namespace 被标记为 istio-injection=enabled 的话，Istio-Initializer 会向应用程序的 Pod 中自动注入 Envoy 容器：12kubectl label namespace &lt;namespace&gt; istio-injection=enabledkubectl create -n &lt;namespace&gt; -f &lt;your-app-spec&gt;.yaml 如果您没有安装 Istio-initializer-injector 的话，您必须使用 istioctl kube-inject 命令在部署应用之前向应用程序的 Pod 中手动注入 Envoy 容器：1kubectl create -f &lt;(istioctl kube-inject -f &lt;your-app-spec&gt;.yaml) Bookinfo 应用由四个单独的微服务构成，用来演示多种 Istio 特性，包含： productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。 运行示例bookinfo，并开启Sidecar自动注入。1234# kubectl label namespace default istio-injection=enabled# kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml# kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml# kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml 访问productpagehttp://172.16.0.180:31380/productpage 31380端口可以通过命令获取1kubectl -n istio-system get svc istio-ingressgateway -o jsonpath='&#123;.spec.ports[0].nodePort&#125;' 使用Ingress暴露管理服务完成Istio的安装后，可以看到安装的组件除了Istio架构中的数据平面和控制平面的各个核心组件，还部署了Prometheus、Grafana、Jaeger、Kiali等辅助组件。 在云原生生态中，我们已经对这些组件很熟悉了。 Prometheus：监控系统，收集Istio的监控数据 Grafana：监控信息的图表展现，Istio部署的Grafana为我们内置了各个组件相关的Dashboard Jaeger：分布式跟踪系统，Istio中集成Jaeger可以对基于Istio的微服务实现调用链跟踪、依赖分析，为性能优化和故障排查提供支持 kiali：kiali作为Istio的可视化管理工具，可以认为是Istio的UI，可以展现服务的网络拓扑、服务的容错情况(超时、重试、短路等)、分布式跟踪等 这些辅助组件都有自己的web界面，这里我们使用ingress的方式将这些组件暴露到集群外，以便在集群外部访问。Istio支持使用自带的istio-ingressgateway将服务暴露到集群外部，这个和Kubernetes中暴露Ingress Controller类似，有很多种方式，如NodePort，LoadBalancer，或直接开启hostNetwork: true等等。为了便于统一管理K8s集群中的服务暴露，笔者更倾向使用Traefik Ingress。 使用Ingress暴露istio服务编写ingress yaml文件，如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# cat istio-ingress.yaml ---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jaeger-query namespace: istio-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: istio.jaeger-query.com http: paths: - path: / backend: serviceName: jaeger-query servicePort: 16686---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus namespace: istio-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: istio.prometheus.com http: paths: - path: / backend: serviceName: prometheus servicePort: 9090---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: grafana namespace: istio-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: istio.grafana.com http: paths: - path: / backend: serviceName: grafana servicePort: 3000---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kiali namespace: istio-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: istio.kiali.com http: paths: - path: / backend: serviceName: kiali servicePort: 20001 执行部署命令1kubectl apply -f istio-ingress.yaml 外部客户端，配置hosts地址解析，如下1234172.16.0.180 istio.prometheus.com172.16.0.180 istio.jaeger-query.com172.16.0.180 istio.grafana.com172.16.0.180 istio.kiali.com 访问jaeger浏览器访问Jaeger之前可以多次刷新productpage页面以便产生访问请求等。选择productpage.default可以查看整个调用链。使用istio.jaeger-query.com域名访问，结果展示： 访问kiali使用域名istio.kiali.com访问kiali页面。用户名admin，密码1f2d1e2e67df。 访问prometheus使用域名istio.prometheus.com访问prometheus页面。 访问grafana使用域名istio.grafana.com访问prometheus页面。 Istio 对 Pod 和服务的要求要成为服务网格的一部分，Kubernetes 集群中的 Pod 和服务必须满足以下几个要求： 需要给端口正确命名：服务端口必须进行命名。端口名称只允许是&lt;协议&gt;[-&lt;后缀&gt;-]模式； Pod必须关联到 Kubernetes服务：如果一个 Pod 属于多个服务，这些服务不能再同一端口上使用不同协议，例如 HTTP 和 TCP。 Deployment应带有app以及version标签：每个 Deployment 都应该有一个有意义的 app 标签和一个用于标识 Deployment 版本的 version 标签。Istio 会用 app 和 version 标签来给监控指标数据加入上下文信息。 总结本文实践了使用istio官方提供的helm chart在Kubernetes上部署Istio 1.0.6的过程，并使用traefik ingress将Istio集成的Prometheus、Grafana、Jaeger、Kiali等辅助组件暴露到集群外部，并对进入集群的流量进行管理。 在生产环境中，如果是基于公有云，如阿里云、AWS等运行Istio，建议Ingress的IP地址使用ELB地址；如果是自建的平台，则建议使用HAproxy+Keepalived提供的VIP地址，作为Ingress的IP地址，实现高可用。 如果Ingress服务，需要暴露在公网，应当使用CA认证机构颁发的证书https化（如使用cert-manager）。此外建议使用NFS、Ceph等方案实现Istio监控以及微服务应用的数据持久化存储。 istio参考资料https://istio.io/zh/docs/https://jimmysong.io/istio-handbook/http://www.servicemesher.com/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes traefik ingress使用]]></title>
    <url>%2F2019%2F03%2F01%2FKubernetes-traefik-ingress%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Traefik介绍简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。 Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。 traefik整体架构如下所示。 Traefik主要特性详解 自动熔断在集群中，当某一个服务大量出现请求错误，或者请求响应时间过久，或者返回500+错误状态码时，我们希望可以主动剔除该服务，也就是不在将请求转发到该服务上，而这一个过程是自动完成，不需要人工执行。Traefik 通过配置很容易就能帮我们实现，Traefik 可以通过定义策略来主动熔断服务。 NetworkErrorRatio() &gt; 0.5：监测服务错误率达到50%时，熔断。 LatencyAtQuantileMS(50.0) &gt; 50：监测延时大于50ms时，熔断。 ResponseCodeRatio(500, 600, 0, 600) &gt; 0.5：监测返回状态码为[500-600]在[0-600]区间占比超过50%时，熔断。 负载均衡策略Traefik 提供两种负载均衡策略支持。一种是 wrr（加权轮训调度算法），一种是 drr（动态加权循环调度算法）。 wrr是默认的负载均衡策略，新创建的 service 权重都是一样为1，这样的话，请求会平均分给每个服务，但是这样很多时候会出现资源分配不均衡的问题，比如由于集群中每个机器配置不一样，而且服务消耗不一样，假设 A 资源使用率已经很高，而 B 属于空闲状态，如果还是均摊到每个服务的话，会加重 A 的负荷，这时候因该有一种策略能够主动识别并分担更多流量到 B 才对。 drr 就更加智能，它是一种动态加权轮训调度方式，它会记录一段时间内转发到 A 的请求数，跟转发到 B 的请求数对比，转发数量多，说明处理速度快，响应时间快。如果 A 处理请求速度比 B 快，那么就会调整 A 的权重，接下来的一段时间，就会转发更多请求给 A，相应的 B 的转发就少一些。整个过程都在不断的调整权重，实现请求的合理分配，从而达到资源使用最大化。 部署Traefik ingress创建ingress-rbac.yaml，将用于service account验证。123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: ingress namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: ingresssubjects: - kind: ServiceAccount name: ingress namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 创建Depeloyment部署traefik，如文件名为deployment.yaml123456789101112131415161718192021222324252627282930313233343536373839apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: traefik-ingress-lb namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationGracePeriodSeconds: 60 hostNetwork: true restartPolicy: Always serviceAccountName: ingress containers: - image: traefik name: traefik-ingress-lb resources: limits: cpu: 1000m memory: 3000Mi requests: cpu: 500m memory: 2000Mi ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8580 hostPort: 8580 args: - --web - --web.address=:8580 - --kubernetes 注意我们这里用的是Deploy类型，没有限定该pod运行在哪个主机上。Traefik的端口是8580。 编写Traefik UI的ingress部署文件，如文件名为traefik-ui.yaml1234567891011121314151617181920212223242526apiVersion: v1kind: Servicemetadata: name: traefik-web-uispec: selector: k8s-app: traefik-ingress-lb ports: - name: web port: 80 targetPort: 8580---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-systemspec: rules: - host: traefik.ui.com http: paths: - path: / backend: serviceName: traefik-web-ui servicePort: web backend中要配置default namespace中启动的service名字。path就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。 配置完成后就可以启动treafik ingress了。1234567# kubectl create -f .deployment.extensions/traefik-ingress-lb createdserviceaccount/ingress createdclusterrolebinding.rbac.authorization.k8s.io/ingress createdservice/traefik-web-ui createdingress.extensions/traefik-web-ui createdingress.extensions/traefik-ingress created 查看是否部署成功123456# kubectl get pods -n kube-system | grep traefiktraefik-ingress-lb-57786f6c44-cwr96 1/1 Running 0 2m27s# kubectl get ingress -o wide --all-namespaces NAMESPACE NAME HOSTS ADDRESS PORTS AGEkube-system traefik-web-ui traefik.ui.com 80 12s 在客户端配置hosts域名解析，如下1172.16.0.180 traefik.ui.com 172.16.0.180是traefik pod所在的K8s节点，通过域名traefik.ui.com访问将可以看到dashboard。 左侧黄色部分列出的是所有的rule，右侧绿色部分是所有的backend。 测试下面模拟部署一个程序，以Nginx 为例，并使用drr动态轮训加权策略。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# cat nginx-deployment.yaml apiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx-podspec: replicas: 2 template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.15.5 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service annotations: traefik.ingress.kubernetes.io/load-balancer-method: drrspec: template: metadata: labels: name: nginx-service namespace: defaultspec: selector: app: nginx-pod ports: - port: 80 targetPort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingress annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: k8s.nginx.com http: paths: - backend: serviceName: nginx-service servicePort: 80 创建nginx12kubectl apply -f nginx-deployment.yaml kubectl get pods 同样，修改客户端的hosts文件。在其中加入12172.16.0.180 traefik.ui.com172.16.0.180 k8s.nginx.com 所有访问这些地址的流量都会发送给172.16.0.180这台主机，就是我们启动traefik的主机。Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。 在外部客户端，访问nginx应用的ingress地址http://k8s.nginx.com/ 在K8s集群节点上访问测试1# curl -x 172.16.0.180:80 http://k8s.nginx.com ingress配置同域名不同路径代理web应用很多使用我们不想配置太多的域名来区别应用，使用同域名分路径的方式来区别应用就简洁方便很多。ingress也提供了相关的配置。 假设两个应用tomcat-test1和tomcat-test2。这里可配置域名tomcat.test.k8s，通过路径test1、test2来分别代理两个tomcat应用。其中，分路径配置需添加配置：traefik.frontend.rule.type: PathPrefixStrip1234567891011121314151617181920212223# vi ingress-tomcat.yaml ---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: tomcat-test-web namespace: default annotations: kubernetes.io/ingress.class: traefik traefik.frontend.rule.type: PathPrefixStripspec: rules: - host: tomcat.test.k8s http: paths: - path: /test1/ backend: serviceName: tomcat-test1 servicePort: 8080 - path: /test2/ backend: serviceName: tomcat-test2 servicePort: 8080 12# kubectl apply -f ingress-tomcat.yaml # kubectl describe ingress tomcat-test-web 从describe信息和ui界面上可以看到，tomcat.test.k8s分别有了/test1/和/test2/的域名代理以及相对应的后端，可以修改hosts测试一下分路径是否生效：1172.16.0.180 tomcat.test.k8s 测试访问 在线修改资源配置如果需要在线修改部署的资源，如deployment、service或ingress等，可以使用kubectl edit命令。如修改deployment。12# kubectl get deploy# kubectl edit deploy nginx-pod 或者直接修改yaml文件后，执行kubectl apply命令更新即可生效。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>traefik</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Helm使用]]></title>
    <url>%2F2019%2F03%2F01%2FKubernetes-Helm%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是Helm在没使用helm之前，向kubernetes部署应用，我们要依次部署deployment、svc等，步骤较繁琐。况且随着很多项目微服务化，复杂的应用在容器中部署以及管理显得较为复杂，helm通过打包的方式，支持发布的版本管理和控制，很大程度上简化了Kubernetes应用的部署和管理. Helm本质就是让K8s的应用管理（Deployment,Service等)可配置，能动态生成。通过动态生成K8s资源清单文件（deployment.yaml，service.yaml）。然后调用Kubectl自动执行K8s资源部署。 Helm和charts的主要作用 应用程序封装 版本管理 依赖检查 便于应用程序分发 组成helm客户端 制作、拉取、查找和验证 Chart 安装服务端Tiller 指示服务端Tiller做事，比如根据chart创建一个Release helm服务端 tiller 安装在Kubernetes集群内的一个应用， 用来执行客户端发来的命令，管理Release 安装Helm客户端下载期望的版本1# wget -c https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz 解压1# tar -zxvf helm-v2.12.3-linux-amd64.tar.gz 在解压后的文件夹中找到Helm命令所在位置, 将它移动到期望位置1# mv linux-amd64/helm /usr/local/bin/helm 安装helm服务端tiller执行命令1# helm init --upgrade --tiller-image registry.cn-beijing.aliyuncs.com/minminmsn/tiller:v2.12.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 确认服务端tiller12# kubectl get pods -n kube-system |grep tillertiller-deploy-99dcdbf5f-ddwbg 1/1 Running 0 4m28s 确认客户端和服务端连接成功。如果只显示了客户端版本，说明没有连上服务端。 它会自动去K8s上kube-system命名空间下查找是否有Tiller的Pod在运行。123# helm versionClient: &amp;version.Version&#123;SemVer:"v2.12.3", GitCommit:"20adb27c7c5868466912eebdf6664e7390ebe710", GitTreeState:"clean"&#125;Server: &amp;version.Version&#123;SemVer:"v2.12.3", GitCommit:"20adb27c7c5868466912eebdf6664e7390ebe710", GitTreeState:"clean"&#125; 查找helm仓库中可用chart，如查找mysql1# helm search mysql 默认安装的 tiller 权限很小，我们执行下面的脚本给它加最大权限，这样方便我们可以用 helm 部署应用到任意 namespace 下:12345# kubectl create serviceaccount --namespace=kube-system tiller# kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller# kubectl patch deploy --namespace=kube-system tiller-deploy -p '&#123;"spec":&#123;"template":&#123;"spec":&#123;"serviceAccount":"tiller"&#125;&#125;&#125;&#125;' 创建自己的chart我们创建一个名为mychart的chart，看一看chart的文件结构。123456789101112$ helm create mongodb$ tree mongodbmongodb├── Chart.yaml #Chart本身的版本和配置信息├── charts #依赖的chart├── templates #配置模板目录│ ├── NOTES.txt #helm提示信息│ ├── _helpers.tpl #用于修改kubernetes objcet配置的模板| |—— ingress.yaml #用于服务暴露或访问│ ├── deployment.yaml #kubernetes Deployment object│ └── service.yaml #kubernetes Serivce└── values.yaml #kubernetes object configuration 如此，我们可以按需编辑自动生成的yaml文件。templates目录下的yaml文件中的变量是从values.yaml文件中获取的。 使用命令验证chart配置。该输出中包含了模板的变量配置与最终渲染的yaml文件。1# helm install --dry-run --debug mongodb 部署到kubernetes在mongodb目录下执行下面的命令将nginx部署到kubernetes集群上。1# helm install . 查看部署的release123# helm listNAME REVISION UPDATED STATUS CHART NAMESPACEgarish-gopher 1 Wed Feb 27 11:24:36 2019 DEPLOYED mongodb-0.1.0 default 打包分享我们可以修改Chart.yaml中的helm chart配置信息，然后使用下列命令将chart打包成一个压缩文件。1# helm package . 打包出mongodb-0.1.0.tgz文件。 依赖我们可以在requirements.yaml中定义应用所依赖的chart，例如定义对mariadb的依赖：1234dependencies:- name: mariadb version: 0.6.0 repository: https://kubernetes-charts.storage.googleapis.com 使用helm lint .命令可以检查依赖和模板配置是否正确。 使用第三方chat库添加fabric8库1# helm repo add fabric8 https://fabric8.io/helm 搜索fabric8提供的工具（主要就是fabric8-platform工具包，包含了CI、CD的全套工具）1# helm search fabric8 我们在前面打包的chart可以通过HTTP server的方式提供。123# helm serve --address 172.16.0.180:8879Regenerating index. This may take a moment.Now serving you on 172.16.0.180:8879 访问http://172.16.0.180:8879可以看到刚刚安装的chart。 解决本地chart依赖打开另外一个终端，在本地当前chart配置的目录下，将该repo加入到repo list中。1# helm repo add local http://172.16.0.180:8879 在浏览器中访问http://172.16.0.180:8879，可以看到所有本地的chart。 然后下载依赖到本地。1# helm dependency update 这样所有的chart都会下载到本地的charts目录下。 设置helm命令自动补全为了方便helm命令的使用，helm提供了自动补全功能，如果使用bash请执行：1# source &lt;(helm completion bash) Example: 安装Mysql执行命令123# helm repo update# helm install stable/mysqlReleased smiling-penguin 每次安装都有一个Release被创建， 所以一个Chart可以在同一个集群中被安装多次，每一个都是独立管理和升级的。其中 stable/mysql是Chart名， smiling-penguid 是Release名，后面管理Release时都是用的这个名字。 在使用一个Chart前，查看它的默认配置，然后使用配置文件覆盖它的默认设置1# helm inspect values stable/mariadb 使用一个YAML文件，内含要覆盖Chart的配置值。12# echo '&#123;mariadbUser: user0, mariadbDatabase: user0db&#125;' &gt; config.yaml# helm install -f config.yaml stable/mariadb values.yaml中的值可以被部署release时用到的参数–values YAML_FILE_PATH 或 –set key1=value1, key2=value2覆盖掉， 比如1# helm install --set image.tag='latest' . 优先级： –set设置的值会覆盖–value设置的值， –value设置的值会覆盖 values.yaml中定义的值 helm一些常用命令Charts:helm search 查找可用的Chartshelm inspect 查看指定Chart的基本信息helm install 根据指定的Chart 部署一个Release到K8shelm create 创建自己的Charthelm package 打包Chart，一般是一个压缩包文件 release:helm list 列出已经部署的Releasehelm delete [RELEASE] 删除一个Release. 并没有物理删除， 出于审计需要，历史可查。helm status [RELEASE] 查看指定的Release信息，即使使用helm delete命令删除的Release.helm upgrade 升级某个Releasehelm rollback [RELEASE] [REVISION] 回滚Release到指定发布版本helm get values [RELEASE] 查看Release的配置文件值helm ls –deleted 列出已经删除的Release repo:helm repo listhelm repo add [RepoName] [RepoUrl]helm repo update]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用client-go自定义开发Kubernetes]]></title>
    <url>%2F2019%2F02%2F13%2F%E4%BD%BF%E7%94%A8client-go%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%80%E5%8F%91Kubernetes%2F</url>
    <content type="text"><![CDATA[1. 安装client-go client-go 安装很简单，前提是本机已经安装并配置好了 Go 环境，安装之前，我们需要先查看下其版本针对 k8s 版本 兼容性列表，针对自己本机安装的 k8s 版本选择对应的 client-go 版本，当然也可以默认选择最新版本，来兼容所有。 client-go 安装方式有多种，比如 go get、Godep、Glide 方式。如果我们本地没有安装 Godep 和 Glide 依赖管理工具的话，可以使用最简单的 go get 下载安装。1$ go get k8s.io/client-go/... 执行该命令将会自动将 k8s.io/client-go 下载到本机 $GOPATH，默认下载的源码中只包含了大部分依赖，并将其放在 k8s.io/client-go/vendor 路径，但是如果想成功运行的话，还需要另外两个依赖库 k8s.io/client-go/vendor 和 glog，所以还需要接着执行如下命令。1$ go get -u k8s.io/apimachinery/... 说明一下，为什么要使用 -u 参数来拉取最新的该依赖库呢？那是因为最新的 client-go 库只能保证跟最新的 apimachinery 库一起运行。 2. 在k8s集群外操作资源示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# cat main.gopackage mainimport ( "flag" "fmt" "os" "time" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/kubernetes" "k8s.io/client-go/tools/clientcmd")func main() &#123; // 配置 k8s 集群外 kubeconfig 配置文件 var kubeconfig *string kubeconfig = flag.String("kubeconfig", "/etc/kubernetes/admin.conf", "absolute path to the kubeconfig file") flag.Parse() //在 kubeconfig 中使用当前上下文环境，config 获取支持 url 和 path 方式 config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig) if err != nil &#123; panic(err.Error()) &#125; // 根据指定的 config 创建一个新的 clientset clientset, err := kubernetes.NewForConfig(config) if err != nil &#123; panic(err.Error()) &#125; for &#123; // 通过实现 clientset 的 CoreV1Interface 接口列表中的 PodsGetter 接口方法 Pods(namespace string)返回 PodInterface // PodInterface 接口拥有操作 Pod 资源的方法，例如 Create、Update、Get、List 等方法 // 注意：Pods() 方法中 namespace 不指定则获取 Cluster 所有 Pod 列表 pods, err := clientset.CoreV1().Pods("").List(metav1.ListOptions&#123;&#125;) if err != nil &#123; panic(err.Error()) &#125; fmt.Printf("There are %d pods in the k8s cluster\n", len(pods.Items)) // 获取指定 namespace 中的 Pod 列表信息 namespace := "default" pods, err = clientset.CoreV1().Pods(namespace).List(metav1.ListOptions&#123;&#125;) if err != nil &#123; panic(err) &#125; fmt.Printf("\nThere are %d pods in namespaces %s\n", len(pods.Items), namespace) for _, pod := range pods.Items &#123; fmt.Printf("Name: %s, Status: %s, CreateTime: %s\n", pod.ObjectMeta.Name, pod.Status.Phase, pod.ObjectMeta.CreationTimestamp) &#125; time.Sleep(10 * time.Second) &#125;&#125;func prettyPrint(maps map[string]interface&#123;&#125;) &#123; lens := 0 for k, _ := range maps &#123; if lens &lt;= len(k) &#123; lens = len(k) &#125; &#125; for key, values := range maps &#123; spaces := lens - len(key) v := "" for i := 0; i &lt; spaces; i++ &#123; v += " " &#125; fmt.Printf("%s: %s%v\n", key, v, values) &#125;&#125;func homeDir() string &#123; if h := os.Getenv("HOME"); h != "" &#123; return h &#125; return os.Getenv("USERPROFILE") // windows&#125; 执行程序1234567# go run main.goThere are 15 pods in the k8s clusterThere are 2 pods in namespaces defaultName: podinfo-7b8c9bc5c9-64g8k, Status: Running, CreateTime: 2019-01-10 22:40:18 +0800 CSTName: podinfo-7b8c9bc5c9-bx7ml, Status: Running, CreateTime: 2019-01-10 22:40:18 +0800 CSTThere are 15 pods in the k8s cluster 3.在k8s集群内操作资源示例 除以上方法外，还可以在 k8s 集群内运行客户端操作资源类型。既然是在 k8s 集群内运行，那么就需要将编写的代码放到镜像内，然后在 k8s 集群内以 Pod 方式运行该镜像容器。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# cat main2.gopackage mainimport ( "fmt" "time" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/kubernetes" "k8s.io/client-go/rest")func main() &#123; // 通过集群内部配置创建 k8s 配置信息，通过 KUBERNETES_SERVICE_HOST 和 KUBERNETES_SERVICE_PORT 环境变量方式获取 // 若集群使用 TLS 认证方式，则默认读取集群内部 tokenFile 和 CAFile // tokenFile = "/var/run/secrets/kubernetes.io/serviceaccount/token" // rootCAFile = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt" config, err := rest.InClusterConfig() if err != nil &#123; panic(err.Error()) &#125; // 根据指定的 config 创建一个新的 clientset clientset, err := kubernetes.NewForConfig(config) if err != nil &#123; panic(err.Error()) &#125; for &#123; // 通过实现 clientset 的 CoreV1Interface 接口列表中的 PodsGetter 接口方法 Pods(namespace string)返回 PodInterface // PodInterface 接口拥有操作 Pod 资源的方法，例如 Create、Update、Get、List 等方法 // 注意：Pods() 方法中 namespace 不指定则获取 Cluster 所有 Pod 列表 pods, err := clientset.CoreV1().Pods("").List(metav1.ListOptions&#123;&#125;) if err != nil &#123; panic(err.Error()) &#125; fmt.Printf("There are %d pods in the k8s cluster\n", len(pods.Items)) // 获取指定 namespace 中的 Pod 列表信息 namespce := "default" pods, err = clientset.CoreV1().Pods(namespce).List(metav1.ListOptions&#123;&#125;) if err != nil &#123; panic(err) &#125; fmt.Printf("\nThere are %d pods in namespaces %s\n", len(pods.Items), namespce) for _, pod := range pods.Items &#123; fmt.Printf("Name: %s, Status: %s, CreateTime: %s\n", pod.ObjectMeta.Name, pod.Status.Phase, pod.ObjectMeta.CreationTimestamp) &#125; // 获取所有的 Namespaces 列表信息 ns, err := clientset.CoreV1().Namespaces().List(metav1.ListOptions&#123;&#125;) if err != nil &#123; panic(err) &#125; nss := ns.Items fmt.Printf("\nThere are %d namespaces in cluster\n", len(nss)) for _, ns := range nss &#123; fmt.Printf("Name: %s, Status: %s, CreateTime: %s\n", ns.ObjectMeta.Name, ns.Status.Phase, ns.CreationTimestamp) &#125; time.Sleep(10 * time.Second) &#125;&#125; 该示例主要演示如何在 k8s 集群内操作 Pod 和 Namespaces 资源类型，包括获取集群所有 Pod 列表数量，获取指定 Namespace 中的 Pod 列表信息，获取集群内所有 Namespace 列表信息。这里，该方式获取 k8s 集群配置的方式跟上边方式不同，它通过集群内部创建的 k8s 配置信息，通过 KUBERNETES_SERVICE_HOST 和 KUBERNETES_SERVICE_PORT 环境变量方式获取，来跟 k8s 建立连接，进而来操作其各个资源类型。如果 k8s 开启了 TLS 认证方式，那么默认读取集群内部指定位置的 tokenFile 和 CAFile。 编译一下，看下是否通过。123# go build main2.go# lsmain2 main2.go 接下来，在同级目录创建一个 Dockerfile 文件如下123FROM debianCOPY ./main2 /optENTRYPOINT /opt/main2 构建docker镜像1234# lsDockerfile main2# docker build -t client-go/in-cluster:1.0 . 因为本机 k8s 默认开启了 RBAC 认证的，所以需要创建一个 clusterrolebinding 来赋予 default 账户 view 权限。12$ kubectl create clusterrolebinding default-view --clusterrole=view --serviceaccount=default:defaultclusterrolebinding.rbac.authorization.k8s.io "default-view" created 最后，在 Pod 中运行该镜像即可，可以使用 yaml 方式或运行 kubectl run 命令来创建。1234567891011121314# kubectl run --rm -i client-go-in-cluster-demo --image=client-go/in-cluster:1.0 --image-pull-policy=NeverThere are 3 pods in namespaces defaultName: client-go-in-cluster-demo-58d9b5bd79-7w5ds, Status: Running, CreateTime: 2019-02-13 14:25:38 +0000 UTCName: podinfo-7b8c9bc5c9-64g8k, Status: Running, CreateTime: 2019-01-10 14:40:18 +0000 UTCName: podinfo-7b8c9bc5c9-bx7ml, Status: Running, CreateTime: 2019-01-10 14:40:18 +0000 UTCThere are 5 namespaces in clusterName: custom-metrics, Status: Active, CreateTime: 2019-01-10 09:01:52 +0000 UTCName: default, Status: Active, CreateTime: 2019-01-05 09:18:02 +0000 UTCName: kube-public, Status: Active, CreateTime: 2019-01-05 09:18:02 +0000 UTCName: kube-system, Status: Active, CreateTime: 2019-01-05 09:18:02 +0000 UTCName: monitoring, Status: Active, CreateTime: 2019-01-08 15:00:41 +0000 UTCThere are 16 pods in the k8s cluster 运行正常，简单验证一下吧！12345# kubectl get pods -n defaultNAME READY STATUS RESTARTS AGEclient-go-in-cluster-demo-58d9b5bd79-7w5ds 1/1 Running 0 10mpodinfo-7b8c9bc5c9-64g8k 1/1 Running 1 33dpodinfo-7b8c9bc5c9-bx7ml 1/1 Running 1 33d 4. k8s各资源对象CRUD操作 上边演示了，在 k8s 集群内外运行客户端操作资源类型，但是仅仅是 Read 相关读取操作，接下来简单演示下如何进行 Create、Update、Delete 操作。创建 main.go 文件如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# cat main3.gopackage mainimport ( "flag" "fmt" apiv1 "k8s.io/api/core/v1" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/kubernetes" "k8s.io/client-go/tools/clientcmd")func main() &#123; // 配置 k8s 集群外 kubeconfig 配置文件 var kubeconfig *string kubeconfig = flag.String("kubeconfig", "/etc/kubernetes/admin.conf", "absolute path to the kubeconfig file") flag.Parse() //在 kubeconfig 中使用当前上下文环境，config 获取支持 url 和 path 方式 config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig) if err != nil &#123; panic(err) &#125; // 根据指定的 config 创建一个新的 clientset clientset, err := kubernetes.NewForConfig(config) if err != nil &#123; panic(err) &#125; // 通过实现 clientset 的 CoreV1Interface 接口列表中的 NamespacesGetter 接口方法 Namespaces 返回 NamespaceInterface // NamespaceInterface 接口拥有操作 Namespace 资源的方法，例如 Create、Update、Get、List 等方法 name := "client-go-test" namespacesClient := clientset.CoreV1().Namespaces() namespace := &amp;apiv1.Namespace&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: name, &#125;, Status: apiv1.NamespaceStatus&#123; Phase: apiv1.NamespaceActive, &#125;, &#125; // 创建一个新的 Namespaces fmt.Println("Creating Namespaces...") result, err := namespacesClient.Create(namespace) if err != nil &#123; panic(err) &#125; fmt.Printf("Created Namespaces %s on %s\n", result.ObjectMeta.Name, result.ObjectMeta.CreationTimestamp) // 获取指定名称的 Namespaces 信息 fmt.Println("Getting Namespaces...") result, err = namespacesClient.Get(name, metav1.GetOptions&#123;&#125;) if err != nil &#123; panic(err) &#125; fmt.Printf("Name: %s, Status: %s, selfLink: %s, uid: %s\n", result.ObjectMeta.Name, result.Status.Phase, result.ObjectMeta.SelfLink, result.ObjectMeta.UID) // 删除指定名称的 Namespaces 信息 fmt.Println("Deleting Namespaces...") deletePolicy := metav1.DeletePropagationForeground if err := namespacesClient.Delete(name, &amp;metav1.DeleteOptions&#123; PropagationPolicy: &amp;deletePolicy, &#125;); err != nil &#123; panic(err) &#125; fmt.Printf("Deleted Namespaces %s\n", name)&#125; 执行程序1234567# go run main3.goCreating Namespaces...Created Namespaces client-go-test on 2019-02-13 21:44:52 +0800 CSTGetting Namespaces...Name: client-go-test, Status: Active, selfLink: /api/v1/namespaces/client-go-test, uid: 8a2de86e-2f95-11e9-b2e0-a0369f3f0404Deleting Namespaces...Deleted Namespaces client-go-test 参考资料https://blog.csdn.net/aixiaoyang168/article/details/84752005]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译和运行Kubernetes源码]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%BC%96%E8%AF%91%E5%92%8C%E8%BF%90%E8%A1%8CKubernetes%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[为什么要编译源码 Kubernetes是一个非常棒的容器集群管理平台。通常情况下，我们并不需要修改K8s代码即可直接使用。但如果，我们在环境中发现了某个问题/缺陷，或按照特定业务需求需要修改K8s代码时，如定制Kubelet的StopContainer 逻辑、kube-scheduler的pod调度逻辑等。为了让修改生效，那么就需要编译K8s代码了。 Kubernetes源码编译，大致分为本地二进制可执行文件编译和docker镜像编译两种。由于在我们的环境中，Kubernetes是由Docker容器方式运行的。故此我们需要采用后面一种方式编译，即镜像编译。 由于Kubernetes每个组件服务的镜像Dockerfile文件是由Kubernetes源码自动生成的，因此，社区并未提供每个组件的镜像Dockerfile文件。编译本地二进制可执行文件很简单，也更直接。而docker镜像编译资料却很少，且碍于某种特殊网络原因，会导致失败。此处，将介绍如何顺利的完成k8s镜像编译。 安装依赖 安装Golang1234wget -c https://dl.google.com/go/go1.11.4.linux-amd64.tar.gz -P /opt/cd /opt/tar -C /usr/local -xzf go1.11.4.linux-amd64.tar.gz echo "export PATH=$PATH:/usr/local/go/bin" &gt;&gt; /etc/profile &amp;&amp; source /etc/profile 指定分支，下载 Kubernetes 源代码（默认$GOPATH目录为/root/go/）1234mkdir -p $GOPATH/src/k8s.iocd $GOPATH/src/k8s.iogit clone https://github.com/kubernetes/kubernetes -b release-1.13cd $GOPATH/src/k8s.io/kubernetes 本地二进制文件编译Kubernetes（方法一） 修改运行平台配置参数（可选）根据自己的运行平台（linux/amd64)修改hack/lib/golang.sh，把KUBE_SERVER_PLATFORMS，KUBE_CLIENT_PLATFORMS和KUBE_TEST_PLATFORMS中除linux/amd64以外的其他平台注释掉，以此来减少编译所用时间。 编译源码进入Kubernetes根目录下1cd kubernetes KUBE_BUILD_PLATFORMS指定目标平台，WHAT指定编译的组件，通过GOFLAGS和GOGCFLAGS传入编译时参数，如此处编译kubelet 组件。1KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS="-N -l" 如果不指定WHAT，则编译全部。 make all是在本地环境中进行编译的。 make release和make quick-release在容器中完成编译、打包成docker镜像。 编译kubelet这部分代码，也可执行make clean &amp;&amp; make WHAT=cmd/kubelet 检查编译成果编译过程较长，请耐心等待，编译后的文件在kubernetes/_output里。 或者进入cmd/kubelet (以kubelet为例子)执行go build -v命令,如果没出错,会生成可执行文件kubelet1go build -v 生成的可执行文件在当前文件夹下面12# ls cmd/kubelet/app BUILD kubelet kubelet.go OWNERS Docker镜像编译Kubernetes（方法二） 查看kube-cross的TAG版本号12# cat ./build/build-image/cross/VERSIONv1.11.4-1 查看debian_iptables_version版本号12# egrep -Rn "debian_iptables_version=" ././build/common.sh:93: local debian_iptables_version=v11.0 这里，我使用DockerHub的Auto build功能，来构建K8s镜像。自然将编译需要用到的base镜像，放在了DockerHub上（也算是为促进国内K8s源码docker编译贡献绵薄之力吧！）。1234567891011docker pull xiaoxu780/pause-amd64:3.1docker pull xiaoxu780/kube-cross:v1.11.2-1docker pull xiaoxu780/debian-base-amd64:0.4.0docker pull xiaoxu780/debian-iptables-amd64:v11.0docker pull xiaoxu780/debian-hyperkube-base-amd64:0.12.0docker tag xiaoxu780/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1docker tag xiaoxu780/kube-cross:v1.11.2-1 k8s.gcr.io/kube-cross:v1.11.4-1docker tag xiaoxu780/debian-base-amd64:0.4.0 k8s.gcr.io/debian-base-amd64:0.4.0docker tag xiaoxu780/debian-iptables-amd64:v11.0 k8s.gcr.io/debian-iptables-amd64:v11.0docker tag xiaoxu780/debian-hyperkube-base-amd64:0.12.0 k8s.gcr.io/debian-hyperkube-base-amd64:0.12.0 把build/lib/release.sh中的–pull去掉，避免构建镜像继续拉取镜像：123 "$&#123;DOCKER[@]&#125;" build --pull -q -t "$&#123;docker_image_tag&#125;" $&#123;docker_build_path&#125; &gt;/dev/null修改为: "$&#123;DOCKER[@]&#125;" build -q -t "$&#123;docker_image_tag&#125;" $&#123;docker_build_path&#125; &gt;/dev/null 编辑文件hack/lib/version.sh将KUBE_GIT_TREE_STATE=”dirty” 改为 KUBE_GIT_TREE_STATE=”clean”，确保版本号干净。 执行编译命令123# cd kubernetes# make clean# KUBE_BUILD_PLATFORMS=linux/amd64 KUBE_BUILD_CONFORMANCE=n KUBE_BUILD_HYPERKUBE=n make release-images GOFLAGS=-v GOGCFLAGS="-N -l" 其中KUBE_BUILD_PLATFORMS=linux/amd64指定目标平台为linux/amd64，GOFLAGS=-v开启verbose日志，GOGCFLAGS=”-N -l”禁止编译优化和内联，减小可执行程序大小。 编译的K8s Docker镜像以压缩包的形式发布在_output/release-tars目录中123# ls _output/release-images/amd64/cloud-controller-manager.tar kube-controller-manager.tar kube-scheduler.tarkube-apiserver.tar kube-proxy.tar 使用编译镜像 等待编译完成后，在_output/release-stage/server/linux-amd64/kubernetes/server/bin/目录下保存了编译生成的二进制可执行程序和docker镜像tar包。如导入kube-apiserver.tar镜像，并更新环境上部署的kube-apiserver镜像。12# docker load -i kube-apiserver.tar# docker tag k8s.gcr.io/kube-apiserver:v1.13.3 registry.com/kube-apiserver:v1.13.3 整个编译过程结束后，现在就可以到master节点上，修改/etc/kubernetes/manifests/kube-apiserver.yaml描述文件中的image，修改完立即生效。 参考资料：https://github.com/kubernetes/kubernetes/tree/master/build/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>源码编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何实现K8s Pod自定义指标弹性伸缩]]></title>
    <url>%2F2019%2F01%2F10%2F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0K8s-Pod%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[弹性伸缩介绍自动弹性伸缩(AutoScaling)，是Kubernetes的一大功能和亮点。在OpenStack IaaS云计算中也有类似的服务，即Senlin。即基于资源使用情况自动弹性缩容和扩容工作负载。Kubernetes的自动弹性伸缩有两个维度： Cluster Autoscaler：处理K8s集群Node节点伸缩，该功能依赖于IaaS云提供商云主机服务和资源监控服务。 Horizontal Pod Autoscaler（HPA）：处理Pod自动弹性伸缩副本集，该功能依赖于监控服务采集到的资源指标数据。 简言之，Cluster Autoscaling和Horizontal Pod Autoscaler（HPA）可用于动态调整计算能力以满足系统SLA的要求。 通常，扩/缩容都是根据内存或者CPU资源的使用率实现，但是现实中，很多时候扩/缩容的依据通常是业务监控指标。如何根据业务监控指标来进行扩/缩容，将是本文探讨的内容。 自Kubernetes 1.11版本起，K8s资源采集指标由Resource Metrics API（Metrics Server 实现）和Custom metrics api（Prometheus实现）两种API实现，传统Heapster监控被废弃。前者主要负责采集Node、Pod的核心资源数据，如内存、CPU等；而后者则主要负责自定义指标数据采集，如网卡流量，磁盘IOPS、HTTP请求数、数据库连接数等。 Custom Metrics Server介绍前面已提过，自heapster被废弃以后，所有的指标数据都从API接口中获取，由此kubernetes将资源指标分为了两种： Core metrics(核心指标)：由metrics-server提供API，即 metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。 Custom Metrics(自定义指标)：由Prometheus Adapter提供API，即 custom.metrics.k8s.io，由此可支持任意Prometheus采集到的自定义指标。 想让K8s的HPA，获取核心指标以外的其它自定义指标，则必须部署一套prometheus监控系统，让prometheus采集其它各种指标，但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(kube-state-metrics)，将prometheus的metrics数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在Master节点上的kube-apiserver中注册，以便直接通过/apis/来访问。 Custom Metrics组件介绍 node-exporter：prometheus的agent端，收集Node级别的监控数据。 prometheus：监控服务端，从node-exporter拉取数据并存储为时序数据。 kube-state-metrics： 将prometheus中可以用PromQL查询到的指标数据转换成k8s对应的数据格式，即转换成Custerom Metrics API接口格式的数据，但是它不能聚合进apiserver中的功能。 k8s-prometheus-adpater：聚合apiserver，即提供了一个apiserver（cuester-metrics-api），自定义APIServer通常都要通过Kubernetes aggregator聚合到apiserver。 Custom Metrics部署获取部署文件12# git clone https://github.com/stefanprodan/k8s-prom-hpa# cd k8s-prom-hpa/ 创建monitoring命名空间1# kubectl create -f ./namespaces.yaml 编辑prometheus部署文件123# vim ./prometheus/prometheus-dep.yamlimage: prom/prometheus:v2.1.0//修改为image: prom/prometheus:v2.5.0 将 Prometheus部署到monitoring命名空间1# kubectl create -f ./prometheus 生成由Prometheus adapter所需的TLS证书1# make certs 编辑custom-metrics-apiserve部署文件123456# vim ./custom-metrics-api/custom-metrics-apiserver-deployment.yaml- --prometheus-url=http://prometheus.monitoring.svc.cluster.local:9090/ //将该参数修改为Prometheus的 service域名地址 image: quay.io/coreos/k8s-prometheus-adapter-amd64:v0.2.0//修改为image: quay.io/coreos/k8s-prometheus-adapter-amd64:v0.4.1 部署Prometheus自定义api适配器1# kubectl create -f ./custom-metrics-api 列出由prometheus提供的自定义指标1# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq . 查看创建的namespace123456# kubectl get namespaceNAME STATUS AGEdefault Active 10dkube-public Active 10dkube-system Active 10dmonitoring Active 6d23h 查看创建的pod1234# kubectl get pods -n monitoringNAME READY STATUS RESTARTS AGEcustom-metrics-apiserver-855cbf8644-6qhmv 1/1 Running 0 2d21hprometheus-788f78d959-xs74p 1/1 Running 0 2d21h 查看创建的service1234# kubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEcustom-metrics-apiserver ClusterIP 10.233.7.116 &lt;none&gt; 443/TCP 2d21hprometheus NodePort 10.233.49.23 &lt;none&gt; 9090:30090/TCP 2d21h 到了这里，我们便可以通过 http://节点IP:30090方式访问Prometheus页面。 查看新创建的api群组123# kubectl api-versions | grep metricscustom.metrics.k8s.io/v1beta1metrics.k8s.io/v1beta1 列出由prometheus提供的自定义指标123456789# yum -y install jq# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq .&#123; "kind": "APIResourceList", "apiVersion": "v1", "groupVersion": "custom.metrics.k8s.io/v1beta1", "resources": [.............] //输出信息太多，此处省略&#125; 列示Pod 上的Prometheus 适配器所提供的缺省定制指标。1# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq . |grep "pods/" 获取monitoring命名空间中所有pod的fs_usage_bytes信息1234567891011121314151617181920212223242526272829303132# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes" | jq .&#123; "kind": "MetricValueList", "apiVersion": "custom.metrics.k8s.io/v1beta1", "metadata": &#123; "selfLink": "/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/%2A/fs_usage_bytes" &#125;, "items": [ &#123; "describedObject": &#123; "kind": "Pod", "namespace": "monitoring", "name": "custom-metrics-apiserver-855cbf8644-6qhmv", "apiVersion": "/__internal" &#125;, "metricName": "fs_usage_bytes", "timestamp": "2019-01-15T14:14:22Z", "value": "233570304" &#125;, &#123; "describedObject": &#123; "kind": "Pod", "namespace": "monitoring", "name": "prometheus-788f78d959-xs74p", "apiVersion": "/__internal" &#125;, "metricName": "fs_usage_bytes", "timestamp": "2019-01-15T14:14:22Z", "value": "36864" &#125; ]&#125; 验证创建一个使用Custom Metrics APIs的Pod，来验证自定义指标弹性伸缩功能。1# cd k8s-prom-hpa/ podinfo 应用暴露了一个自定义的度量指标：http_requests_total。Prometheus adapter（即 custom-metrics-apiserver）删除了 _total 后缀并将该指标标记为 counter metric。 在default命名空间中创建podinfo服务，podinfo应用程序暴露了一个自定义的指标，即http_requests。1# kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml 从自定义指标API获取每秒的总请求数:1# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests" | jq . m代表milli-units，例如，901m 意味着901 milli-requests （就是大约0.9个请求）。 创建一个HPA，如果请求数超过每秒10，将扩容podinfo这个Pod的副本数。1# kubectl create -f ./podinfo/podinfo-hpa-custom.yaml 过几秒钟HPA从自定义指标API取得http_requests的值：123# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpodinfo Deployment/podinfo 899m / 10 2 10 2 1m 用每秒25次请求数给podinfo服务加压安装hey12# go get -u github.com/rakyll/hey# hey -n 10000 -q 5 -c 5 http://&lt;K8S-IP&gt;:31198/healthz 几分钟后，HPA开始扩容该Pod副本数。12345678910111213# kubectl describe hpaName: podinfoNamespace: defaultReference: Deployment/podinfoMetrics: ( current / target ) "http_requests" on pods: 9059m / 10Min replicas: 2Max replicas: 10Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 2m horizontal-pod-autoscaler New size: 3; reason: pods metric http_requests above target 可看到，在当前压力测试下，Pod副本将自动扩容到三副本，但不会超过10副本这个最大值。同时三副本Pod已经满足当前负载。 负载测试结束后，HPA向下自动缩容到1个副本Pod。12345Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 5m horizontal-pod-autoscaler New size: 3; reason: pods metric http_requests above target Normal SuccessfulRescale 21s horizontal-pod-autoscaler New size: 2; reason: All metrics below target 后续custom metrics大大丰富了K8s pod弹性伸缩的能力，使K8s Pod AutoScaling从资源伸缩向业务应用伸缩方向转变成为现实。限于篇幅，后面将介绍部署Grafana+Influxdb集成Prometheus可视化查看K8s集群资源使用情况。 此外，在生产环境中，应当使用NFS、hostPath等方式持久化存储Prometheus监控数据。 参考资料：https://github.com/stefanprodan/k8s-prom-hpa]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>自动弹性伸缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化部署K8s 1.13.1]]></title>
    <url>%2F2019%2F01%2F04%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2K8s-1-13-1%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言部署Kubernetes除了手动方式外，还有诸如Kubeadm、Kubespray、Breeze、Rancher、kargo等多种自动化方式。工具没有好坏之分，能干事、效率高就行。这里，笔者仍使用Kubespray部署当前K8s最新版本（用着真的很贴身），可自动化部署HA集群、可灵活定制开发、高稳定性等。 本篇将介绍如何在不用科学上网的背景下，快速自动化部署K8s集群。那么，开始吧！ 初始化环境环境说明环境实在有限，只有一台机器，想玩HA集群也没环境啊 主机名 IP地址 角色 K8s 172.16.0.180 Master+node 环境为Centos 7系统，各节点配置hosts和hostname，如12cat /etc/hosts172.16.0.180 K8s 关闭防火墙等12sed -i 's/SELINUX=*/SELINUX=disabled/' /etc/selinux/configsystemctl disable firewalld &amp;&amp; systemctl stop firewalld Kubernetes 1.8开始要求关闭系统的Swap交换分区，方法如下1swapoff -a &amp;&amp; echo "vm.swappiness=0" &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p &amp;&amp; free –h Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，在各个Docker节点执行下面的命令：1iptables -P FORWARD ACCEPT 配置ssh key 认证。确保本机也可以 ssh 连接，否则下面部署失败12ssh-keygen -t rsa -N ""cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 更新系统内核为 4.4.x , CentOS 默认为3.10.x1234rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpmyum --enablerepo=elrepo-kernel install -y kernel-lt kernel-lt-devel grub2-set-default 0 重启系统1reboot 增加内核配置1234vim /etc/sysctl.conf# dockernet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1 使其内核配置生效1sysctl -p 安装kubespray安装 centos的epel源1yum -y install epel-release make缓存1yum clean all &amp;&amp; yum makecache 安装软件，ansible 必须 &gt;= 2.71yum install -y python-pip python34 python-netaddr python34-pip ansible git 下载源码，当前kubespray项目的master分支默认安装k8s 1.13.1版本1git clone https://github.com/kubernetes-sigs/kubespray 安装 kubespray 依赖，若无特殊说明，后续操作均在~/kubespray目录下执行12cd kubespraypip install -r requirements.txt 配置kubespray1cp -rfp inventory/sample inventory/mycluster 修改配置文件 hosts.ini。123456789101112131415161718vim inventory/mycluster/hosts.ini[all]k8s ansible_host=k8s ip=172.16.0.180[kube-master]k8s[etcd]k8s[kube-node]k8s[k8s-cluster:children]kube-masterkube-node[calico-rr] 修改配置文件all.yaml12345vim inventory/mycluster/group_vars/all/all.yml# 修改如下配置:loadbalancer_apiserver_localhost: true# 加载内核模块，否则 ceph, gfs 等无法挂载客户端kubelet_load_modules: true 默认镜像从 gcr.io/google-containers 下载，由于墙的原因不能下载。这里我将k8s 1.13.1版本所必须的镜像push到了DockerHub上，方便大家下载使用。1234567891011121314# 下载镜像(该步骤可不用执行)docker pull xiaoxu780/kube-proxy:v1.13.1docker pull xiaoxu780/kube-controller-manager:v1.13.1docker pull xiaoxu780/kube-scheduler:v1.13.1docker pull xiaoxu780/kube-apiserver:v1.13.1docker pull xiaoxu780/coredns:1.2.6docker pull xiaoxu780/cluster-proportional-autoscaler-amd64:1.3.0docker pull xiaoxu780/kubernetes-dashboard-amd64:v1.10.0docker pull xiaoxu780/etcd:3.2.24docker pull xiaoxu780/node:v3.1.3docker pull xiaoxu780/ctl:v3.1.3docker pull xiaoxu780/kube-controllers:v3.1.3docker pull xiaoxu780/cni:v3.1.3docker pull xiaoxu780/pause-amd64:3.1 修改镜像默认的repo地址，使用Calico三层网络，同时可以指定安装的k8s版本，参数为kube_version。编辑文件12vim inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.ymlkube_image_repo: "gcr.io/google-containers" //修改为kube_image_repo: "xiaoxu780" 修改配置文件main.yml，使用sed命令批量替换123sed -i 's/gcr\.io\/google_containers/xiaoxu780/g' roles/download/defaults/main.ymlsed -i 's/quay\.io\/coreos/xiaoxu780/g' roles/download/defaults/main.ymlsed -i 's/quay\.io\/calico/xiaoxu780/g' roles/download/defaults/main.yml 修改代码，使用NodePort方式访问Dashboard。12345678vim ./roles/kubernetes-apps/ansible/templates/dashboard.yml.j2# ------------------- Dashboard Service ------------------- #………… targetPort: 8443 type: NodePort //添加这一行 selector:k8s-app: kubernetes-dashboard 注意 如果是单节点部署K8s，Kubespray默认会创建2个coredns Pod，但Deployment中又用到了podAntiAffinity，因此会导致其中一个coredns pod pending，所以需要修改代码如下12345678910111213vim ./roles/kubernetes-apps/ansible/templates/coredns-deployment.yml.j2//注释掉以下几行代码 affinity: #podAntiAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # - topologyKey: "kubernetes.io/hostname" # labelSelector: # matchLabels: # k8s-app: coredns&#123;&#123; coredns_ordinal_suffix | default('') &#125;&#125;或者在spec一行添加代码：spec: replicas: 1 //指定pod为1个副本 安装K8s集群K8s高可用方案Kubernetes的高可用，要解决的核心其实是kube-apiserver组件和etcd的高可用，其它组件在多节点模式下，本身拥有天然容灾性。 etcd高可用 etcd本身就支持集群模式，所以啥都不用考虑，只要保证节点数量足够，升级备份之类的事情，kubespray本身就支持多节点etcd部署。由于etcd采用Raft一致性算法，集群规模最好不要超过9个，推荐3，5，7，9个数量。具体看集群规模。如果性能不够，宁可多分配资源，也最好不要超过9个。 api 高可用 api的高可用，一般有2种思路。 各节点自己代理使用这种方式，会在每个Node节点启动一个Nginx代理，然后由这个Nginx代理负载所有的master节点的api。master会访问自己节点下的api（localhost）。这是Kubespray部署的默认方式。 外置负载均衡利用外部的负载均衡实现，例如阿里云的SLB或自建的HAproxy等。 将hyperkube和kubeadm包下载到所有k8s节点的/tmp/releases 目录下，为了避免科学上网，此处，我下载存放到了网盘上。https://pan.baidu.com/s/1m2rF1dRXIZh_15OevTDbnA 执行部署命令1ansible-playbook -i inventory/mycluster/hosts.ini cluster.yml -b -v -k 运维经验如果需要扩容Work节点，则修改hosts.ini文件，增加新增的机器信息。然后执行下面的命令：1ansible-playbook -i inventory/mycluster/hosts.ini scale.yml -b -v -k 将hosts.ini文件中的master和etcd的机器增加到多台，执行部署命令1ansible-playbook -i inventory/mycluster/hosts.ini cluster.yml -b -vvv 刪除节点，如果不指定节点就是刪除整个集群：1ansible-playbook -i inventory/mycluster/hosts.ini remove-node.yml -b -v 如果需要卸载，可以执行以下命令：1ansible-playbook -i inventory/mycluster/hosts.ini reset.yml -b –vvv 升级K8s集群，选择对应的k8s版本信息，执行升级命令。涉及文件为upgrade-cluster.yml。1ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/hosts.ini -e kube_version=vX.XX.XX -vvv 登录Dashboard登陆Dashboard 支持 kubeconfig 和 token 两种认证方式，kubeconfig 也依赖 token 字段，所以生成 token这一步是必不可少的。此处，我们获取集群管理员（拥有所有命名空间的 admin 权限）的token。 查看kubernetes-dashboard暴露的端口，如下所示，这里是31777端口。12# kubectl get svc --all-namespaces | grep kubernetes-dashboardkube-system kubernetes-dashboard NodePort 10.233.34.183 &lt;none&gt; 443:31777/TCP 12h 获取admin的token1234# kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep tokenName: namespace-controller-token-kmtg7Type: kubernetes.io/service-account-tokentoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1rbXRnNyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImQwYTI0N2JkLTEwY2EtMTFlOS1iYTFiLWEwMzY5ZjNmMDQwNCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.v689nSk_SxDLWk5Mna0t9uITRE1Jy2mstZxeJfZmQmm2UsQ-vIm4ueUNtCoA-PNx49s9hic-Pn6PfqyWQQW_QQ1yLDjjp1wl4J3tdar8fBfuR7Zvm5aKw8kyRhfQzQQZgEKlgBEHaYyKicgVUwEupa3zevXdUTnLH8FudcOdWEwgCflCveHdkRwoy88pYPyL5wh2egEKpeDhzOEztULsi2g3tpdlyg_uQIaKJ1OBODJZz5PXVFMYyIk06SyciEOX0YxF3pH_uSlPqg4RxMaeTfPhlWTnFPlIjQ2juK4s0o2Tyg_sftLSXvd3QtOg3tBavRm3pzHISIPbtN7EZAyWZQ 在dashboard登录页面上使用上面输出中的那个非常长的字符串作为token登录，即可以拥有管理员权限操作整个kubernetes集群中的对象。当然您也可以将这串token加到admin用户的kubeconfig文件中，继续使用kubeconfig登录，两种认证方式任您选择。登录dashboard。https://172.16.0.180:31777 注意由于这里使用的HTTPS，并未使用证书，因此使用Google等浏览器会终止访问（真想说一句，Google真尼玛操蛋，强制使用HTTPS协议）。建议使用firefox浏览器。 验证K8s集群查看集群状态123# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s Ready master,node 36m v1.13.1 查看集群Pod状态123456789101112# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-687b7cc79c-knj87 1/1 Running 0 35mkube-system calico-node-7rj8c 1/1 Running 0 35mkube-system coredns-5b47d4476c-8wdb7 1/1 Running 0 35mkube-system coredns-5b47d4476c-92wnq 1/1 Running 0 35mkube-system dns-autoscaler-5b547856bc-95cft 1/1 Running 0 35mkube-system kube-apiserver-k8s 1/1 Running 0 36mkube-system kube-controller-manager-k8s 1/1 Running 0 36mkube-system kube-proxy-cdlzp 1/1 Running 0 35mkube-system kube-scheduler-k8s 1/1 Running 0 36mkube-system kubernetes-dashboard-d7978b5cc-lvf6l 1/1 Running 0 35m 查看ipvs12345678910111213141516171819202122232425# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.16.0.180:32714 rr -&gt; 10.233.65.133:8443 Masq 1 0 0 TCP 172.17.0.1:32714 rr -&gt; 10.233.65.133:8443 Masq 1 0 0 TCP 10.233.0.1:443 rr -&gt; 172.16.0.180:6443 Masq 1 5 0 TCP 10.233.0.3:53 rr -&gt; 10.233.65.131:53 Masq 1 0 0 -&gt; 10.233.65.134:53 Masq 1 0 0 TCP 10.233.0.3:9153 rr -&gt; 10.233.65.131:9153 Masq 1 0 0 -&gt; 10.233.65.134:9153 Masq 1 0 0 TCP 10.233.45.198:443 rr -&gt; 10.233.65.133:8443 Masq 1 0 0 TCP 10.233.65.128:32714 rr -&gt; 10.233.65.133:8443 Masq 1 0 0 TCP 127.0.0.1:32714 rr -&gt; 10.233.65.133:8443 Masq 1 0 0 UDP 10.233.0.3:53 rr -&gt; 10.233.65.131:53 Masq 1 0 0 -&gt; 10.233.65.134:53 Masq 1 0 0 创建一个Nginx应用的deplpyment。K8s中，针对无状态类服务推荐使用Deployment，有状态类服务则建议使用Statefulset。RC和RS已不支持目前K8s的诸多新特性了。12345678910111213141516171819202122232425262728293031323334353637# vim nginx-deployment.yaml apiVersion: apps/v1kind: Deployment metadata: name: nginx-dmspec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http---apiVersion: v1 kind: Servicemetadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx 123456789101112131415161718# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm createdservice/nginx-svc created# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-dm-799879696c-9cdgz 1/1 Running 0 30snginx-dm-799879696c-cwzn5 1/1 Running 0 30snginx-dm-799879696c-xwjd7 1/1 Running 0 30s # kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.233.0.1 &lt;none&gt; 443/TCP 39m &lt;none&gt;nginx-svc ClusterIP 10.233.42.172 &lt;none&gt; 80/TCP 65s name=nginx# ipvsadm -L -n 测试nginx服务是否正常12345678910# curl -I 10.233.42.172HTTP/1.1 200 OKServer: nginx/1.15.8Date: Sat, 05 Jan 2019 09:58:16 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Wed, 26 Dec 2018 23:21:49 GMTConnection: keep-aliveETag: "5c240d0d-264"Accept-Ranges: bytes 后续K8s从1.11版本起便废弃了Heapster监控组件，取而代之的是metrics-server 和 custom metrics API，后面将陆续完善包括Prometheus+Grafana监控，Kibana+Fluentd日志管理，cephfs-provisioner存储（可能需要重新build kube-controller-manager装上rbd相关的包才能使用Ceph RBD StorageClass），traefik ingress等服务。 参考资料https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>kubespray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrum管理软件开发项目]]></title>
    <url>%2F2018%2F10%2F23%2F%E4%BD%BF%E7%94%A8Scrum%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Scrum 价值观 Scrum三种角色 Product Owner：产品负责人，确定「大家要做什么」。一般由相关的产品经理担任；如果是为客户做项目，PO 一般就是客户负责人。 Scrum Master：Scrum的推动者，掌控大节奏的人。 Team：一般由多个 developer 组成，开发的主力。 Scrum执行流程 概言之，就是1.产品经理整理出需求列表，做好优先级排序，这些需求可以来自客户或者产品经理的判断2.开个会，和研发团队一起讨论需求列表的工作量，确定要进行Sprint（冲刺开发）的任务3.研发团队进入开发环节，这个期间可以让产品经理修改需求，但是不得延期或修改验收标准。Sprint期间每天要进行站会，汇报工作进展、问题和下一步计划4.再开个会，和产品经理一起验收产品，并确定最终的可发布版本。5.整个团队一起回顾项目，提出改进点，然后进入下一个Sprint Scrum敏捷开发执行由Product Backlog、Sprint Backlog、燃尽图/看板和Sprint会议等要素组成。 说明：Backlog 是 Scrum中的一个专用名词，意思是待办工作事项的集合。 1.Sprint迭代周期整个开发过程由若干个短的迭代周期组成，一个短的迭代周期称为一个Sprint，每个Sprint的长度是2到4周。 2.组建团队Scrum团队，推荐人数在5-10人左右，可以按需组建相应的团队等。如根据所开发的软件系统特点，将全员分成4个小组，分别是管理和产品组、前端组A、后端组B、测试组。 3.Product Backlog即产品待办事项列表，也被称为用户故事，是量化的用户需求。即PO、Scrum Master、Team围绕产品创建一个团队需要做的所有事情的列表。然后产品负责人进行筛选并按照优先级进行排序，以商业价值作为排序的主要原则，从Product Backlog中挑选高优先级的需求进行开发，挑选的需求在Sprint计划会议上经过讨论、分析和估算得到相应的任务列表，称它为Sprint backlog。 Product backlog中列举了本项目应该实现的需求，需求采用了用户故事的方式进行描述，用户故事是一句简短的采用用户熟悉的术语表达的需求，是用户讲给开发人员的故事，不是开发人员讲给用户的故事。 故事可以有标准的格式，称为三段论式故事，哪三段呢？ 用户角色 需要的功能 目的 比如，有这样一个故事：作为一个家庭主妇，我需要一个30平米的餐厅，以便于我可以招待10位朋友来用餐，光线明亮。 除了用户故事本身之外，还包括用户故事的验收标准，或者叫用户故事的测试要点，这也是由product owner来完成的，product owner可以先完成前三段，在和team member的沟通过程中，逐步丰富完善验收标准。Product backlog在项目进展过程中是会发生变化的，只有product owner有权来修改此文档。可以用EXCEL文件来管理它。 Product backlog = 用户故事 + 优先级 + 验收标准 4.Sprint Backlog即任务列表，是一次迭代中团队需要完成的具体任务，也是开发过程用得最多的Backlog，非常细化。当Scrum团队完成Sprint backlog列表中的所有任务时，本次Sprint结束，进入下一个Sprint迭代周期。可以用Jira类敏捷工具来管理。 每个月，团队都努力实现列表最顶端的任务，这一部分是他们估计需要一个月完成的工作。把它展开成一个详细的任务列表，叫做Sprint Backlog。这个团队承诺在月底向出资人演示或交付结果。用故事点的方式，即用斐波那契数列的数字（1，2，3，5，8，13，21……）的形式去估算时间。单个用户故事点数不超过8是最理性的状态，超过21则需要拆分。 如图所示。 Product Backlog和Sprint Backlog两者区别如图所示。 5.燃尽图和看板5.1燃尽图即实时评估完成任务数量和剩余时间的趋势。每天Scrum Master都会记录待完成的剩余点数，而后画在燃尽图上。可以使用Jira系统的燃尽图。 横坐标为工作日期，纵坐标为工作量（任务数），每个点代表了在那一天估计剩余的工作量，通过折线依次连接起所有的点形成估计剩余工作量的趋势线。另外还有一条控制线，为最初的估计工作量到结束日期的连线，一般用不同的颜色画上面的两根线。 对此图的研判规则如下：1）如果趋势线在控制线以下，说明进展顺利，有比较大的概率按期或提前完工；2）如果趋势线在控制线以上，说明有比较大的概率延期，此时需要关注进度了。 注意，趋势线并非一直下行，也有可能上行，即发生了错误的估计或遗漏的任务时，估计剩余的工作量也有可能在某天上升了。 每天开完15分钟站立会议后，由scrum master根据进展更新燃尽图。第1个点是项目最初的工作量估计值，第2个点是第最初的估计工作量减去第1天已经完成的任务的工作量，依次类推计算后续的点。任务完成的标志是什么呢？准则如下：1）开发人员检测：所有的单元测试用例都通过；2）Product owner或测试人员检测：通过了所有的功能测试； 燃尽图最好是张贴在白板上，让每个人项目组成员抬头就能看见，这样给大家一个明确的视觉效果，每个人随时都能看到我们离目标有多远。燃尽图可以每天画，表示完成某个迭代的进展趋势，也可以某次迭代结束的时候画，表示完成整个项目的进展趋势，此时横坐标就是迭代的顺序号。 5.2看板让工作透明化除了使用燃尽图之外，还有看板。看板的栏目大致包括待办事项、进行中事项以及已完成事项三个部分。随着迭代进度的推进，由Team每天及时将事项转移到对应看板栏目下。 6.任务申领原则上，奉行Scrum各个组里的成员主动申领任务。 7.Scrum四种会议在SCRUM方法中定义了4种会议活动： Sprint planning即Sprint规划会(Sprint Planning)。这是第一场Scrum会议。团队成员、Scrum Master以及产品负责人坐到一起，规划冲刺的内容。PO 向大家介绍排好序的产品待办事项（Production Backlog），然后大家共同思考决定如何推进计划，梳理出 Sprint Backlog 来完成后续的工作。简单点说，就是一个大家理解「需要做什么」，然后讨论「怎么完成」，并形成待办事项列表的会议。 冲刺周期一般是固定的，不超过一个月，一般是2-4周。团队要从待办事项清单的顶端着手（即从最重要的事项着手），评估一个冲刺阶段能完成多少。 Daily meeting即每日站会，每天团队都面对面地开5～10分钟的会。每日例会后每个人更新自己的任务表，还要每天更新下燃尽图。1）昨天我完成了什么工作？2）今天我打算做什么工作？3）我遇到了哪些困难，打算怎么办，或需要哪些帮助？ Sprint review即Sprint评估或展示会议(Sprint Review)。在冲刺结束前，给产品负责人展示成果，也就是展示哪些事项可以挪到“完成事项”那一栏，并接受评价。这是一场公开的会议，任何人都可以是参与者，不仅仅包括产品负责人、ScrumMaster及开发团队，还包括利益相关者、管理人员与客户。团队应该只展示那些符合“完成定义”的事项，也就是全部完成，不需要再做工作就能交付的成果。这个成果或许不是完整的产品，但至少是一项完整的、可以使用的功能。 Sprint retrospective即Sprint回顾会议，会议维持在2小时以内，主要总结此次Sprint实施的经验教训，如何在下一个Sprint中发挥。冲刺回顾会要认真分析以下几个问题：1）发生了哪些有待改进的事；2）为什么会发生那件事；3）为什么我们当时忽略了；4）怎样才能加快工作进度。 除去开发活动外这4种会议构成了scrum方法的核心活动。这四种会议的要点如下。 最后，敏捷开发在线协作工具，有Jira、Teambition和Worktitle等。]]></content>
      <categories>
        <category>项目管理</category>
      </categories>
      <tags>
        <tag>Scrum</tag>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Gnocchi使用Ceph后端存储]]></title>
    <url>%2F2018%2F06%2F10%2F%E9%85%8D%E7%BD%AEGnocchi%E4%BD%BF%E7%94%A8Ceph%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[1.给gnocchi创建一个专用的ceph pool，用来存放计量数据。1# ceph osd pool create gnocchi 128 128 gnocchi pool的pg_num需要根据实际的ceph环境确定。 2.给gnocchi创建一个ceph用户。123# ceph auth get-or-create client.gnocchi mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=gnocchi'[client.gnocchi] key = AQB+5xxbZKKQIhAA7FgJKYBQNjF6dD3FZzJvUQ== 3.保存keyring文件1ceph auth get-or-create client.gnocchi | tee ceph.client.gnocchi.keyring 4.在gnocchi容器中创建/etc/ceph目录123# docker exec -u root -it gnocchi_api mkdir /etc/ceph# docker exec -u root -it gnocchi_statsd mkdir /etc/ceph# docker exec -u root -it gnocchi_metricd mkdir /etc/ceph 5.在ceph节点拷贝相关文件到controller节点12# scp ceph.client.gnocchi.keyring controller1:/home# scp /etc/ceph/ceph.conf controller1:/home 6.拷贝gnocchi的keyring文件和ceph配置文件到gnocchi容器的/etc/ceph目录下12345678# docker cp /home/ceph.conf gnocchi_api:/etc/ceph/# docker cp /home/ceph.client.gnocchi.keyring gnocchi_api:/etc/ceph/# docker cp /home/ceph.conf gnocchi_statsd:/etc/ceph/# docker cp /home/ceph.client.gnocchi.keyring gnocchi_statsd:/etc/ceph/# docker cp /home/ceph.conf gnocchi_metricd:/etc/ceph/# docker cp /home/ceph.client.gnocchi.keyring gnocchi_metricd:/etc/ceph/ 7.修改文件gnocchi.conf123456789101112# vim /etc/kolla/gnocchi-api/gnocchi.conf # vim /etc/kolla/gnocchi-metricd/gnocchi.conf # vim /etc/kolla/gnocchi-statsd/gnocchi.conf 添加如下内容[storage]driver = cephceph_username = gnocchiceph_keyring = /etc/ceph/ceph.client.gnocchi.keyringceph_conffile = /etc/ceph/ceph.confceph_secret = AQB+5xxbZKKQIhAA7FgJKYBQNjF6dD3FZzJvUQ==ceph_pool = gnocchi 初始化数据库123# docker exec -it gnocchi_api gnocchi-upgrade# docker exec -it gnocchi_metricd gnocchi-upgrade# docker exec -it gnocchi_statsd gnocchi-upgrade 8.设置权限123# docker exec -u root -it gnocchi_api chown -R gnocchi:gnocchi /etc/ceph/# docker exec -u root -it gnocchi_metricd chown -R gnocchi:gnocchi /etc/ceph/# docker exec -u root -it gnocchi_statsd chown -R gnocchi:gnocchi /etc/ceph/ 9.重启服务生效1# docker restart gnocchi_api gnocchi_metricd gnocchi_statsd]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装OpenStack Zaqar服务]]></title>
    <url>%2F2018%2F05%2F26%2F%E5%AE%89%E8%A3%85OpenStack-Zaqar%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[关于什么是Zaqar，有什么作用。国内已有介绍读者可以自行Google查阅。若在此再阐述，已显多余。由于安装Zaqar服务官方文档还有坑且国内无资料，故这里就写一写吧。 依赖服务 一个基本的OpenStack正常运行环境 MongoDB（必须大于或等于2个节点，否则会报错） Memcache 说明 本环境，MongoDB和memcache已有kolla-ansible事先安装好。所以，这里只需要安装配置zaqar服务即可。 1.创建keystone认证信息12345678# source admin-openrc.sh# useradd zaqar# openstack user create --domain default --password-prompt zaqar# openstack role add --project service --user zaqar admin# openstack service create --name zaqar --description "Messaging" messaging# openstack endpoint create --region RegionOne messaging public http://172.17.223.21:8888# openstack endpoint create --region RegionOne messaging internal http://172.17.223.21:8888# openstack endpoint create --region RegionOne messaging admin http://172.17.223.21:8888 2.安装zaqar，这里使用queens版本1234# git clone https://git.openstack.org/openstack/zaqar.git -b stable/queens# cd zaqar# pip install . -r ./requirements.txt --upgrade --log /tmp/zaqar-pip.log# pip install --upgrade pymongo gevent uwsgi 3.创建zaqar配置目录123# mkdir /etc/zaqar# oslopolicy-sample-generator --config-file etc/oslo-config-generator/zaqar-policy-generator.conf# cp etc/zaqar.policy.yaml.sample /etc/zaqar/policy.yaml 4.创建zaqar的log文件123# touch /var/log/zaqar-server.log# chown zaqar:zaqar /var/log/zaqar-server.log# chmod 600 /var/log/zaqar-server.log 5.创建/srv/zaqar目录12345# mkdir /srv/zaqar# vim /srv/zaqar/zaqar_uwsgi.pyfrom keystonemiddleware import auth_tokenfrom zaqar.transport.wsgi import appapp = auth_token.AuthProtocol(app.app, &#123;&#125;) 说明 注意，下面的参数“listen = 1024”，超过了系统的默认值128，会报错。因此需要修改默认值，如这里的2048。1234567891011121314151617# echo "net.core.somaxconn=2048" | sudo tee --append /etc/sysctl.conf# echo 2048 &gt; /proc/sys/net/core/somaxconn# echo 2048 &gt; /proc/sys/net/ipv4/tcp_max_syn_backlog# vim /srv/zaqar/uwsgi.ini[uwsgi]http = 172.17.223.21:8888pidfile = /var/run/zaqar.pidgevent = 2000gevent-monkey-patch = truelisten = 1024enable-threads = truechdir = /srv/zaqarmodule = zaqar_uwsgi:appworkers = 4harakiri = 60add-header = Connection: close 6.创建pid文件12# touch /var/run/zaqar.pid# chown zaqar:zaqar /var/run/zaqar.pid 7.编辑zaqar配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# vim /etc/zaqar/zaqar.conf[DEFAULT]pooling = Trueadmin_mode = Truedebug = Truelog_file = /var/log/zaqar-server.log;auth_strategy = keystone[keystone_authtoken]auth_uri = http://172.17.223.20:5000auth_url = http://172.17.223.20:35357auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = zaqarpassword = zaqarmemcache_security_strategy = ENCRYPTmemcache_secret_key = wRJgCSJjqUT5JWlGVyvSygovRiyXFgJg7kYz1sXXmemcached_servers = 172.17.223.21:11211,172.17.223.26:11211[cache]backend = oslo_cache.memcache_poolenabled = Truememcache_servers = 172.17.223.21:11211,172.17.223.26:11211[drivers]transport = wsgimessage_store = mongodbmanagement_store = mongodb[drivers:management_store:mongodb]uri = mongodb://172.17.223.21,172.17.223.26:27017/?replicaSet=rs0&amp;w=2&amp;readPreference=secondaryPreferreddatabase = zaqarmanagementstorepartitions = 8;max_attempts = 1000;max_retry_sleep = 0.1;max_retry_jitter = 0.005;gc_interval = 5 * 60;gc_threshold = 1000[drivers:message_store:mongodb]database = zaqarmessagestoreuri = mongodb://172.17.223.21,172.17.223.26:27017/?replicaSet=rs0&amp;w=2&amp;readPreference=secondaryPreferred[drivers:transport:wsgi]bind = 0.0.0.0[transport]max_queues_per_page = 1000max_queue_metadata = 262144max_mesages_per_page = 10max_messages_post_size = 262144max_message_ttl = 1209600max_claim_ttl = 43200max_claim_grace = 43200[signed_url]secret_key = SOMELONGSECRETKEY 8.编辑uwsgi服务启动文件1234567891011121314151617# vim /etc/systemd/system/zaqar.uwsgi.service[Unit]Description=uWSGI ZaqarAfter=syslog.target[Service]ExecStart=/usr/bin/uwsgi --ini /srv/zaqar/uwsgi.ini# Requires systemd version 211 or newerRuntimeDirectory=uwsgiRestart=alwaysKillSignal=SIGQUITType=notifyStandardError=syslogNotifyAccess=allUser=zaqarGroup=zaqar[Install]WantedBy=multi-user.target 启动uwsgi服务12# systemctl start zaqar.uwsgi.service# systemctl enable zaqar.uwsgi.service 说明，如果报错可以查看日志。1# tailf /var/log/messages 9.配置Pool 生成一个UUID12# uuidgen7289f400-2439-4822-9e3a-928af262d843 运行cURL命令去请求一个keystone token。1234567891011121314151617181920212223242526# cat auth_token.json &#123; "auth": &#123; "identity": &#123; "methods": [ "password" ], "password": &#123; "user": &#123; "name": "zaqar", "domain": &#123; "id": "default", "name": "Default" &#125;, "password": "zaqar" &#125; &#125; &#125;, "scope": &#123; "project": &#123; "name": "service", "domain": &#123;"id": "default"&#125; &#125; &#125; &#125;&#125; 比如，这里获取到的token是12# curl -i -s -d@auth_token.json -H "Content-Type: application/json" http://172.17.223.21:5000/v3/auth/tokens |grep 'X-Subject-Token'X-Subject-Token: 7212ad333e6d4a1781974f8e203bf555 配置Pool123456# curl -i -X PUT http://172.17.223.21:8888/v2/pools/testpool -d '&#123;"weight": 100, "uri": "mongodb://172.17.223.21,172.17.223.26:27017/?replicaSet=rs0&amp;w=2&amp;readPreference=secondaryPreferred", "options": &#123;"partitions": 8&#125;&#125;' -H "Client-ID: 7289f400-2439-4822-9e3a-928af262d843" -H "X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b" -H "Content-type: application/json"HTTP/1.1 201 Createdcontent-length: 0content-type: application/json; charset=UTF-8location: http://172.17.223.21:8888/v2/pools/testpoolConnection: close 验证操作 12345678# curl -i -X POST http://172.17.223.21:8888/v2/queues/samplequeue/messages -d '&#123;"messages": [&#123;"body": &#123;"event": 1&#125;, "ttl": 600&#125;, &#123;"body": &#123;"event": 2&#125;, "ttl": 600&#125;]&#125;' -H "Content-type: application/json" -H "Client-ID: 7289f400-2439-4822-9e3a-928af262d843" -H "X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b"HTTP/1.1 201 Createdcontent-length: 135content-type: application/json; charset=UTF-8location: http://172.17.223.21:8888/v2/queues/samplequeue/messages?ids=5b0bb7289140d605340bf3a6,5b0bb7289140d605340bf3a7Connection: close&#123;"resources": ["/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a6", "/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a7"]&#125; 安装Zaqar UI 123456# git clone https://github.com/openstack/zaqar-ui -b stable/queens# docker cp zaqar-ui horizon:/home# docker exec -u root -it horizon bash# cd /home &amp;&amp; pip install -e zaqar-ui/# cp zaqar-ui/zaqar_ui/enabled/_1510_project_messaging_group.py /var/lib/kolla/venv/lib/python2.7/site-packages/openstack_dashboard/local/enabled# cp zaqar-ui/zaqar_ui/enabled/_1520_project_queues.py /var/lib/kolla/venv/lib/python2.7/site-packages/openstack_dashboard/local/enabled 最后，你就可以愉快的使用Zaqar服务啦。当然，由于该项目目前尚不成熟，参阅资料很少，希望你能一边使用一边填坑。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从数据删除看备份的重要性]]></title>
    <url>%2F2018%2F05%2F23%2F%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E7%9C%8B%E5%A4%87%E4%BB%BD%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%2F</url>
    <content type="text"><![CDATA[概述所谓，“常在河边走，哪有不湿鞋”。在一个实际的环境中，由于种种原因，可能发生数据被删除的情况。比如，云平台中的数据库、虚拟机、数据卷、镜像或底层存储被删除等，如果数据没有进行备份，则是灾难性的后果。 在笔者的工作中，经历过2次在生产环境云平台上，客户虚拟机数据被删除的情况，一次是研发部门开发的代码逻辑判断错误导致，另一次是运维同事人为误操作。因此，觉得有必要调研整理下数据备份相关的小文，以此共勉。 在一个由OpenStack+Ceph架构组成的云平台环境中，有N种数据备份方案。如OpenStack有自带的Karbor、Freezer云服务，Ceph也有相关的备份方案，也有其他商业的备份方案等。实际上，OpenStack云平台本身也提供了一些较好易用的备份功能，比如虚拟机快照/备份、数据卷快照/备份，在使用时也倡导通过将数据卷挂载给虚拟机，从而将数据写入到云盘中，间接的实现数据容灾备份。 但，如果删除的是底层存储数据，上层的备份操作基本都将报废。那么有什么好的备份方案吗。这里，我们阐述下Ceph相关的备份方案。 方案1 Snapshot原理 异步备份，基于RBD的snapshot机制。 介绍 在此方案下，Cluster A &amp; B是独立的Ceph集群，通过RBD的snapshot机制，在Cluster A端，针对image定期通过rbd创建image的快照，然后通过rbd export-diff, rbd import-diff命令来完成image备份到Cluster B。 命令和步骤 把 Cluster A 的 pool 中的testimage 异步备份到 Cluster B 的 pool 中。 1）在Cluster A/B上创建rbd/testimage1rbd create -p rbd --size 10240 testimage 2）在准备备份image前，暂停Cluster A端对testimage的IO操作，然后创建个snapshot1rbd snap create &lt;snap-name&gt; 3）导出Cluster A端的testimage数据，不指定from-snap1rbd export-diff &lt;image-name&gt; &lt;path&gt; 4）copy上一步中导出的文件到Cluster B，并导入数据到testimage1rbd import-diff &lt;path&gt; &lt;image-name&gt; 后续需周期性的暂停Cluster A端的testimage的IO，然后创建snapshot，通过 rbd export-diff [–from-snap ] 命令导出incremental diff，之后把差异数据文件copy到Cluster B上，然后通过命令rbd import-diff 导入。 【注】：也可不暂停Cluster A端的IO，直接take snapshot；这样并不会引起image的数据不一致，只是有可能会使rbd export-diff导出的数据在take snapshot之后。 此方案优缺点 优点： 命令简单，通过定制执行脚本就能实现rbd块设备的跨区备份 缺点： 每次同步前都需要在源端take snapshot或暂停IO操作 持续的snapshots可能导致image的读写性能下降 还要考虑后续删除不用的snapshots snapshot只能保证IO的一致性，并不能保证使用rbd块设备上的系统一致性； 方案2 RBD Mirroring原理 异步备份，Ceph新的rbd mirror功能 介绍 Ceph新的rbd mirror功能支持配置两个Ceph Cluster之间的rbd同步。 在此方案下，Master Cluster使用性能较高的存储设备，提供给OpenStack的Glance、Cinder（cinder-volume、cinder-backup）和Nova服务使用；而Backup Cluster则使用容量空间大且廉价的存储设备（如SATA盘）来备份Ceph数据。不同的Ceph Cluster集群，可以根据实际需要，选择是否跨物理机房备份。 优点： Ceph新的功能，不需要额外开发 同步的粒度比较小，为一个块设备的transaction 保证了Crash consistency 可配置pool的备份，也可单独指定image备份 同步备份，不同机房的Ceph集群，底层存储的跨机房容灾 方案3 ceph备份软件ceph-backup介绍 ceph-backup是一个用来备份ceph RBD块设备的开源软件，提供了两种模式。 增量：在给定备份时间窗口内基于rbd快照的增量备份 完全：完整映像导出时不包含快照 编译安装123# git clone https://github.com/teralytics/ceph-backup.git# cd ceph-backup# python setup.py install 安装过程中会下载一些东西，注意要有网络，需要等待一会准备配置文件12# mkdir /etc/cephbackup/# cp ceph-backup.cfg /etc/cephbackup/cephbackup.conf 我的配置文件如下，备份rbd存储的zp的镜像，支持多image，images后面用逗号隔开就可以12345678910# cat /etc/cephbackup/cephbackup.conf [rbd]window size = 7window unit = daysdestination directory = /tmp/images = zpcompress = yesceph config = /etc/ceph/ceph.confbackup mode = fullcheck mode = no 开始备份 全量备份配置上面的配置文件已经写好了，直接执行备份命令就可以了123456789101112131415161718# cephbackupStarting backup for pool rbdFull ceph backupImages to backup: rbd/zpBackup folder: /tmp/Compression: TrueCheck mode: FalseTaking full backup of images: zprbd image 'zp': size 40960 MB in 10240 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.25496b8b4567 format: 2 features: layering flags: Exporting image zp to /tmp/rbd/zp/zp_UTC20170119T092933.fullCompress mode activated 12# rbd export rbd/zp /tmp/rbd/zp/zp_UTC20170119T092933.fullExporting image: 100% complete...done. 12# tar scvfz /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz /tmp/rbd/zp/zp_UTC20170119T092933.fulltar: Removing leading `/' from member names 压缩如果打开了，正好文件也是稀疏文件的话，需要等很久，压缩的效果很好，dd生成的文件可以压缩到很小检查备份生成的文件123# ll /tmp/rbd/zp/zp_UTC20170119T092933.full*-rw-r--r-- 1 root root 42949672960 Jan 19 17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full-rw-r--r-- 1 root root 0 Jan 19 17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz 全量备份的还原1# rbd import /tmp/rbd/zp/zp_UTC20170119T092933.full zpbk 检查数据，没有问题增量备份配置写下增量配置的文件，修改下备份模式的选项123456789[rbd]window size = 7window unit = daydestination directory = /tmp/images = zpcompress = yesceph config = /etc/ceph/ceph.confbackup mode = incrementalcheck mode = no 执行多次进行增量备份以后是这样的1234567# ll /tmp/rbd/zpbk/total 146452-rw-r--r-- 1 root root 42949672960 Jan 19 18:04 zpbk@UTC20170119T100339.full-rw-r--r-- 1 root root 66150 Jan 19 18:05 zpbk@UTC20170119T100546.diff_from_UTC20170119T100339-rw-r--r-- 1 root root 68 Jan 19 18:05 zpbk@UTC20170119T100550.diff_from_UTC20170119T100546-rw-r--r-- 1 root root 68 Jan 19 18:06 zpbk@UTC20170119T100606.diff_from_UTC20170119T100550-rw-r--r-- 1 root root 68 Jan 19 18:06 zpbk@UTC20170119T100638.diff_from_UTC20170119T100606 增量备份的还原 分成多个步骤进行 1、进行全量的恢复1# rbd import config@UTC20161130T170848.full dest_image 2、重新创建基础快照1# rbd snap create dest_image@UTC20161130T170848 3、还原增量的快照1# rbd import-diff config@UTC20161130T170929.diff_from_UTC20161130T170848 dest_image 总结综合评估，以上三种方案的优缺点，推荐使用“方案二RBD Mirroring”。 参考链接http://www.yangguanjun.com/2017/02/22/rbd-data-replication/https://ceph.com/planet/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/]]></content>
      <categories>
        <category>OpenStack</category>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊OpenStack运维架构那些事儿]]></title>
    <url>%2F2018%2F05%2F09%2F%E8%81%8A%E8%81%8AOpenStack%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%2F</url>
    <content type="text"><![CDATA[前言想一想，从事OpenStack杂七杂八的事儿，至今正好三年半了。做过QA测试（手动的、自动的）、CI（gerrit、jenkins、gitlab、harbor）、云产品封装（从系统pxe到openstack代码）、自动化部署开发、运维监控、分布式存储、底层功能调研和实现、开源社区参与、Docker等等。 一个良好的架构设计和运维保障措施，能为OpenStack云平台的稳定健康运行，产生不可估量的积极影响。 下面，是笔者从业OpenStack以来，关于OpenStack运维、架构设计、实施的点滴之想。在此，做一个回顾和总结。如有差错，欢迎拍砖。 OK，咱们言归正传进入话题吧。如果化繁为简，简单的来说，要部署一套生产环境级别的OpenStack云平台，至少会涉及到四个层次的内容，即物理基础设施层、存储层、OpenStack云服务层和用户应用层。如下图所示。 物理基础设施层首先，从最底层开始说起，即“物理基础设施层”。一个基本的物理基础设施IT环境，包括了电力设备、空调和防火设备、网络设备（如交换机、路由器、防火墙等）、存储设备和服务器等。由于专业知识的限制，这里，只涉及交换机和服务器方面。一个基本的物理IT环境，如下图所示。 交换机设备一般地，在OpenStack生产环境上，交换机端口应该做聚合（channel）。也就是将2个或多个物理端口组合在一起成为一条逻辑的链路从而增加交换机和网络节点之间的带宽，将属于这几个端口的带宽合并，给端口提供一个几倍于独立端口的独享的高带宽。Trunk是一种封装技术，它是一条点到点的链路，链路的两端可以都是交换机，也可以是交换机和路由器，还可以是主机和交换机或路由器。 服务器网卡OpenStack云平台涉及到的网络有管理网络（用于OpenStack各服务之间通信）、外部网络（提供floating ip）、存储网络（如ceph存储网络）和虚机网络（也称租户网络、业务网络）四种类型。 对应到每一种网络，服务器都应该做网卡Bond，来提供服务器网络的冗余、高可用和负载均衡的能力，根据实际需求，可以选择模式0或模式1。在网络流量较大的场景下推荐使用bond 0；在可靠性要求较高的场景下推荐使用bond 1。 二者优劣比较。 在生产环境中，如果是少于90台OpenStack节点规模的私有云，一般网络类型对应的带宽是（PS：90台只是一个相对值，非绝对值，有洁癖的人请绕过）。 管理网络：千兆网络 外部网络：千兆网络 存储网络：万兆网络 租户网络：千兆网络 如果是多于90台OpenStack节点规模的私有云或公有云环境，则推荐尽量都使用万兆网络。 硬盘服务器操作系统使用的系统盘，应该用2块硬盘来做RAID 1，以提供系统存储的高可靠性。且推荐使用高性能且成本可控的SAS硬盘，以提高操作系统、MySQL数据库和Docker容器（如果使用kolla部署openstack）的存储性能。 CPUOpenStack各计算节点的CPU型号，必须一致，以保证虚拟机的迁移功能正常可用等。 内存OpenStack各计算节点的内存大小，应该一致，以保证虚拟机创建管理的均衡调度等。同时，主机的Swap交换分区，应该科学合理的设置，不能使用系统默认创建的。如何设置，请参考此文。如何设置OpenStack节点Swap分区。 数据中心中少部分机器用于做控制节点，大部分机器都是需要运行虚拟化软件的，虚拟化平台上有大量的vm，而宿主机本身的系统也会跑一些服务，那么这就势必会造成vm之间资源抢占，vm与宿主机系统之间的资源抢占，我们需要通过设定游戏规则，让他们在各自的界限内高效运行，减少冲突抢占。 我们可以让宿主机运行操作系统时候，更多的选择指定的几个核，这样就不会过多抢占虚拟化中虚机的vcpu调度，通过修改内核启动参数我们可以做到: 修改 /etc/default/grub文件，让系统只使用前三个核 隔离其余核。1GRUB_CMDLINE_LINUX_DEFAULT="isolcpus=4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31" 更新内核参数12# update-grub# reboot 内存配置方面，网易私有云的实践是关闭 KVM 内存共享，打开透明大页：12345echo 0 &gt; /sys/kernel/mm/ksm/pages_sharedecho 0 &gt; /sys/kernel/mm/ksm/pages_sharingecho always &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho 0 &gt; /sys/kernel/mm/transparent_hugepage/khugepaged/defrag 据说，经过 SPEC CPU2006 测试，这些配置对云主机 CPU 性能大概有7%左右的提升。 OpenStack云平台层云平台高可用（HA）高可用（HA）介绍高可用性是指提供在本地系统单个组件故障情况下，能继续访问应用的能力，无论这个故障是业务流程、物理设施、IT软/硬件的故障。最好的可用性， 就是你的一台机器宕机了，但是使用你的服务的用户完全感觉不到。你的机器宕机了，在该机器上运行的服务肯定得做故障切换（failover），切换有两个维度的成本：RTO （Recovery Time Objective）和 RPO（Recovery Point Objective）。RTO 是服务恢复的时间，最佳的情况是 0，这意味着服务立即恢复；最坏是无穷大意味着服务永远恢复不了；RPO 是切换时向前恢复的数据的时间长度，0 意味着使用同步的数据，大于 0 意味着有数据丢失，比如 ” RPO = 1 天“ 意味着恢复时使用一天前的数据，那么一天之内的数据就丢失了。因此，恢复的最佳结果是 RTO = RPO = 0，但是这个太理想，或者要实现的话成本太高。 对 HA 来说，往往使用分布式存储，这样的话，RPO =0 ；同时使用 Active/Active （双活集群） HA 模式来使得 RTO 几乎为0，如果使用 Active/Passive HA模式的话，则需要将 RTO 减少到最小限度。HA 的计算公式是[ 1 - (宕机时间)/（宕机时间 + 运行时间）]，我们常常用几个 9 表示可用性： 2 个9：99% = 1% 365 = 3.65 24 小时/年 = 87.6 小时/年的宕机时间 4 个9: 99.99% = 0.01% 365 24 * 60 = 52.56 分钟/年 5 个9：99.999% = 0.001% * 365 = 5.265 分钟/年的宕机时间，也就意味着每次停机时间在一到两分钟。 11 个 9：几年宕机几分钟。 服务的分类 HA 将服务分为两类： 有状态服务：后续对服务的请求依赖于之前对服务的请求。OpenStack中有状态的服务包括MySQL数据库和AMQP消息队列。对于有状态类服务的HA，如neutron-l3-agent、neutron-metadata-agent、nova-compute、cinder-volume等服务，最简便的方法就是多节点部署。比如某一节点上的nova-compute服务挂了，也并不会影响到整个云平台不能创建虚拟机，或者所在节点的虚拟机无法使用（比如ssh等）。 无状态服务：对服务的请求之间没有依赖关系，是完全独立的，基于冗余实例和负载均衡实现HA。OpenStack中无状态的服务包括nova-api、nova-conductor、glance-api、keystone-api、neutron-api、nova-scheduler等。由于API服务，属于无状态类服务，天然支持Active/Active HA模式。因此，一般使用 keepalived +HAProxy方案来做。 HA 的种类 HA 需要使用冗余的服务器组成集群来运行负载，包括应用和服务。这种冗余性也可以将 HA 分为两类： Active/Passive HA：集群只包括两个节点简称主备。在这种配置下，系统采用主和备用机器来提供服务，系统只在主设备上提供服务。在主设备故障时，备设备上的服务被启动来替代主设备提供的服务。典型地，可以采用 CRM 软件比如 Pacemaker 来控制主备设备之间的切换，并提供一个虚机 IP 来提供服务。 Active/Active HA：集群只包括两个节点时简称双活，包括多节点时成为多主（Multi-master）。在这种配置下，系统在集群内所有服务器上运行同样的负载。以数据库为例，对一个实例的更新，会被同步到所有实例上。这种配置下往往采用负载均衡软件比如 HAProxy 来提供服务的虚拟 IP。 OpenStack云环境高可用（HA）云环境是一个广泛的系统，包括了基础设施层、OpenStack云平台服务层、虚拟机和最终用户应用层。 云环境的 HA 包括： 用户应用的 HA 虚拟机的 HA OpenStack云平台服务的 HA 基础设施层的HA：电力、空调和防火设施、网络设备（如交换机、路由器）、服务器设备和存储设备等 仅就OpenStack云平台服务（如nova-api、nova-scheduler、nova-compute等）而言，少则几十，多则上百个。如果某一个服务挂了，则对应的功能便不能正常使用。因此，如何保障整体云环境的HA高可用，便成为了架构设计和运维的重中之重。 OpenStack HA高可用架构，如下图所示。 OpenStack高可用内容 如果，从部署层面来划分，OpenStack高可用的内容包括： 控制节点（Rabbitmq、mariadb、Keystone、nova-api等） 网络节点（neutron_dhcp_agent、neutron_l3_agent、neutron_openvswitch_agent等） 计算节点（Nova-Compute、neutron_openvswitch_agent、虚拟机等） 存储节点（cinder-volume、swift等） 控制节点HA在生产环境中，建议至少部署三台控制节点，其余可做计算节点、网络节点或存储节点。采用Haproxy + KeepAlived方式，代理数据库服务和OpenStack服务，对外暴露VIP提供API访问。 MySQL数据库HA mysql 的HA 方案有很多，这里只讨论openstack 官方推荐的mariadb galara 集群。Galera Cluster 是一套在innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到各个节点上去。特点如下：1）同步复制，（&gt;=3）奇数个节点2）Active-active的多主拓扑结构3）集群任意节点可以读和写4）自动身份控制,失败节点自动脱离集群5）自动节点接入6）真正的基于”行”级别和ID检查的并行复制7）无单点故障,易扩展 采用MariaDB + Galera方案部署至少三个节点（最好节点数量为奇数），外部访问通过Haproxy的active + backend方式代理。平时主库为A，当A出现故障，则切换到B或C节点。如下图所示。 RabbitMQ 消息队列HA RabbitMQ采用原生Cluster集群方案，所有节点同步镜像队列。三台物理机，其中2个Mem节点主要提供服务，1个Disk节点用于持久化消息，客户端根据需求分别配置主从策略。 OpenStack API服务HA OpenStack控制节点上运行的基本上是API 无状态类服务，如nova-api、neutron-server、glance-registry、nova-novncproxy、keystone等。因此，可以由 HAProxy 提供负载均衡，将请求按照一定的算法转到某个节点上的 API 服务，并由KeepAlived提供 VIP。 网络节点HA网络节点上运行的Neutron服务包括很多的组件，比如 L3 Agent，openvswitch Agent，LBaas，VPNaas，FWaas，Metadata Agent 等，其中部分组件提供了原生的HA 支持。 Openvswitch Agent HA： openvswitch agent 只在所在的网络或者计算节点上提供服务，因此它是不需要HA的 L3 Agent HA：成熟主流的有VRRP 和DVR两种方案 DHCP Agent HA：在多个网络节点上部署DHCP Agent，实现HA LBaas Agent HA：Pacemaker + 共享存储（放置 /var/lib/neutron/lbaas/ 目录） 的方式来部署 A/P 方式的 LBaas Agent HA 存储节点HA存储节点的HA，主要是针对cinder-volume、cinder-backup服务做HA，最简便的方法就是部署多个存储节点，某一节点上的服务挂了，不至于影响到全局。 计算节点和虚拟机 HA计算节点和虚拟机的HA，社区从2016年9月开始一直致力于一个虚拟机HA的统一方案，详细参考：High Availability for Virtual Machines。目前还处于开发阶段。业界目前使用的方案大致有以下几种： 1)检查compute计算节点和nova 服务运行状态，对于有问题的节点或服务进行自动修复。该方案的实现是： ①Pacemaker 监控每个计算节点上的 pacemaker_remote 的连接，来检查该节点是否处于活动状态。发现它不可以连接的话，启动恢复（recovery）过程。 运行 ‘nova service-disable’ 将该节点关机 等待 nova 发现该节点失效了 将该节点开机 如果节点启动成功，执行 ‘nova service-enable’ 如果节点启动失败，则执行 ‘nova evacuate’ 把该节点上的虚机移到别的可用计算节点上。 ②Pacemaker 监控每个服务的状态，如果状态失效，该服务会被重启，重启失败则触发防护行为（fencing action），即停掉该服务。 2)分布式健康检查，参考分布式健康检查：实现OpenStack计算节点高可用 如果使用第一种方案，实现计算节点和虚拟机HA，要做的事情基本有三件，即。 监控 监控主要做两个事情，一个是监控计算节点的硬件和软件故障。第二个是触发故障的处理事件，也就是隔离和恢复。 OpenStack 计算节点高可用，可以用pacemaker和pacemaker_remote来做。使用pacemaker_remote后，我们可以把所有的计算节点都加入到这个集群中，计算节点只需要安装pacemaker_remote即可。pacemaker集群会监控计算节点上的pacemaker_remote是否 “活着”，你可以定义什么是“活着”。在计算节点上可以监控nova-compute、neutron-ovs-agent、libvirt等进程，从而确定计算节点是否活着，甚至我们还可以在该计算节点上启动虚拟机来确定计算节点是否活着。如果监控到某个pacemaker_remote有问题，可以马上触发之后的隔离和恢复事件。 隔离 隔离最主要的任务是将不能正常工作的计算节点从OpenStack集群环境中移除，nova-scheduler就不会在把create_instance的message发给该计算节点。 Pacemaker 已经集成了fence这个功能，因此我们可以使用fence_ipmilan来关闭计算节点。Pacemaker集群中fence_compute 会一直监控这个计算节点是否down了，因为nova只能在计算节点down了之后才可以执行host-evacuate来迁移虚拟机，期间等待的时间稍长。这里有个更好的办法， 就是调用nova service-force-down 命令，直接把计算节点标记为down，方便更快的迁移虚拟机。 恢复 恢复就是将状态为down的计算节点上的虚拟机迁移到其他计算节点上。Pacemaker集群会调用host-evacuate API将所有虚拟机迁移。host-evacuate最后是使用rebuild来迁移虚拟机，每个虚拟机都会通过scheduler调度在不同的计算节点上启动。 虚拟机操作系统故障恢复 OpenStack 中的 libvirt/KVM 驱动已经能够很好地自动化处理这类问题。具体地，你可以在flavor的 extra_specs 或者镜像的属性中加上 hw:watchdog_action ，这样 一个 watchdog 设备会配置到虚拟机上。如果 hw:watchdog_action 设置为 reset，那么虚拟机的操作系统一旦奔溃，watchdog 会将虚拟机自动重启。 OpenStack计算资源限制设置内存 #内存分配超售比例，默认是 1.5 倍，生产环境不建议开启超售1ram_allocation_ratio = 1 #内存预留量，这部分内存不能被虚拟机使用，以便保证系统的正常运行1reserved_host_memory_mb = 10240 //如预留10GB 设置CPU 在虚拟化资源使用上，我们可以通过nova来控制，OpenStack提供了一些配置，我们可以很容易的做到，修改文件nova.conf。 #虚拟机 vCPU 的绑定范围，可以防止虚拟机争抢宿主机进程的 CPU 资源，建议值是预留前几个物理 CPU1vcpu_pin_set = 4-31 #物理 CPU 超售比例，默认是 16 倍，超线程也算作一个物理 CPU1cpu_allocation_ratio = 8 使用多Region和AZ如果，OpenStack云平台需要跨机房或地区部署，可以使用多Region和 Availability Zone（以下简称AZ）的方案。这样，每个机房之间在地理位置上自然隔离，这对上层的应用来说是天然的容灾方法。 多区域（Region）部署 OpenStack支持依据地理位置划分为不同的Region，所有的Regino除了共享Keystone、Horizon服务外，每个Region都是一个完整的OpenStack环境，从整体上看，多个区域之间的部署相对独立，但可通过内网专线实现互通（如BGP-EVPN）。其架构如下图所示。 部署时只需要部署一套公共的Keystone和Horizon服务，其它服务按照单Region方式部署即可，通过Endpoint指定Region。用户在请求任何资源时必须指定具体的区域。采用这种方式能够把分布在不同的区域的资源统一管理起来，各个区域之间可以采取不同的部署架构甚至不同的版本。其优点如下： 部署简单，每个区域部署几乎不需要额外的配置，并且区域很容易实现横向扩展。 故障域隔离，各个区域之间互不影响。 灵活自由，各个区域可以使用不同的架构、存储、网络。 但该方案也存在明显的不足： 各个区域之间完全隔离，彼此之间不能共享资源。比如在Region A创建的Volume，不能挂载到Region B的虚拟机中。在Region A的资源，也不能分配到Region B中，可能出现Region负载不均衡问题。 各个区域之间完全独立，不支持跨区域迁移，其中一个区域集群发生故障，虚拟机不能疏散到另一个区域集群中。 Keystone成为最主要的性能瓶颈，必须保证Keystone的可用性，否则将影响所有区域的服务。该问题可以通过部署多Keystone节点解决。 OpenStack多Region方案通过把一个大的集群划分为多个小集群统一管理起来，从而实现了大规模物理资源的统一管理，它特别适合跨数据中心并且分布在不同区域的场景，此时根据区域位置划分Region，比如北京和上海。而对于用户来说，还有以下好处: 用户能根据自己的位置选择离自己最近的区域，从而减少网络延迟,加快访问速度。 用户可以选择在不同的Region间实现异地容灾。当其中一个Region发生重大故障时，能够快速把业务迁移到另一个Region中。 多Availability Zone部署 如果，只是想在一个机房中部署OpenStack云环境。则只需要使用AZ方案即可。每个AZ有自己独立供电的机架，以及OpenStack计算节点。 Availability Zone 一个Region可以被细分为一个或多个物理隔离或逻辑隔离的availability zones（AZ）。启动虚拟机时，可以指定特定的AZ甚至特定AZ中的某一个节点来启动该虚拟机。AZ可以简单理解为一组节点的集合，这组节点具有独立的电力供应设备，比如一个个独立供电的机房，或一个个独立供电的机架都可以被划分成AZ。 然后将应用的多个虚拟机分别部署在Region的多个AZ上，提高虚拟机的容灾性和可用性。由于，AZ是物理隔离的，所以一个AZ挂了不会影响到其他的AZ。同时，还可以将挂了的AZ上的虚拟机，迁移到其他正常可用的AZ上，类似于异地双活。 Host Aggregate 除了AZ，计算节点也可以在逻辑上划分为主机聚合（Host Aggregates简称HA）。主机聚合使用元数据去标记计算节点组。一个计算节点可以同时属于一个主机聚合以及AZ而不会有冲突，它也可以属于多个主机聚合。 主机聚合的节点具有共同的属性，比如：cpu是特定类型的一组节点，disks是ssd的一组节点，os是linux或windows的一组节点等等。需要注意的是，Host Aggregates是用户不可见的概念，主要用来给nova-scheduler通过某一属性来进行instance的调度，比如讲数据库服务的 instances都调度到具有ssd属性的Host Aggregate中，又或者让某个flavor或某个image的instance调度到同一个Host Aggregates中。 简单的来说，Region、Availability Zone和Host Aggregate这三者是从大范围到小范围的关系，即前者包含了后者。一个地理区域Region包含多个可用区AZ (availability zone)，同一个AZ中的计算节点又可以根据某种规则逻辑上的组合成一个组。例如在北京有一个Region，成都有一个Region，做容灾之用。同时，在北京Region下，有2个AZ可用区（如酒仙桥机房和石景山机房），每个AZ都有自己独立的网络和供电设备，以及OpenStack计算节点等，如果用户是在北京，那么用户在部署VM的时候选择北京，可以提高用户的访问速度和较好的SLA（服务等级协议）。 备份你的数据如果因为某些原因，没有跨物理机房或地区的Region和AZ。那么OpenStack云平台相关的数据备份，则是必须要做的。比如MySQL数据库等，可以根据实际需求，每隔几小时进行一次备份。而备份的数据，建议存放到其他机器上。 使用合适的Docker存储如果，OpenStack云平台是用kolla容器化部署和管理的。那么选择一个正确、合适的Docker存储，关乎你的平台稳定和性能。 Docker 使用存储驱动来管理镜像每层内容及可读写的容器层，存储驱动有 devicemapper、aufs、overlay、overlay2、btrfs、zfs 等，不同的存储驱动实现方式有差异，镜像组织形式可能也稍有不同，但都采用栈式存储，并采用 Copy-on-Write(CoW) 策略。且存储驱动采用热插拔架构，可动态调整。那么，存储驱动那么多，该如何选择合适的呢？大致可从以下几方面考虑： 若内核支持多种存储驱动，且没有显式配置，Docker 会根据它内部设置的优先级来选择。优先级为 aufs &gt; btrfs/zfs &gt; overlay2 &gt; overlay &gt; devicemapper。若使用 devicemapper 的话，在生产环境，一定要选择 direct-lvm, loopback-lvm 性能非常差。 选择会受限于 Docker 版本、操作系统、系统版本等。例如，aufs 只能用于 Ubuntu 或 Debian 系统，btrfs 只能用于 SLES （SUSE Linux Enterprise Server, 仅 Docker EE 支持）。 有些存储驱动依赖于后端的文件系统。例如，btrfs 只能运行于后端文件系统 btrfs 上。 不同的存储驱动在不同的应用场景下性能不同。例如，aufs、overlay、overlay2 操作在文件级别，内存使用相对更高效，但大文件读写时，容器层会变得很大；devicemapper、btrfs、zfs 操作在块级别，适合工作在写负载高的场景；容器层数多，且写小文件频繁时，overlay2 效率比 overlay更高；btrfs、zfs 更耗内存。 Docker 容器其实是在镜像的最上层加了一层读写层，通常也称为容器层。在运行中的容器里做的所有改动，如写新文件、修改已有文件、删除文件等操作其实都写到了容器层。存储驱动决定了镜像及容器在文件系统中的存储方式及组织形式。 在我们的生产环境中，使用的是Centos 7.4系统及其4.15内核版本+Docker 1.13.1版本。因此使用的是overlay2存储。下面对overlay2作一些简单介绍。 Overlay介绍 OverlayFS 是一种类似 AUFS 的联合文件系统，但实现更简单，性能更优。OverlayFS 严格来说是 Linux 内核的一种文件系统，对应的 Docker 存储驱动为 overlay 或者 overlay2，overlay2 需 Linux 内核 4.0 及以上，overlay 需内核 3.18 及以上。且目前仅 Docker 社区版支持。条件许可的话，尽量使用 overlay2，与 overlay 相比，它的 inode 利用率更高。 和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的容器层和镜像层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： 分析 从kernel 3.18进入主流Linux内核。设计简单，速度快，比AUFS和Device mapper速度快。在某些情况下，也比Btrfs速度快。是Docker存储方式选择的未来。因为OverlayFS只有两层，不是多层，所以OverlayFS “copy-up”操作快于AUFS。以此可以减少操作延时。 OverlayFS支持页缓存共享，多个容器访问同一个文件能共享一个页缓存，以此提高内存使用率。 OverlayFS消耗inode，随着镜像和容器增加，inode会遇到瓶颈。Overlay2能解决这个问题。在Overlay下，为了解决inode问题，可以考虑将/var/lib/docker挂在单独的文件系统上，或者增加系统inode设置。 使用分布式存储如果OpenStack云平台使用开源的分布式存储系统，如Ceph、GlusterFS等。如何保证存储系统的冗余容灾性、可靠性、安全性和性能，便显得尤为重要。这里，以Ceph开源分布式存储为例进行讲解。 Mon节点和OSD节点部署一般地，在生产环境中，至少需要部署有3个Ceph Mon节点（数量最好为奇数）以及多个OSD节点。 开启CephX认证同时，开启CephX认证方式，以提高数据存储的安全性，防范被攻击。如下所示。123456789# cat /etc/ceph/ceph.conf [global]fsid = e10d7336-23e8-4dac-a07a-d012d9208ae1mon_initial_members = computer1, computer2, computer3mon_host = 172.17.51.54,172.17.51.55,172.17.51.56auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx……… 网络配置如果Ceph节点少于90台，建议Ceph公共网络（即Public Network）使用千兆网络，集群网络（即Cluster Network）使用万兆网络。如果Ceph节点多于90台，且业务负载较高，则尽量都使用万兆网络，在重要的环境上，Ceph公共网络和集群网络，都应该单独分开。需要注意的是，Ceph存储节点使用的网卡，必须要做网卡Bond，防止网卡因故障而导致网络中断。 使用Cache Tier在一个云存储环境中，出于成本的考虑，基本会少量使用SSD硬盘，大量使用SATA硬盘。在OpenStack集成Ceph的云环境中，如何使用SSD和SATA硬盘。一般有两种使用方法。 第一种：分别创建独立的SSD和SATA存储资源集群。然后，Cinder块存储服务对接这两套Ceph后端存储，这样云平台便可以同时创建和使用SSD介质和SATA介质的云硬盘。 第二种：使用SSD硬盘创建容量相对较小但性能高的缓存池（Cache tier），SATA硬盘创建容量大的但性能较低的存储池（Storage tier）。 以第二种方式为例进行讲解。当客户端访问操作数据时，会优先读写cache tier数据(当然要根据cache mode来决定)，如果数据在storage tier 则会提升到cache tier中，在cache tier中会有请求命中算法、缓存刷写算法、缓存淘汰算法等，将热数据提升到cache tier中，将冷数据下放到storage tier中。 缓存层代理自动处理缓存层和后端存储之间的数据迁移。在使用过程中，我们可以根据自己的需要，来配置迁移规则，主要有两种场景： 回写模式： 管理员把缓存层配置为 writeback 模式时， Ceph 客户端们会把数据写入缓存层、并收到缓存层发来的 ACK ；写入缓存层的数据会被迁移到存储层、然后从缓存层刷掉。直观地看，缓存层位于后端存储层的“前面”，当 Ceph 客户端要读取的数据位于存储层时，缓存层代理会把这些数据迁移到缓存层，然后再发往 Ceph 客户端。从此， Ceph 客户端将与缓存层进行 I/O 操作，直到数据不再被读写。此模式对于易变数据来说较理想（如照片/视频编辑、事务数据等）。 只读模式： 管理员把缓存层配置为 readonly 模式时， Ceph 直接把数据写入后端。读取时， Ceph 把相应对象从后端复制到缓存层，根据已定义策略、脏对象会被缓存层踢出。此模式适合不变数据（如社交网络上展示的图片/视频、 DNA 数据、 X-Ray 照片等），因为从缓存层读出的数据可能包含过期数据，即一致性较差。对易变数据不要用 readonly 模式。 独立使用PoolCeph可以统一OpenStack Cinder块存储服务（cinder-volume、cinder-backup）、Nova计算服务和Glance镜像服务的后端存储。在生产环境上，建议单独创建4个存储资源池（Pool）以分别对应OpenStack的4种服务存储。同时，每个Pool的副本数建议设置为3份，如下表所示。 Openstack服务 Ceph存储池 认证用户 Cinder-volumes volumes cinder Cinder-backups backups cinder Nova vms cinder Glance images cinder、glance 最后，Ceph分布式存储部署架构，如下图所示。 用户应用层在相当多的业务中，都会涉及到服务高可用。而一般的高可用的实现都是通过VIP(Vitrual IP)实现。VIP不像IP一样，对应一个实际的网络接口（网卡），它是虚拟出来的IP地址，所以利用其特性可以实现服务的容错和迁移工作。 在常见节点中VIP配置非常简单，没有多大的限制。但OpenStack实例中，一个IP对应一个Port设备。并且Neutron 有“Allowed address pairs”限制，该功能要求 Port 的MAC/IP 相互对应，那么该IP才能连通。对Port设备的进行操作可以实现下面几个功能： 一个Port设备添加多组Allowed address Pairs，允许多个IP通过该Port连通。 一个IP对应多组MAC地址。 一个MAC地址对应多个IP 另外在OpenStack创建的实例中建立VIP并使其能正常工作可以使用下面方法： 创建VIP的Port设备(防止该VIP被再次分配) 更新Port设备的Allowed address pairs 第一步，创建Port设备123456#source admin-openrc.sh //导入租户环境变量#openstack network list //查看现有网络，从中选择创建port设备的网络#openstack subnet list //查看网络下存在子网，从中选择port设备所处子网#openstack port create --network NetWork_Name --fixed-ip subnet=SubNet_Name,\ip-address=IP Port_Name#openstack port show Port_Name 此时Port设备已经创建，但该Port设备与需要建立VIP的实例没有任何关系，在该实例上创建VIP也是不能工作的。原因在于下面1#neutron port-list |grep Instance-IP //找到需要配置VIP的实例的Port ID 查看该Port的详细信息1#neutron port-show 17b580e8-1733-4e2e-b248-cde4863f4985 此时的allowed_address_pairs为空，就算在该实例中创建VIP，其MAC/IP也是不对应，不能工作的。那么就要进行第二步，即更新Port的allowed_address_pairs信息1#neutron port-update Port-ID --allowed_address_pair list-true type=dict ip_address=IP 例如12#neutron port-update 17b580e8-1733-4e2e-b248-cde4863f4985 \--allowed_address_pairs list=true type=dict ip_address=172.24.1.202 现在再来查看实例Port信息1#neutron port-show 17b580e8-1733-4e2e-b248-cde4863f4985 此时在虚拟机中创建VIP，就能够正常工作了。 运维平台建设监控是整个运维乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供详实的数据用于追查定位问题。目前业界有很多不错的开源产品可供选择。选择一些开源的监控系统，是一个省时省力，效率最高的方案。 使用Kolla容器化部署和管理OpenStack云平台，已成为主流趋势。这里，我们以容器化部署和管理OpenStack云平台为背景，聊聊云平台相关的运维平台建设。 监控目标我们先来了解什么是监控、监控的重要性以及监控的目标，当然每个人所在的行业不同、公司不同、业务不同、岗位不同，对监控的理解也不同，但是我们需要注意，监控是需要站在公司的业务角度去考虑，而不是针对某个监控技术的使用。 监控的目标，包括： 1）对系统不间断实时监控：实际上是对系统不间断的实时监控(这就是监控)；2）实时反馈系统当前状态：我们监控某个硬件、或者某个系统，都是需要能实时看到当前系统的状态，是正常、异常、或者故障；3）保证服务可靠性安全性：我们监控的目的就是要保证系统、服务、业务正常运行；4）保证业务持续稳定运行：如果我们的监控做得很完善，即使出现故障，能第一时间接收到故障报警，在第一时间处理解决，从而保证业务持续性的稳定运行； 监控体系分层 监控有赖于运维各专业条线协同完善，通过将监控体系进行分层、分类，各专业条线再去有重点的丰富监控指标。监控的对象，主要有基础设施硬件类和应用软件类等，如下图所示： 硬件设施层：交换机、路由器、负载均衡设备、防火墙、服务器（硬盘、CPU、内存和网卡）等。 云平台层：日志、数据库、消息队列、操作系统、OpenStack服务、Ceph存储、Docker容器、系统和应用负载等。 应用层：虚拟机、数据卷、虚拟网卡等。 监控手段通常情况下，随着系统的运行，操作系统会产生系统日志，应用程序会产生应用程序的访问日志、错误日志、运行日志、网络日志，我们可以使用 EFK 来进行日志监控。对于日志监控来说，最常见的需求就是收集、存储、查询、展示。 除了对日志进行监控外，我们还需要对系统和应用的运行状况进行实时监控。不同的监控目标，有不同的监控手段。OpenStack云资源的监控，如虚拟机、镜像、数据卷、虚拟网卡等，天然的可以由OpenStack自带的Ceilometer+Gnocchi+Aodh等服务来做（PS：ceilometer可以将采集数据交给gnocchi做数据聚合，最后用grafana来出图）。 如果，OpenStack云平台是基于Kolla容器化部署和运行管理的。那么诸如Docker容器、操作系统负载、存储空间等，又该使用什么来运维监控并告警呢。自然，TPG栈便呼之欲出了（不要问我为啥不用Zabbix）。 什么是TPIG栈。即由Telegraf + Influxdb + Grafana + Prometheus组合成的一套运维监控工具集合。它们之间的关系是。Prometheus/Telegraf(收集数据) —-&gt; Influxdb(保存数据) —-&gt; Grafana(显示数据) 说明：Prometheus和Telegraf不是必须同时部署使用的，你可以根据自己的需要，选择二者都部署，也可以二者选其一。 如下几种开源工具或方案，Kolla社区都是默认支持的。最重要的是，如何去使用、完善它们。 日志收集和分析处理的开源方案有EFK栈：fluentd/filebeat + elasticsearch +kibana 性能采集和分析处理的开源方案有TPIG栈：telegraf + influxdb + grafana + Prometheus 监控方法 了解监控对象：我们要监控的对象你是否了解呢？比如硬盘的IOPS？对象性能指标：我们要监控这个东西的什么属性？比如 CPU 的使用率、负载、用户态、内核态、上下文切换。报警阈值定义：怎么样才算是故障，要报警呢？比如 CPU 的负载到底多少算高，用户态、内核态分别跑多少算高？故障处理流程：收到了故障报警，我们怎么处理呢？有什么更高效的处理流程吗？ 监控流程 数据采集：通过telegraf/Prometheus等对系统和应用进行数据采集； 数据存储：监控数据存储在MySQL、influxdb上，也可以存储在其他数据库中； 数据分析：当我们事后需要分析故障时，EFK栈 能给我们提供图形以及时间等相关信息，方面我们确定故障所在； 数据展示：web 界面展示； 监控报警：电话报警、邮件报警、微信报警、短信报警、报警升级机制等（无论什么报警都可以）； 报警处理：当接收到报警，我们需要根据故障的级别进行处理，比如:重要紧急、重要不紧急等。根据故障的级别，配合相关的人员进行快速处理； 监控告警当监控的对象超过了某一阈值或者某一服务出现了异常时，便自动发送邮件、短信或微信给相关人员进行告警。 建立集中监控平台 在一体化运维体系中，监控平台贯穿所有环节，它起到了生产系统涉及的软硬件环境实时运行状况的“监”，监控平台事件驱动的特性也为一体化运维体系起到神经网络驱动的作用，进而进行了“控”，另外，监控平台优质的运维数据可以作为运维大数据分析的数据源，实现运维数据采集的角色。为了提高投入效率，减少重复投入，需要建立集中监控平台实现统一展示、统一管理，支持两地三中心建设，具备灵活的扩展性，支持运维大数据分析。 指标权重与阀值分级 需要重点强调一下监控指标的指标权重、阀值分级与上升机制问题，做监控的人知道“监”的最重要目标是不漏报，为了不漏报在实际实施过程中会出现监控告警过多的困难。如何让运维人员在不漏处理监控事件，又能快速解决风险最高的事件，则需要监控的指标需要进行指标权重、阀值分级与上升机制： 1）指标权重： 监控指标的权重是为了定义此项监控指标是否为必须配置，比如应用软件服务、端口监听是一个应用可用性的重要指标，权重定义为一级指标；对于批量状态，则由于不少应用系统并没有批量状态，则定义为二级指标。通常来说一级指标将作为监控覆盖面的底线，通过设置好权重，一是为了让运维人员知道哪些监控指标必须确保覆盖，同时加以引入KPI考核；二是为了让监控平台建设人员有侧重的优化，实现一级指标的自动配置，无需运维人员手工配置。 2）阀值分级与上升机制： 有监控指标，就需要针对监控指标定义阀值，监控阀值的设立需要有分级机制，以分通知、预警、告警三级为例：通知需要运维人员关注，比如“交易系统登录数2000，登录成功率95%，平时登录数基线500，登录成功率96%”，由于登录成功率并未明显下降，可能是由于业务作了业务推广，运维人员只需关注当前应用运行状态再做判断；预警代表监控事件需要运维人员处理，但重要性略低，比如“CPU使用率71%，增长趋势非突增”，管理员受理到这个预警可以先设置为一个维护期，待当天找个时间集中处理；告警则必须马上处理的事件，比如“交易成功率为10%，平时为90%”这类监控事件己反映出交易运行问题。 对于升级，是指一个预警当长时间未处理时，需要有一个上升机制，转化为告警，以督办运维人员完成监控事件的处理。 事件分级标准 前面提到了事件分级的问题，分级是将事件当前紧急程度进行标识显示，事件升级是对于低级的事件当达到一定的程度，比如处理时间过长，则需要进行升级。我们将监控事件等级事件级别分为通知、预警、故障三种： 通知：指一般的通知信息类事件。 预警：指已经出现异常，即将要引起生产故障的事件。 故障：指已经发生问题，并且已经影响到生产流程的事件，如果需要进一步细化故障级别，可以分为一般故障和紧急故障：一般故障不需要紧急处理的故障，紧急故障需要管理员紧急处理的故障。事件细分的粒度需根据各企业团队的管理要求而定。 事件应急响应 运维最基本的指标就是保证系统的可用性，应急恢复的时效性是系统可用性的关键指标。通常来讲应急恢复的方法有不少，比如： 服务整体性能下降或异常，可以考虑重启服务； 应用做过变更，可以考虑是否需要回切变更； 资源不足，可以考虑应急扩容； 应用性能问题，可以考虑调整应用参数、日志参数； 数据库繁忙，可以考虑通过数据库快照分析，优化SQL； 应用功能设计有误，可以考虑紧急关闭功能菜单； 还有很多…… 问题处理上面，我们了解到了监控目标、监控手段、监控告警、监控方法和流程之后，我们也更需要知道监控的核心是什么。即1）发现问题：当系统发生故障报警，我们会收到故障报警的信息 ；2）定位问题：故障邮件一般都会写某某主机故障、具体故障的内容，我们需要对报警内容进行分析，比如一台服务器连不上：我们就需要考虑是网络问题、还是负载太高导致长时间无法连接，又或者某开发触发了防火墙禁止的相关策略等等，我们就需要去分析故障具体原因；3）解决问题：当然我们了解到故障的原因后，就需要通过故障解决的优先级去解决该故障；4）总结问题：当我们解决完重大故障后，需要对故障原因以及防范进行总结归纳，避免以后重复出现； 最后 关于，如何具体的使用EFK栈和TPG栈监控和采集OpenStack云平台的Log日志和性能数据实现一体化的运维监控告警，将在后面进行专题分享。 参考链接：http://www.yunweipai.com/archives/13243.html]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>Kolla</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何获取Kolla的OpenStack镜像]]></title>
    <url>%2F2018%2F05%2F07%2F%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96Kolla%E7%9A%84OpenStack%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[由于，OpenStack社区自Queens版本起，便不再提供将打包好的kolla openstack镜像放在该链接上 所以，我们要获取Kolla的OpenStack镜像，就只能依靠自己手动获取。有如下几种方法。 从官方源下载镜像安装kolla-ansible12# git clone https://github.com/openstack/kolla-ansible -b stable/queens# pip install kolla-ansible/ 编辑globals.yml文件，设置相关参数1234# vim /etc/kolla/globals.ymlkolla_install_type: "source"kolla_base_distro: "centos"openstack_release: "queens" 下载镜像12# kolla-ansible -i all-in-one bootstrap-servers# kolla-ansible pull 手动构建镜像下载kolla项目1# git clone https://github.com/openstack/kolla.git -b stable/queens 生成kolla-build.conf文件123# pip install tox# cd kolla/# tox -e genconfig 构建openstack镜像12345//构建基于centos系统的source源码安装的openstack镜像# kolla-build -t source -b centos//或者，构建基于centos系统的binary二进制包安装的openstack镜像# kolla-build -t binary -b centos 自动化拉取kolla镜像由于OpenStack社区，已经开始正式将kolla镜像托管在DockerHub上。所以，我们还可以从Docker Hub上直接拉取kolla镜像，由于openstack镜像少则几十，多则上百，因此，这里我编写了一个bash脚本，用于自动化拉取queens版本的kolla镜像。 为了加快从docker hub上拉取镜像，这里，配置上阿里云的镜像加速器。1234# cat /etc/docker/daemon.json &#123;"registry-mirrors": ["https://a5aghnme.mirror.aliyuncs.com"]&#125; 重启docker服务，生效1# systemctl daemon-reload &amp;&amp; systemctl restart docker 拉取queens版本的kolla镜像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153# cat 01_pull_kolla.sh #!/usr/bin/bashimage_tag=queens #该变量，你可以根据自己需要进行修改# delete imagesdocker images | awk '&#123;print $3&#125;' | xargs docker rmi -f# pull public imagesfor public_images in memcached kolla-toolbox cron mongodb mariadb rabbitmq keepalived haproxy chrony iscsid tgtddo docker pull kolla/centos-source-$public_images:$image_tagdone# pull monitor manage images# it is recommended to use telegraf + influxdb + grafana + collectd + Prometheusfor monitor_images in collectd telegraf grafana influxdb prometheus-server prometheus-haproxy-exporter prometheus-node-exporter prometheus-mysqld-exporter prometheus-memcached-exporter prometheus-cadvisordo docker pull kolla/centos-source-$monitor_images:$image_tagdone# pull log manage imagesfor log_images in fluentd elasticsearch kibanado docker pull kolla/centos-source-$log_images:$image_tagdone# pull novafor nova in nova-compute nova-consoleauth nova-ssh nova-placement-api nova-api nova-compute-ironic nova-consoleauth nova-serialproxy nova-scheduler nova-novncproxy nova-conductor nova-libvirtdo docker pull kolla/centos-source-$nova:$image_tagdone# pull keystonefor keystone in keystone keystone-fernet keystone-sshdo docker pull kolla/centos-source-$keystone:$image_tagdone# pull freezerdocker pull kolla/centos-source-freezer-api:$image_tag# pull glance for glance in glance-api glance-registrydo docker pull kolla/centos-source-$glance:$image_tagdone# pull cinderfor cinder in cinder-volume cinder-api cinder-backup cinder-schedulerdo docker pull kolla/centos-source-$cinder:$image_tagdone# pull neutronfor neutron in neutron-server neutron-lbaas-agent neutron-dhcp-agent neutron-l3-agent neutron-openvswitch-agent neutron-metadata-agent neutron-server-opendaylight opendaylightdo docker pull kolla/centos-source-$neutron:$image_tagdone# pull openvswitchfor openvswitch in openvswitch-db-server openvswitch-vswitchd neutron-openvswitch-agentdo docker pull kolla/centos-source-$openvswitch:$image_tagdone# pull ceilometerfor ceilometer in ceilometer-api ceilometer-compute ceilometer-notification ceilometer-centraldo docker pull kolla/centos-source-$ceilometer:$image_tagdone# pull gnocchifor gnocchi in gnocchi-metricd gnocchi-api gnocchi-statsddo docker pull kolla/centos-source-$gnocchi:$image_tagdone# pull aodhfor aodh in aodh-evaluator aodh-api aodh-listener aodh-notifierdo docker pull kolla/centos-source-$aodh:$image_tagdone# pull heatfor heat in heat-api heat-api-cfn heat-enginedo docker pull kolla/centos-source-$heat:$image_tagdone# pull horizondocker pull kolla/centos-source-horizon:$image_tag# pull muranofor murano in murano-api murano-enginedo docker pull kolla/centos-source-$murano:$image_tagdone# pull magnumfor magnum in magnum-api magnum-conductordo docker pull kolla/centos-source-$magnum:$image_tagdone# pull senlinfor senlin in senlin-api senlin-enginedo docker pull kolla/centos-source-$senlin:$image_tagdone# pull saharafor sahara in sahara-engine sahara-apido docker pull kolla/centos-source-$sahara:$image_tagdone# pull trovefor trove in trove-api trove-taskmanager trove-conductordo docker pull kolla/centos-source-$trove:$image_tagdone# pull swiftfor swift in swift-rsyncd swift-proxy-server swift-object-expirer swift-object swift-account swift-container swift-basedo docker pull kolla/centos-source-$swift:$image_tagdone# pull ironicfor ironic in ironic-conductor ironic-pxe ironic-api ironic-inspectordo docker pull kolla/centos-source-$ironic:$image_tagdonedocker pull kolla/centos-source-dnsmasq:pikedocker tag kolla/centos-source-dnsmasq:pike kolla/centos-source-dnsmasq:queensdocker rmi -f kolla/centos-source-dnsmasq:pike# pull cloudkittyfor cloudkitty in cloudkitty-api cloudkitty-processor panko-apido docker pull kolla/centos-source-$cloudkitty:$image_tagdone# pull kuryrdocker pull kolla/centos-source-kuryr-libnetwork:$image_tag# save imagesimages=`docker images | grep queens | awk '&#123;print $1&#125;'`docker save -o kolla_queens_images.tar $images# clean pull's imagesdocker images | awk '&#123;print $3&#125;' | xargs docker rmi -f 将镜像push到本地Registry12345678910111213141516171819202122232425262728# cat 02_push_kolla.sh #!/usr/bin/bash# load imagesdocker load --input kolla_queens_images.tarregistry=172.17.51.27:4000 #请将该registry地址，改为你自己环境的地址# tag imagesimages=`docker images | grep queens | awk '&#123;print $1&#125;'`for images_tag in $imagesdo docker tag $images_tag:queens $registry/$images_tag:queensdone# delete old's imagesdelete_images=`docker images | grep '^kolla' | awk '&#123;print $1&#125;'`for delete_images1 in $delete_imagesdo docker rmi $delete_images1:queensdone# push imagespush_images=`docker images | grep queens | awk '&#123;print $1&#125;'`for push_images1 in $push_imagesdo docker push $push_images1:queensdone 综合，比较以上三种方法的优缺点。这里，推荐使用第三种方法，速度更快，也更便捷。PS：脚本写得有点搓，不喜可喷。]]></content>
      <categories>
        <category>OpenStack</category>
        <category>Kolla</category>
      </categories>
      <tags>
        <tag>Kolla</tag>
        <tag>OpenStack</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设置OpenStack节点Swap分区]]></title>
    <url>%2F2018%2F05%2F07%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEOpenStack%E8%8A%82%E7%82%B9Swap%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[Swap分区介绍Swap分区，即交换分区。它的功能就是在物理内存不够的情况下，操作系统先把内存中暂时不用的数据，存到硬盘的交换空间，腾出内存来让别的程序运行，当程序需要用到交换空间内的数据的时候，操作系统再将数据从交换分区恢复到物理内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 如何设置Swap分区大小以上是SWAP 交换分区的作用。 实际上，我们更关注的应该是SWAP分区的大小问题。 设置多大才是最优的。如下，提供了两种方案。 方案一在Linux系统中，我们可以参照Red Hat公司为RHEL 7推荐的SWAP空间的大小划分原则，在你没有其他特别需求时，可以作为很好的参考依据。 内存小于2GB，推荐2倍于内存的swap空间； 内存2GB~8GB，推荐和内存大小一样的swap空间； 内存8GB~64GB，推荐至少4GB的swap空间； 内存大于64GB，推荐至少4GB的swap空间。 原文链接 实际上，系统中交换分区的大小并不取决于物理内存的量，而是取决于系统中内存的负荷，所以在安装系统时要根据具体的业务来设置SWAP的值。 在OpenStack中，默认的CPU超配比例是1:16，内存超配比例是1:1.5。当宿主机使用swap交换分区来为虚拟机分配内存的时候，则虚拟机的性能将急速下降。生产环境上不建议开启内存超售（建议配置比例1:1）。另外，建议设置nova.conf文件中的reserved_host_memory_mb 参数，即内存预留量（建议至少预留4GB），保证该部分内存不能被虚拟机使用。 方案二系统在什么情况下才会使用Swap？实际上，并不是等所有的物理内存都消耗完毕之后，才去使用swap的空间，什么时候使用是由swappiness 参数值控制的。12# cat /proc/sys/vm/swappiness60 该值默认值是60。 swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间。 swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。 由于，现在服务器的内存一般是上百GB，所以我们可以把这个参数值设置的低一些（如10-30之间），让操作系统尽可能的使用物理内存，降低系统对swap的使用，从而提高宿主机系统和虚拟机的性能。 永久性修改1# echo 'vm.swappiness=10' &gt;&gt;/etc/sysctl.conf 保存，重启就生效了。 查看系统当前SWAP 空间大小1# free –h 小结 为了保证主机系统和应用程序的稳定运行（内存不足或泄露，易导致系统或应用崩溃），建议在实际使用过程中，服务器仍然需要创建一定的Swap分区。其分区大小可以结合以上两种方案进行设置，已达到最佳效果。 FAQ1.释放SWAP 空间 假设我们的系统出现了性能问题，我们通过vmstat命令看到有大量的swap，而我们的物理内存又很充足，那么我们可以手工把swap 空间释放出来。让进程去使用物理内存，从而提高性能。 我们对swap 空间的释放，可以通过关闭swap分区，再启动swap 分区来实现。123# vmstat 1 5 // 1表示每隔1秒采集一次服务器状态，5表示只采集5次# free -h# swapon -s //显示交换分区的使用状况 关闭swap 交换分区：12# swapoff /dev/sda2# swapon -s 启用swap分区：1# swapon /dev/sda2 验证状态：1# swapon -s Swap分区的拓展和缩小：https://www.e-learn.cn/content/linux/339010]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kolla中配置OpenStack虚机网络vxlan和vlan共存]]></title>
    <url>%2F2018%2F05%2F03%2FKolla%E4%B8%AD%E9%85%8D%E7%BD%AEOpenStack%E8%99%9A%E6%9C%BA%E7%BD%91%E7%BB%9Cvxlan%E5%92%8Cvlan%E5%85%B1%E5%AD%98%2F</url>
    <content type="text"><![CDATA[OpenStack Neutron网络服务定义了四种网络模式：1234# tenant_network_type = local# tenant_network_type = vlan # tenant_network_type = gre# tenant_network_type = vxlan 这里，本文以vlan、vxlan为例，阐述如何实现OpenStack虚机网络（亦称租户网络、业务网络）同时支持vxlan和vlan两种网络。 说明 环境：Openstack queens版本 部署工具：kolla-ansible 在kolla-ansible部署节点的/etc/kolla/globals.yml文件中，配置网卡。如下所示。 eth0：openstack管理网络；vlan 51，交换机端口设置为Access模式 eth1：虚机网络(vxlan)；vlan 52，交换机端口设置为Access模式 eth2：外部网络兼虚机网络(vlan)；vlan网段53-54，交换机端口设置为trunk模式，主机不配置IP地址 在所有网络节点上，操作如下修改文件/etc/kolla/neutron-server/ml2_conf.ini 修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini 重启neutron容器1# docker restart neutron_server neutron_openvswitch_agent 在网络节点上，查看br-ex网桥设置情况，如下。 在所有计算节点上，操作如下修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini 创建一个br-ex外部网桥，并关联到主机的eth2物理网卡上。这样，当计算节点上的虚拟机使用vlan网络时，便可以直接通过qbr-&gt;br-int-&gt;br-ex-&gt;eth2连接到外网。（vlan网络的三层路由，建议使用物理路由器，这样性能和稳定性更好，而不需要通过网络节点上的L3 vRouter虚拟路由）。12# docker exec -u root -it neutron_openvswitch_agent ovs-vsctl add-br br-ex# docker exec -u root -it neutron_openvswitch_agent ovs-vsctl add-port br-ex eth2 最后，重启相关容器1# docker restart neutron_openvswitch_agent 在计算节点上，查看br-ex网桥设置情况，如下。 创建一个vlan id为53的网段123# neutron net-create vlan-53 --shared --provider:physical_network physnet1 --provider:network_type vlan --provider:segmentation_id 53# neutron subnet-create vlan-53 172.17.53.0/24 --name provider-53-subnet --gateway 172.17.53.1 查看创建的网络，如下。12345678# neutron net-list+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+| id | name | tenant_id | subnets |+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+| 5d9c4874-e03b-4bde-aee0-947d7dde4860 | vlan-53 | 48fbadff0ab84229b429166babbe488f | 9bade37c-ff44-4004-8e82-20d61348fdc0 172.17.53.0/24 || 7b0152da-a975-4dbf-b35b-437951c66efa | tenant_network | 48fbadff0ab84229b429166babbe488f | a45516a4-4ce9-4c2e-8052-8c71eae0e219 10.0.0.0/24 || 9630cf8b-4072-415b-a9a9-99ff815748f8 | public_network | 48fbadff0ab84229b429166babbe488f | a98f8c80-78de-43ba-af52-d86c19fc59ef 172.17.54.0/24 |+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+ 最后，创建一个虚拟机并使用该vlan网络。1234# nova boot --flavor 1Gmem_1cpu --image centos7 --nic net-id=5d9c4874-e03b-4bde-aee0-947d7dde4860 test_vm# nova list | grep test_vm| f506129b-610f-4e2d-886b-5d791cdcb282 | test_vm | ACTIVE | - | Running | vlan-53=172.17.53.7 测试虚拟机网络通信123456# ping -c 4 172.17.53.7PING 172.17.53.7 (172.17.53.7) 56(84) bytes of data.64 bytes from 172.17.53.7: icmp_seq=1 ttl=63 time=0.421 ms64 bytes from 172.17.53.7: icmp_seq=2 ttl=63 time=0.503 ms64 bytes from 172.17.53.7: icmp_seq=3 ttl=63 time=0.543 ms64 bytes from 172.17.53.7: icmp_seq=4 ttl=63 time=0.469 ms br-int和br-ex说明 br-int br-int是OpenVswitch中的集成网桥，类似于一个二层的交换机。上面挂载了大量的agent来提供各种网络服务，另外负责对发往br-ex的流量，实现local vlan转化为外部vlan。12345# ovs-ofctl dump-flows br-int NXST_FLOW reply (xid=0x4): cookie=0x0, duration=147294.121s, table=0, n_packets=224, n_bytes=33961, idle_age=13, hard_age=65534, priority=3,in_port=4,dl_vlan=1 actions=mod_vlan_vid:101,NORMAL cookie=0x0, duration=603538.84s, table=0, n_packets=19, n_bytes=2234, idle_age=18963, hard_age=65534, priority=2,in_port=4 actions=drop cookie=0x0, duration=603547.134s, table=0, n_packets=31901, n_bytes=6419756, idle_age=13, hard_age=65534, priority=1 actions=NORMAL br-ex br-ex是OpenVswitch中的一个外部网桥，要做的事情很简单，只需要正常转发数据流量即可。123# ovs-ofctl dump-flows br-ex NXST_FLOW reply (xid=0x4): cookie=0x0, duration=6770.969s, table=0, n_packets=5411, n_bytes=306944, idle_age=0, hard_age=65534, priority=0 actions=NORMAL]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>Kolla</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何删除Registry中kolla-ansible的镜像]]></title>
    <url>%2F2018%2F04%2F30%2F%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4Registry%E4%B8%ADkolla-ansible%E7%9A%84%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[出于某些情况，如释放磁盘空间、旧镜像删除等原因，需要我们删除本地Registry仓库中的镜像。本篇文章，将讲解如何在OpenStack环境的kolla-ansible中，删除本地Registry中的镜像。 Registry中的镜像管理查看Registry仓库中现有的镜像：1#curl -XGET http://172.17.51.51:4000/v2/_catalog 查看Registry仓库中指定的镜像，如这里的centos-source-magnum-conductor。1#curl -XGET http://172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/tags/list 如何删除私有 registry 中的镜像首先，在默认情况下，docker registry 是不允许删除镜像的，需要在配置文件config.yml中启用。123456789101112131415161718192021#vim /etc/docker/registry/config.ymlversion: 0.1log: fields: service: registrystorage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry delete: enabled: truehttp: addr: :5000 headers: X-Content-Type-Options: [nosniff]health: storagedriver: enabled: true interval: 10s threshold: 3 修改后，需要重启registry容器1#docker restart registry 使用API接口 GET /v2/&lt;镜像名&gt;/manifests/ 来取得要删除的镜像:Tag所对应的 digest。比如，要删除kolla/centos-source-magnum-conductor:queens镜像，那么取得 digest 的命令是：123456789#curl --header "Accept: application/vnd.docker.distribution.manifest.v2+json" -I -X HEAD http://172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/queensHTTP/1.1 200 OKContent-Length: 8666Content-Type: application/vnd.docker.distribution.manifest.v2+jsonDocker-Content-Digest: sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868Docker-Distribution-Api-Version: registry/2.0Etag: "sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868"X-Content-Type-Options: nosniffDate: Sat, 28 Apr 2018 02:44:46 GMT 得到 Docker-Content-Digest:1sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868 然后调用API接口 DELETE /v2/&lt;镜像名&gt;/manifests/ 来删除镜像。比如：1234567#curl -I -X DELETE http://172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868HTTP/1.1 202 AcceptedDocker-Distribution-Api-Version: registry/2.0X-Content-Type-Options: nosniffDate: Sat, 28 Apr 2018 03:34:31 GMTContent-Length: 0Content-Type: text/plain; charset=utf-8 至此，镜像已从 registry 中标记删除，外界访问 pull 不到了。但是 registry 的本地空间并未释放，需要垃圾收集才会释放。1#docker exec registry bin/registry garbage-collect /etc/docker/registry/config.yml]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Kolla</tag>
        <tag>OpenStack</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
