<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>起风了</title>
  
  <subtitle>xuchao&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-05-30T15:09:37.193Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>XuChao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>安装OpenStack Zaqar服务</title>
    <link href="http://yoursite.com/2018/05/26/%E5%AE%89%E8%A3%85OpenStack-Zaqar%E6%9C%8D%E5%8A%A1/"/>
    <id>http://yoursite.com/2018/05/26/安装OpenStack-Zaqar服务/</id>
    <published>2018-05-26T07:41:52.000Z</published>
    <updated>2018-05-30T15:09:37.193Z</updated>
    
    <content type="html"><![CDATA[<p>关于什么是Zaqar，有什么作用。国内已有介绍读者可以自行Google查阅。若在此再阐述，已显多余。由于安装Zaqar服务官方文档还有坑且国内无资料，故这里写下中文第一篇吧。</p><p><strong>依赖服务</strong></p><ul><li>一个基本的OpenStack正常运行环境</li><li>MongoDB</li><li>Memcache</li></ul><p><strong>说明</strong></p><p>本环境，MongoDB和memcache已有kolla-ansible事先安装好。所以，这里只需要安装配置zaqar服务即可。</p><p>1.创建keystone认证信息<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> admin-openrc.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack user create --domain default --password-prompt zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack role add --project service --user zaqar admin</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack service create --name zaqar --description <span class="string">"Messaging"</span> messaging</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging public http://172.17.223.21:8888</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging internal http://172.17.223.21:8888</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging admin http://172.17.223.21:8888</span></span><br></pre></td></tr></table></figure></p><p>2.安装zaqar，这里使用queens版本<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://git.openstack.org/openstack/zaqar.git -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install . -r ./requirements.txt --upgrade --<span class="built_in">log</span> /tmp/zaqar-pip.log</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install --upgrade pymongo gevent uwsgi</span></span><br></pre></td></tr></table></figure></p><p>3.创建zaqar配置目录<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> oslopolicy-sample-generator --config-file etc/oslo-config-generator/zaqar-policy-generator.conf</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp etc/zaqar.policy.yaml.sample /etc/zaqar/policy.yaml</span></span><br></pre></td></tr></table></figure></p><p>4.创建zaqar的log文件<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># touch /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br><span class="line"># chown zaqar:zaqar /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br><span class="line"># chmod <span class="number">600</span> /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br></pre></td></tr></table></figure></p><p>5.创建/srv/zaqar目录<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir /srv/zaqar</span></span><br><span class="line"><span class="comment"># vim /srv/zaqar/zaqar_uwsgi.py</span></span><br><span class="line"><span class="keyword">from</span> keystonemiddleware <span class="keyword">import</span> auth_token</span><br><span class="line"><span class="keyword">from</span> zaqar.transport.wsgi <span class="keyword">import</span> app</span><br><span class="line">app = auth_token.AuthProtocol(app.app, &#123;&#125;)</span><br></pre></td></tr></table></figure></p><p><strong>说明</strong></p><p>注意，下面的参数“listen = 1024”，超过了系统的默认值128，会报错。因此需要修改默认值，如这里的2048。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># echo "net.core.somaxconn=2048" | sudo tee --append /etc/sysctl.conf</span></span><br><span class="line"><span class="comment"># echo 2048 &gt; /proc/sys/net/core/somaxconn</span></span><br><span class="line"><span class="comment"># echo 2048 &gt; /proc/sys/net/ipv4/tcp_max_syn_backlog</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim /srv/zaqar/uwsgi.ini</span></span><br><span class="line"><span class="section">[uwsgi]</span></span><br><span class="line"><span class="attr">http</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">8888</span></span><br><span class="line"><span class="attr">pidfile</span> = /var/run/zaqar.pid</span><br><span class="line"><span class="attr">gevent</span> = <span class="number">2000</span></span><br><span class="line"><span class="attr">gevent-monkey-patch</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">listen</span> = <span class="number">1024</span></span><br><span class="line"><span class="attr">enable-threads</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">chdir</span> = /srv/zaqar</span><br><span class="line"><span class="attr">module</span> = zaqar_uwsgi:app</span><br><span class="line"><span class="attr">workers</span> = <span class="number">4</span></span><br><span class="line"><span class="attr">harakiri</span> = <span class="number">60</span></span><br><span class="line"><span class="attr">add-header</span> = Connection: close</span><br></pre></td></tr></table></figure></p><p>6.创建pid文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> touch /var/run/zaqar.pid</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chown zaqar:zaqar /var/run/zaqar.pid</span></span><br></pre></td></tr></table></figure></p><p>7.编辑zaqar配置文件<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/zaqar/zaqar.conf</span></span><br><span class="line"><span class="section">[DEFAULT]</span></span><br><span class="line"><span class="attr">pooling</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">admin_mode</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">debug</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">log_file</span> = /var/log/zaqar-server.log</span><br><span class="line"><span class="comment">;auth_strategy = keystone</span></span><br><span class="line"><span class="section">[keystone_authtoken]</span></span><br><span class="line"><span class="attr">auth_uri</span> = http://<span class="number">172.17</span>.<span class="number">223.20</span>:<span class="number">5000</span></span><br><span class="line"><span class="attr">auth_url</span> = http://<span class="number">172.17</span>.<span class="number">223.20</span>:<span class="number">35357</span></span><br><span class="line"><span class="attr">auth_type</span> = password</span><br><span class="line"><span class="attr">project_domain_id</span> = default</span><br><span class="line"><span class="attr">user_domain_id</span> = default</span><br><span class="line"><span class="attr">project_name</span> = service</span><br><span class="line"><span class="attr">username</span> = zaqar</span><br><span class="line"><span class="attr">password</span> = zaqar</span><br><span class="line"><span class="attr">memcache_security_strategy</span> = ENCRYPT</span><br><span class="line"><span class="attr">memcache_secret_key</span> = wRJgCSJjqUT5JWlGVyvSygovRiyXFgJg7kYz1sXX</span><br><span class="line"><span class="attr">memcached_servers</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">11211</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">11211</span></span><br><span class="line"><span class="section">[cache]</span></span><br><span class="line"><span class="attr">backend</span> = oslo_cache.memcache_pool</span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">memcache_servers</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">11211</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">11211</span></span><br><span class="line"><span class="section">[drivers]</span></span><br><span class="line"><span class="attr">transport</span> = wsgi</span><br><span class="line"><span class="attr">message_store</span> = mongodb</span><br><span class="line"><span class="attr">management_store</span> = mongodb</span><br><span class="line"><span class="section">[drivers:management_store:mongodb]</span></span><br><span class="line"><span class="attr">uri</span> = mongodb://<span class="number">172.17</span>.<span class="number">223.21</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">27017</span>/?replicaSet=rs0&amp;w=<span class="number">2</span>&amp;readPreference=secondaryPreferred</span><br><span class="line"><span class="attr">database</span> = zaqarmanagementstore</span><br><span class="line"><span class="attr">partitions</span> = <span class="number">8</span></span><br><span class="line"><span class="comment">;max_attempts = 1000</span></span><br><span class="line"><span class="comment">;max_retry_sleep = 0.1</span></span><br><span class="line"><span class="comment">;max_retry_jitter = 0.005</span></span><br><span class="line"><span class="comment">;gc_interval = 5 * 60</span></span><br><span class="line"><span class="comment">;gc_threshold = 1000</span></span><br><span class="line"><span class="section">[drivers:message_store:mongodb]</span></span><br><span class="line"><span class="attr">database</span> = zaqarmessagestore</span><br><span class="line"><span class="attr">uri</span> = mongodb://<span class="number">172.17</span>.<span class="number">223.21</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">27017</span>/?replicaSet=rs0&amp;w=<span class="number">2</span>&amp;readPreference=secondaryPreferred</span><br><span class="line"><span class="section">[drivers:transport:wsgi]</span></span><br><span class="line"><span class="attr">bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="section">[transport]</span></span><br><span class="line"><span class="attr">max_queues_per_page</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">max_queue_metadata</span> = <span class="number">262144</span></span><br><span class="line"><span class="attr">max_mesages_per_page</span> = <span class="number">10</span></span><br><span class="line"><span class="attr">max_messages_post_size</span> = <span class="number">262144</span></span><br><span class="line"><span class="attr">max_message_ttl</span> = <span class="number">1209600</span></span><br><span class="line"><span class="attr">max_claim_ttl</span> = <span class="number">43200</span></span><br><span class="line"><span class="attr">max_claim_grace</span> = <span class="number">43200</span></span><br><span class="line"><span class="section">[signed_url]</span></span><br><span class="line"><span class="attr">secret_key</span> = SOMELONGSECRETKEY</span><br></pre></td></tr></table></figure></p><p>8.编辑uwsgi服务启动文件<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/systemd/system/zaqar.uwsgi.service</span></span><br><span class="line"><span class="section">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=uWSGI Zaqar</span><br><span class="line"><span class="attr">After</span>=syslog.target</span><br><span class="line"><span class="section">[Service]</span></span><br><span class="line"><span class="attr">ExecStart</span>=/usr/bin/uwsgi --ini /srv/zaqar/uwsgi.ini</span><br><span class="line"><span class="comment"># Requires systemd version 211 or newer</span></span><br><span class="line"><span class="attr">RuntimeDirectory</span>=uwsgi</span><br><span class="line"><span class="attr">Restart</span>=always</span><br><span class="line"><span class="attr">KillSignal</span>=SIGQUIT</span><br><span class="line"><span class="attr">Type</span>=notify</span><br><span class="line"><span class="attr">StandardError</span>=syslog</span><br><span class="line"><span class="attr">NotifyAccess</span>=all</span><br><span class="line"><span class="attr">User</span>=zaqar</span><br><span class="line"><span class="attr">Group</span>=zaqar</span><br><span class="line"><span class="section">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=multi-user.target</span><br></pre></td></tr></table></figure></p><p>启动uwsgi服务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl start zaqar.uwsgi.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> zaqar.uwsgi.service</span></span><br></pre></td></tr></table></figure></p><p>说明，如果报错可以查看日志。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tailf /var/<span class="built_in">log</span>/messages</span></span><br></pre></td></tr></table></figure></p><p>9.配置Pool</p><p>生成一个UUID<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># uuidgen</span><br><span class="line"><span class="number">7289</span>f400<span class="number">-2439</span><span class="number">-4822</span><span class="number">-9e3</span>a<span class="number">-928</span>af262d843</span><br></pre></td></tr></table></figure></p><p>运行cURL命令去请求一个keystone token。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat auth_token.json </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"auth"</span>: &#123;</span><br><span class="line">        <span class="string">"identity"</span>: &#123;</span><br><span class="line">            <span class="string">"methods"</span>: [</span><br><span class="line">                <span class="string">"password"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="string">"password"</span>: &#123;</span><br><span class="line">                <span class="string">"user"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"zaqar"</span>,</span><br><span class="line">                    <span class="string">"domain"</span>: &#123;</span><br><span class="line">                        <span class="string">"id"</span>: <span class="string">"default"</span>,</span><br><span class="line">                        <span class="string">"name"</span>: <span class="string">"Default"</span></span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"password"</span>: <span class="string">"zaqar"</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"scope"</span>: &#123;</span><br><span class="line">            <span class="string">"project"</span>: &#123;</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"service"</span>,</span><br><span class="line">                <span class="string">"domain"</span>: &#123;<span class="string">"id"</span>: <span class="string">"default"</span>&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>比如，这里获取到的token是<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">curl</span> <span class="selector-tag">-i</span> <span class="selector-tag">-s</span> <span class="selector-tag">-d</span>@<span class="keyword">auth_token</span>.<span class="keyword">json</span> -H <span class="string">"Content-Type: application/json"</span> http://<span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">5000</span>/v3/auth/tokens  |grep <span class="string">'X-Subject-Token'</span></span><br><span class="line">X-Subject-Token: <span class="number">7212</span>ad333e6d4a1781974f8e203bf555</span><br></pre></td></tr></table></figure></p><p>配置Pool<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># curl -i -X PUT http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/pools/testpool -d '&#123;<span class="string">"weight"</span>: <span class="number">100</span>, <span class="string">"uri"</span>: <span class="string">"mongodb://172.17.223.21,172.17.223.26:27017/?replicaSet=rs0&amp;w=2&amp;readPreference=secondaryPreferred"</span>, <span class="string">"options"</span>: &#123;<span class="string">"partitions"</span>: <span class="number">8</span>&#125;&#125;' -H <span class="string">"Client-ID: 7289f400-2439-4822-9e3a-928af262d843"</span> -H <span class="string">"X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b"</span> -H <span class="string">"Content-type: application/json"</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">201</span> Created</span><br><span class="line"><span class="built_in">content</span>-<span class="built_in">length</span>: <span class="number">0</span></span><br><span class="line"><span class="built_in">content</span>-type: application/json; charset=UTF-<span class="number">8</span></span><br><span class="line">location: http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/pools/testpool</span><br><span class="line">Connection: <span class="built_in">close</span></span><br></pre></td></tr></table></figure></p><ol start="10"><li>验证操作<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># curl -i -X POST http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/queues/samplequeue/messages -d '&#123;<span class="string">"messages"</span>: [&#123;<span class="string">"body"</span>: &#123;<span class="string">"event"</span>: <span class="number">1</span>&#125;, <span class="string">"ttl"</span>: <span class="number">600</span>&#125;, &#123;<span class="string">"body"</span>: &#123;<span class="string">"event"</span>: <span class="number">2</span>&#125;, <span class="string">"ttl"</span>: <span class="number">600</span>&#125;]&#125;' -H <span class="string">"Content-type: application/json"</span> -H <span class="string">"Client-ID: 7289f400-2439-4822-9e3a-928af262d843"</span> -H <span class="string">"X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b"</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">201</span> Created</span><br><span class="line"><span class="built_in">content</span>-<span class="built_in">length</span>: <span class="number">135</span></span><br><span class="line"><span class="built_in">content</span>-type: application/json; charset=UTF-<span class="number">8</span></span><br><span class="line">location: http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/queues/samplequeue/messages?ids=5b0bb7289140d605340bf3a6,5b0bb7289140d605340bf3a7</span><br><span class="line">Connection: <span class="built_in">close</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"resources"</span>: [<span class="string">"/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a6"</span>, <span class="string">"/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a7"</span>]&#125;</span><br></pre></td></tr></table></figure></li></ol><p>最后，你就可以愉快的使用Zaqar服务啦。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于什么是Zaqar，有什么作用。国内已有介绍读者可以自行Google查阅。若在此再阐述，已显多余。由于安装Zaqar服务官方文档还有坑且国内无资料，故这里写下中文第一篇吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;依赖服务&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个基本的Op
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>从数据删除看备份的重要性</title>
    <link href="http://yoursite.com/2018/05/23/%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E7%9C%8B%E5%A4%87%E4%BB%BD%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/"/>
    <id>http://yoursite.com/2018/05/23/从数据删除看备份的重要性/</id>
    <published>2018-05-23T14:55:14.000Z</published>
    <updated>2018-05-26T07:25:27.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>所谓，“常在河边走，哪有不湿鞋”。在一个实际的环境中，由于种种原因，可能发生数据被删除的情况。比如，云平台中的数据库、虚拟机、数据卷、镜像或底层存储被删除等，如果数据没有进行备份，则是灾难性的后果。</p><p>在笔者的工作中，经历过2次在生产环境云平台上，客户虚拟机数据被删除的情况，一次是研发部门开发的代码逻辑判断错误导致，另一次是运维同事人为误操作。因此，觉得有必要调研整理下数据备份相关的小文，以此共勉。</p><p>在一个由OpenStack+Ceph架构组成的云平台环境中，有N种数据备份方案。如OpenStack有自带的Karbor、Freezer云服务，Ceph也有相关的备份方案，也有其他商业的备份方案等。实际上，OpenStack云平台本身也提供了一些较好易用的备份功能，比如虚拟机快照/备份、数据卷快照/备份，在使用时也倡导通过将数据卷挂载给虚拟机，从而将数据写入到云盘中，间接的实现数据容灾备份。</p><p>但，如果删除的是底层存储数据，上层的备份操作基本都将报废。那么有什么好的备份方案吗。这里，我们阐述下Ceph相关的备份方案。</p><h2 id="方案1-Snapshot"><a href="#方案1-Snapshot" class="headerlink" title="方案1 Snapshot"></a>方案1 Snapshot</h2><p><strong>原理</strong></p><p>异步备份，基于RBD的snapshot机制。<br> <img src="/images/ceph-01.png" alt="image"></p><p><strong>介绍</strong></p><p>在此方案下，Cluster A &amp; B是独立的Ceph集群，通过RBD的snapshot机制，在Cluster A端，针对image定期通过rbd创建image的快照，然后通过rbd export-diff, rbd import-diff命令来完成image备份到Cluster B。</p><p><strong>命令和步骤</strong></p><p>把 Cluster A 的 pool 中的testimage 异步备份到 Cluster B 的 pool 中。</p><p>1）在Cluster A/B上创建rbd/testimage<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">create</span> -p rbd <span class="comment">--size 10240 testimage</span></span><br></pre></td></tr></table></figure></p><p>2）在准备备份image前，暂停Cluster A端对testimage的IO操作，然后创建个snapshot<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="keyword">create</span> &lt;snap-<span class="keyword">name</span>&gt;</span><br></pre></td></tr></table></figure></p><p>3）导出Cluster A端的testimage数据，不指定from-snap<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="keyword">export</span>-diff &lt;<span class="built_in">image</span>-name&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure></p><p>4）copy上一步中导出的文件到Cluster B，并导入数据到testimage<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="keyword">import</span>-diff &lt;<span class="built_in">path</span>&gt; &lt;image-<span class="keyword">name</span>&gt;</span><br></pre></td></tr></table></figure></p><p>后续需周期性的暂停Cluster A端的testimage的IO，然后创建snapshot，通过 rbd export-diff <image-name> [–from-snap <snap-name>] <path></path>命令导出incremental diff，之后把差异数据文件copy到Cluster B上，然后通过命令rbd import-diff <path></path> <image-name>导入。</image-name></snap-name></image-name></p><p>【注】：也可不暂停Cluster A端的IO，直接take snapshot；这样并不会引起image的数据不一致，只是有可能会使rbd export-diff导出的数据在take snapshot之后。</p><p><strong>此方案优缺点</strong></p><p><strong>优点：</strong></p><ul><li>命令简单，通过定制执行脚本就能实现rbd块设备的跨区备份</li></ul><p><strong>缺点：</strong></p><ul><li>每次同步前都需要在源端take snapshot或暂停IO操作</li><li>持续的snapshots可能导致image的读写性能下降</li><li>还要考虑后续删除不用的snapshots</li><li>snapshot只能保证IO的一致性，并不能保证使用rbd块设备上的系统一致性；</li></ul><h2 id="方案2-RBD-Mirroring"><a href="#方案2-RBD-Mirroring" class="headerlink" title="方案2  RBD Mirroring"></a>方案2  RBD Mirroring</h2><p><strong>原理</strong></p><p>异步备份，Ceph新的rbd mirror功能<br><img src="/images/ceph-02.png" alt="image"></p><p><strong>介绍</strong></p><p>Ceph新的rbd mirror功能支持配置两个Ceph Cluster之间的rbd同步。</p><p>在此方案下，Master Cluster使用性能较高的存储设备，提供给OpenStack的Glance、Cinder（cinder-volume、cinder-backup）和Nova服务使用；而Backup Cluster则使用容量空间大且廉价的存储设备（如SATA盘）来备份Ceph数据。不同的Ceph Cluster集群，可以根据实际需要，选择是否跨物理机房备份。</p><p><strong>优点：</strong></p><ul><li>Ceph新的功能，不需要额外开发</li><li>同步的粒度比较小，为一个块设备的transaction</li><li>保证了Crash consistency</li><li>可配置pool的备份，也可单独指定image备份</li><li>同步备份，不同机房的Ceph集群，底层存储的跨机房容灾</li></ul><h2 id="方案3-ceph备份软件ceph-backup"><a href="#方案3-ceph备份软件ceph-backup" class="headerlink" title="方案3  ceph备份软件ceph-backup"></a>方案3  ceph备份软件ceph-backup</h2><p><strong>介绍</strong></p><p>ceph-backup是一个用来备份ceph RBD块设备的开源软件，提供了两种模式。</p><ul><li>增量：在给定备份时间窗口内基于rbd快照的增量备份</li><li>完全：完整映像导出时不包含快照</li></ul><p><strong>编译安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/teralytics/ceph-backup.git</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> ceph-backup</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> python setup.py install</span></span><br></pre></td></tr></table></figure></p><p>安装过程中会下载一些东西，注意要有网络，需要等待一会<br>准备配置文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/cephbackup/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp ceph-backup.cfg /etc/cephbackup/cephbackup.conf</span></span><br></pre></td></tr></table></figure></p><p>我的配置文件如下，备份rbd存储的zp的镜像，支持多image，images后面用逗号隔开就可以<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/cephbackup/cephbackup.conf </span></span><br><span class="line">[rbd]</span><br><span class="line">window size = 7</span><br><span class="line">window unit = days</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = <span class="literal">yes</span></span><br><span class="line">ceph<span class="built_in"> config </span>= /etc/ceph/ceph.conf</span><br><span class="line">backup mode = full</span><br><span class="line">check mode = <span class="literal">no</span></span><br></pre></td></tr></table></figure></p><p><strong>开始备份</strong></p><p>全量备份配置<br>上面的配置文件已经写好了，直接执行备份命令就可以了<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cephbackup</span></span><br><span class="line"><span class="string">Starting</span> <span class="string">backup</span> <span class="string">for</span> <span class="string">pool</span> <span class="string">rbd</span></span><br><span class="line"><span class="string">Full</span> <span class="string">ceph</span> <span class="string">backup</span></span><br><span class="line"><span class="string">Images</span> <span class="string">to</span> <span class="attr">backup:</span></span><br><span class="line"><span class="string">rbd/zp</span></span><br><span class="line"><span class="string">Backup</span> <span class="attr">folder:</span> <span class="string">/tmp/</span></span><br><span class="line"><span class="attr">Compression:</span> <span class="literal">True</span></span><br><span class="line"><span class="string">Check</span> <span class="attr">mode:</span> <span class="literal">False</span></span><br><span class="line"><span class="string">Taking</span> <span class="string">full</span> <span class="string">backup</span> <span class="string">of</span> <span class="attr">images:</span> <span class="string">zp</span></span><br><span class="line"><span class="string">rbd</span> <span class="string">image</span> <span class="string">'zp'</span><span class="string">:</span></span><br><span class="line"><span class="string">size</span> <span class="number">40960</span> <span class="string">MB</span> <span class="string">in</span> <span class="number">10240</span> <span class="string">objects</span></span><br><span class="line"><span class="string">order</span> <span class="number">22</span> <span class="string">(4096</span> <span class="string">kB</span> <span class="string">objects)</span></span><br><span class="line"><span class="attr">block_name_prefix:</span> <span class="string">rbd_data.25496b8b4567</span></span><br><span class="line"><span class="attr">format:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">features:</span> <span class="string">layering</span></span><br><span class="line"><span class="attr">flags:</span> </span><br><span class="line"><span class="string">Exporting</span> <span class="string">image</span> <span class="string">zp</span> <span class="string">to</span> <span class="string">/tmp/rbd/zp/zp_UTC20170119T092933.full</span></span><br><span class="line"><span class="string">Compress</span> <span class="string">mode</span> <span class="string">activated</span></span><br></pre></td></tr></table></figure></p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">export</span> rbd/zp /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">Exporting image: <span class="number">100</span>% complete...done.</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tar scvfz <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full.tar.gz <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full</span><br><span class="line">tar: Removing leading `<span class="regexp">/' from member names</span></span><br></pre></td></tr></table></figure><p>压缩如果打开了，正好文件也是稀疏文件的话，需要等很久，压缩的效果很好，dd生成的文件可以压缩到很小<br>检查备份生成的文件<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ll /tmp/rbd/zp/zp_UTC20170119T092933.full*</span></span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root<span class="number"> 42949672960 </span>Jan<span class="number"> 19 </span>17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root          <span class="number"> 0 </span>Jan<span class="number"> 19 </span>17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz</span><br></pre></td></tr></table></figure></p><p>全量备份的还原<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span> <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full zpbk</span><br></pre></td></tr></table></figure></p><p>检查数据，没有问题<br>增量备份配置<br>写下增量配置的文件，修改下备份模式的选项<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[rbd]</span><br><span class="line">window size = 7</span><br><span class="line">window unit = day</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = <span class="literal">yes</span></span><br><span class="line">ceph<span class="built_in"> config </span>= /etc/ceph/ceph.conf</span><br><span class="line">backup mode = incremental</span><br><span class="line">check mode = <span class="literal">no</span></span><br></pre></td></tr></table></figure></p><p>执行多次进行增量备份以后是这样的<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ll  /tmp/rbd/zpbk/</span></span><br><span class="line">total 146452</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root<span class="number"> 42949672960 </span>Jan<span class="number"> 19 </span>18:04 zpbk@UTC20170119T100339.full</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root      <span class="number"> 66150 </span>Jan<span class="number"> 19 </span>18:05 zpbk@UTC20170119T100546.diff_from_UTC20170119T100339</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:05 zpbk@UTC20170119T100550.diff_from_UTC20170119T100546</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:06 zpbk@UTC20170119T100606.diff_from_UTC20170119T100550</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:06 zpbk@UTC20170119T100638.diff_from_UTC20170119T100606</span><br></pre></td></tr></table></figure></p><p><strong>增量备份的还原</strong></p><p>分成多个步骤进行</p><p>1、进行全量的恢复<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span> config@UTC20161130T170848.full dest_image</span><br></pre></td></tr></table></figure></p><p>2、重新创建基础快照<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rbd snap create dest_image<span class="doctag">@UTC</span>20161130T170848</span></span><br></pre></td></tr></table></figure></p><p>3、还原增量的快照<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span>-diff config@UTC20161130T170929.diff_from_UTC20161130T170848 dest_image</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综合评估，以上三种方案的优缺点，推荐使用“方案二RBD Mirroring”。</p><p>参考链接<br><a href="http://www.yangguanjun.com/2017/02/22/rbd-data-replication/" target="_blank" rel="noopener">http://www.yangguanjun.com/2017/02/22/rbd-data-replication/</a><br><a href="https://ceph.com/planet/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/" target="_blank" rel="noopener">https://ceph.com/planet/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;所谓，“常在河边走，哪有不湿鞋”。在一个实际的环境中，由于种种原因，可能发生数据被删除的情况。比如，云平台中的数据库、虚拟机、数据卷、镜像或
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/OpenStack/Ceph/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>聊聊OpenStack运维架构那些事儿</title>
    <link href="http://yoursite.com/2018/05/09/%E8%81%8A%E8%81%8AOpenStack%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/"/>
    <id>http://yoursite.com/2018/05/09/聊聊OpenStack运维架构那些事儿/</id>
    <published>2018-05-09T13:22:43.000Z</published>
    <updated>2018-05-10T14:58:47.155Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>想一想，从事OpenStack杂七杂八的事儿，至今正好三年半了。做过QA测试（手动的、自动的）、CI（gerrit、jenkins、gitlab、harbor）、云产品封装（从系统pxe到openstack代码）、自动化部署开发、运维监控、分布式存储、底层功能调研和实现、开源社区参与、Docker等等。</p><p>一个良好的架构设计和运维保障措施，能为OpenStack云平台的稳定健康运行，产生不可估量的积极影响。</p><p>下面，是笔者从业OpenStack以来，关于OpenStack运维、架构设计、实施的点滴之想。在此，做一个回顾和总结。如有差错，欢迎拍砖。</p><p>OK，咱们言归正传进入话题吧。如果化繁为简，简单的来说，要部署一套生产环境级别的OpenStack云平台，至少会涉及到四个层次的内容，即物理基础设施层、存储层、OpenStack云服务层和用户应用层。如下图所示。</p><p><img src="/images/openstack-ceng.png" alt="image"></p><h2 id="物理基础设施层"><a href="#物理基础设施层" class="headerlink" title="物理基础设施层"></a>物理基础设施层</h2><p>首先，从最底层开始说起，即“物理基础设施层”。一个基本的物理基础设施IT环境，包括了电力设备、空调和防火设备、网络设备（如交换机、路由器、防火墙等）、存储设备和服务器等。由于专业知识的限制，这里，只涉及交换机和服务器方面。一个基本的物理IT环境，如下图所示。</p><p><img src="/images/wangluoceng.png" alt="image"></p><h3 id="交换机设备"><a href="#交换机设备" class="headerlink" title="交换机设备"></a>交换机设备</h3><p>一般地，在OpenStack生产环境上，交换机端口应该做聚合（channel）。也就是将2个或多个物理端口组合在一起成为一条逻辑的链路从而增加交换机和网络节点之间的带宽，将属于这几个端口的带宽合并，给端口提供一个几倍于独立端口的独享的高带宽。Trunk是一种封装技术，它是一条点到点的链路，链路的两端可以都是交换机，也可以是交换机和路由器，还可以是主机和交换机或路由器。</p><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><h4 id="网卡"><a href="#网卡" class="headerlink" title="网卡"></a>网卡</h4><p>OpenStack云平台涉及到的网络有管理网络（用于OpenStack各服务之间通信）、外部网络（提供floating ip）、存储网络（如ceph存储网络）和虚机网络（也称租户网络、业务网络）四种类型。</p><p>对应到每一种网络，服务器都应该做网卡Bond，来提供服务器网络的冗余、高可用和负载均衡的能力，根据实际需求，可以选择模式0或模式1。在网络流量较大的场景下推荐使用bond 0；在可靠性要求较高的场景下推荐使用bond 1。</p><p>二者优劣比较。</p><p><img src="/images/bond.png" alt="image"></p><p>在生产环境中，如果是少于90台OpenStack节点规模的私有云，一般网络类型对应的带宽是（PS：90台只是一个相对值，非绝对值，有洁癖的人请绕过）。</p><ul><li>管理网络：千兆网络</li><li>外部网络：千兆网络</li><li>存储网络：万兆网络</li><li>租户网络：千兆网络</li></ul><p>如果是多于90台OpenStack节点规模的私有云或公有云环境，则推荐尽量都使用万兆网络。</p><h4 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a>硬盘</h4><p>服务器操作系统使用的系统盘，应该用2块硬盘来做RAID 1，以提供系统存储的高可靠性。且推荐使用高性能且成本可控的SAS硬盘，以提高操作系统、MySQL数据库和Docker容器（如果使用kolla部署openstack）的存储性能。</p><h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>OpenStack各计算节点的CPU型号，必须一致，以保证虚拟机的迁移功能正常可用等。</p><h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><p>OpenStack各计算节点的内存大小，应该一致，以保证虚拟机创建管理的均衡调度等。同时，主机的Swap交换分区，应该科学合理的设置，不能使用系统默认创建的。如何设置，请参考此文。<a href="https://xuchao918.github.io/2018/05/07/如何设置OpenStack节点Swap分区/" target="_blank" rel="noopener">如何设置OpenStack节点Swap分区</a>。</p><p>数据中心中少部分机器用于做控制节点，大部分机器都是需要运行虚拟化软件的，虚拟化平台上有大量的vm，而宿主机本身的系统也会跑一些服务，那么这就势必会造成vm之间资源抢占，vm与宿主机系统之间的资源抢占，我们需要通过设定游戏规则，让他们在各自的界限内高效运行，减少冲突抢占。</p><p>我们可以让宿主机运行操作系统时候，更多的选择指定的几个核，这样就不会过多抢占虚拟化中虚机的vcpu调度，通过修改内核启动参数我们可以做到:</p><p>修改 /etc/default/grub文件，让系统只使用前三个核 隔离其余核。<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX_DEFAULT="isolcpus=<span class="number">4,5,6,7</span>,<span class="number">8,9,10,11</span>,<span class="number">12,13,14,15</span>,<span class="number">16,17,18,19</span>,<span class="number">20,21,22,23</span>,<span class="number">24,25,26,27</span>,<span class="number">28,29,30,31</span>"</span><br></pre></td></tr></table></figure></p><p>更新内核参数<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> update-grub</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure></p><p>内存配置方面，网易私有云的实践是关闭 KVM 内存共享，打开透明大页：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="number">0</span> &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/ksm</span><span class="regexp">/pages_shared</span></span><br><span class="line"><span class="regexp">echo 0 &gt; /sys</span><span class="regexp">/kernel/mm</span><span class="regexp">/ksm/pages</span>_sharing</span><br><span class="line">echo always &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/enabled</span><br><span class="line">echo never &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/defrag</span><br><span class="line">echo <span class="number">0</span> &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/khugepaged/defrag</span><br></pre></td></tr></table></figure></p><p>据说，经过 SPEC CPU2006 测试，这些配置对云主机 CPU 性能大概有7%左右的提升。</p><h2 id="OpenStack云平台层"><a href="#OpenStack云平台层" class="headerlink" title="OpenStack云平台层"></a>OpenStack云平台层</h2><h3 id="云平台高可用（HA）"><a href="#云平台高可用（HA）" class="headerlink" title="云平台高可用（HA）"></a>云平台高可用（HA）</h3><h4 id="高可用（HA）介绍"><a href="#高可用（HA）介绍" class="headerlink" title="高可用（HA）介绍"></a>高可用（HA）介绍</h4><p>高可用性是指提供在本地系统单个组件故障情况下，能继续访问应用的能力，无论这个故障是业务流程、物理设施、IT软/硬件的故障。最好的可用性， 就是你的一台机器宕机了，但是使用你的服务的用户完全感觉不到。你的机器宕机了，在该机器上运行的服务肯定得做故障切换（failover），切换有两个维度的成本：RTO （Recovery Time Objective）和 RPO（Recovery Point Objective）。RTO 是服务恢复的时间，最佳的情况是 0，这意味着服务立即恢复；最坏是无穷大意味着服务永远恢复不了；RPO 是切换时向前恢复的数据的时间长度，0 意味着使用同步的数据，大于 0 意味着有数据丢失，比如 ” RPO = 1 天“ 意味着恢复时使用一天前的数据，那么一天之内的数据就丢失了。因此，恢复的最佳结果是 RTO = RPO = 0，但是这个太理想，或者要实现的话成本太高。</p><p>对 HA 来说，往往使用分布式存储，这样的话，RPO =0 ；同时使用 Active/Active （双活集群） HA 模式来使得 RTO 几乎为0，如果使用 Active/Passive  HA模式的话，则需要将 RTO 减少到最小限度。HA 的计算公式是[ 1 - (宕机时间)/（宕机时间 + 运行时间）]，我们常常用几个 9 表示可用性：</p><ul><li>2 个9：99% = 1% <em> 365 = 3.65 </em> 24 小时/年 = 87.6 小时/年的宕机时间</li><li>4 个9: 99.99% = 0.01% <em> 365 </em> 24 * 60 = 52.56 分钟/年</li><li>5 个9：99.999% = 0.001% * 365 = 5.265 分钟/年的宕机时间，也就意味着每次停机时间在一到两分钟。</li><li>11 个 9：几年宕机几分钟。</li></ul><p><strong>服务的分类</strong></p><p>HA 将服务分为两类：</p><ul><li><p>有状态服务：后续对服务的请求依赖于之前对服务的请求。OpenStack中有状态的服务包括MySQL数据库和AMQP消息队列。对于有状态类服务的HA，如neutron-l3-agent、neutron-metadata-agent、nova-compute、cinder-volume等服务，最简便的方法就是多节点部署。比如某一节点上的nova-compute服务挂了，也并不会影响到整个云平台不能创建虚拟机，或者所在节点的虚拟机无法使用（比如ssh等）。 </p></li><li><p>无状态服务：对服务的请求之间没有依赖关系，是完全独立的，基于冗余实例和负载均衡实现HA。OpenStack中无状态的服务包括nova-api、nova-conductor、glance-api、keystone-api、neutron-api、nova-scheduler等。由于API服务，属于无状态类服务，天然支持Active/Active HA模式。因此，一般使用 keepalived +HAProxy方案来做。</p></li></ul><p><strong>HA 的种类</strong></p><p>HA 需要使用冗余的服务器组成集群来运行负载，包括应用和服务。这种冗余性也可以将 HA 分为两类：</p><ul><li><p>Active/Passive HA：集群只包括两个节点简称主备。在这种配置下，系统采用主和备用机器来提供服务，系统只在主设备上提供服务。在主设备故障时，备设备上的服务被启动来替代主设备提供的服务。典型地，可以采用 CRM 软件比如 Pacemaker 来控制主备设备之间的切换，并提供一个虚机 IP 来提供服务。</p></li><li><p>Active/Active HA：集群只包括两个节点时简称双活，包括多节点时成为多主（Multi-master）。在这种配置下，系统在集群内所有服务器上运行同样的负载。以数据库为例，对一个实例的更新，会被同步到所有实例上。这种配置下往往采用负载均衡软件比如 HAProxy 来提供服务的虚拟 IP。</p></li></ul><h4 id="OpenStack云环境高可用（HA）"><a href="#OpenStack云环境高可用（HA）" class="headerlink" title="OpenStack云环境高可用（HA）"></a>OpenStack云环境高可用（HA）</h4><p>云环境是一个广泛的系统，包括了基础设施层、OpenStack云平台服务层、虚拟机和最终用户应用层。</p><p><strong>云环境的 HA 包括：</strong></p><ul><li>用户应用的 HA</li><li>虚拟机的 HA</li><li>OpenStack云平台服务的 HA</li><li>基础设施层的HA：电力、空调和防火设施、网络设备（如交换机、路由器）、服务器设备和存储设备等</li></ul><p>仅就OpenStack云平台服务（如nova-api、nova-scheduler、nova-compute等）而言，少则几十，多则上百个。如果某一个服务挂了，则对应的功能便不能正常使用。因此，如何保障整体云环境的HA高可用，便成为了架构设计和运维的重中之重。</p><p>OpenStack HA高可用架构，如下图所示。</p><p><img src="/images/vmha.png" alt="image"></p><p><strong>OpenStack高可用内容</strong></p><p>如果，从部署层面来划分，OpenStack高可用的内容包括：</p><ul><li>控制节点（Rabbitmq、mariadb、Keystone、nova-api等）</li><li>网络节点（neutron_dhcp_agent、neutron_l3_agent、neutron_openvswitch_agent等）</li><li>计算节点（Nova-Compute、neutron_openvswitch_agent、虚拟机等）</li><li>存储节点（cinder-volume、swift等）</li></ul><h5 id="控制节点HA"><a href="#控制节点HA" class="headerlink" title="控制节点HA"></a>控制节点HA</h5><p>在生产环境中，建议至少部署三台控制节点，其余可做计算节点、网络节点或存储节点。采用Haproxy + KeepAlived方式，代理数据库服务和OpenStack服务，对外暴露VIP提供API访问。</p><p><strong>MySQL数据库HA</strong></p><p>mysql 的HA 方案有很多，这里只讨论openstack 官方推荐的mariadb galara 集群。Galera Cluster 是一套在innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到各个节点上去。特点如下：<br>1）同步复制，（&gt;=3）奇数个节点<br>2）Active-active的多主拓扑结构<br>3）集群任意节点可以读和写<br>4）自动身份控制,失败节点自动脱离集群<br>5）自动节点接入<br>6）真正的基于”行”级别和ID检查的并行复制<br>7）无单点故障,易扩展</p><p>采用MariaDB + Galera方案部署至少三个节点（最好节点数量为奇数），外部访问通过Haproxy的active + backend方式代理。平时主库为A，当A出现故障，则切换到B或C节点。如下图所示。</p><p><img src="/images/mysqlha.jpg" alt="image"></p><p><strong>RabbitMQ 消息队列HA</strong></p><p>RabbitMQ采用原生Cluster集群方案，所有节点同步镜像队列。三台物理机，其中2个Mem节点主要提供服务，1个Disk节点用于持久化消息，客户端根据需求分别配置主从策略。</p><p><strong>OpenStack API服务HA</strong></p><p>OpenStack控制节点上运行的基本上是API 无状态类服务，如nova-api、neutron-server、glance-registry、nova-novncproxy、keystone等。因此，可以由 HAProxy 提供负载均衡，将请求按照一定的算法转到某个节点上的 API 服务，并由KeepAlived提供 VIP。</p><h5 id="网络节点HA"><a href="#网络节点HA" class="headerlink" title="网络节点HA"></a>网络节点HA</h5><p>网络节点上运行的Neutron服务包括很多的组件，比如 L3 Agent，openvswitch Agent，LBaas，VPNaas，FWaas，Metadata Agent 等，其中部分组件提供了原生的HA 支持。</p><ul><li>Openvswitch Agent HA： openvswitch agent 只在所在的网络或者计算节点上提供服务，因此它是不需要HA的</li><li>L3 Agent HA：成熟主流的有VRRP 和DVR两种方案</li><li>DHCP Agent HA：在多个网络节点上部署DHCP Agent，实现HA </li><li>LBaas Agent HA：Pacemaker + 共享存储（放置 /var/lib/neutron/lbaas/ 目录） 的方式来部署 A/P 方式的 LBaas Agent HA</li></ul><h5 id="存储节点HA"><a href="#存储节点HA" class="headerlink" title="存储节点HA"></a>存储节点HA</h5><p>存储节点的HA，主要是针对cinder-volume、cinder-backup服务做HA，最简便的方法就是部署多个存储节点，某一节点上的服务挂了，不至于影响到全局。</p><h5 id="计算节点和虚拟机-HA"><a href="#计算节点和虚拟机-HA" class="headerlink" title="计算节点和虚拟机 HA"></a>计算节点和虚拟机 HA</h5><p>计算节点和虚拟机的HA，社区从2016年9月开始一直致力于一个虚拟机HA的统一方案，<br>详细参考：High Availability for Virtual Machines。目前还处于开发阶段。业界目前使用的方案大致有以下几种：</p><p>1)检查compute计算节点和nova 服务运行状态，对于有问题的节点或服务进行自动修复。该方案的实现是：</p><p>①Pacemaker 监控每个计算节点上的 pacemaker_remote 的连接，来检查该节点是否处于活动状态。发现它不可以连接的话，启动恢复（recovery）过程。</p><ul><li>运行 ‘nova service-disable’</li><li>将该节点关机</li><li>等待 nova 发现该节点失效了</li><li>将该节点开机</li><li>如果节点启动成功，执行 ‘nova service-enable’</li><li>如果节点启动失败，则执行 ‘nova evacuate’ 把该节点上的虚机移到别的可用计算节点上。</li></ul><p>②Pacemaker 监控每个服务的状态，如果状态失效，该服务会被重启，重启失败则触发防护行为（fencing action），即停掉该服务。</p><p>2)分布式健康检查，参考分布式健康检查：实现OpenStack计算节点高可用</p><p>如果使用第一种方案，实现计算节点和虚拟机HA，要做的事情基本有三件，即。</p><p><strong>监控</strong></p><p>监控主要做两个事情，一个是监控计算节点的硬件和软件故障。第二个是触发故障的处理事件，也就是隔离和恢复。</p><p>OpenStack 计算节点高可用，可以用pacemaker和pacemaker_remote来做。使用pacemaker_remote后，我们可以把所有的计算节点都加入到这个集群中，计算节点只需要安装pacemaker_remote即可。pacemaker集群会监控计算节点上的pacemaker_remote是否 “活着”，你可以定义什么是“活着”。在计算节点上可以监控nova-compute、neutron-ovs-agent、libvirt等进程，从而确定计算节点是否活着，甚至我们还可以在该计算节点上启动虚拟机来确定计算节点是否活着。如果监控到某个pacemaker_remote有问题，可以马上触发之后的隔离和恢复事件。</p><p><strong>隔离</strong></p><p>隔离最主要的任务是将不能正常工作的计算节点从OpenStack集群环境中移除，nova-scheduler就不会在把create_instance的message发给该计算节点。</p><p>Pacemaker 已经集成了fence这个功能，因此我们可以使用fence_ipmilan来关闭计算节点。Pacemaker集群中fence_compute 会一直监控这个计算节点是否down了，因为nova只能在计算节点down了之后才可以执行host-evacuate来迁移虚拟机，期间等待的时间稍长。这里有个更好的办法， 就是调用nova service-force-down 命令，直接把计算节点标记为down，方便更快的迁移虚拟机。</p><p><strong>恢复</strong></p><p>恢复就是将状态为down的计算节点上的虚拟机迁移到其他计算节点上。Pacemaker集群会调用host-evacuate API将所有虚拟机迁移。host-evacuate最后是使用rebuild来迁移虚拟机，每个虚拟机都会通过scheduler调度在不同的计算节点上启动。</p><p><strong>虚拟机操作系统故障恢复</strong></p><p>OpenStack 中的 libvirt/KVM 驱动已经能够很好地自动化处理这类问题。具体地，你可以在flavor的 extra_specs 或者镜像的属性中加上 hw:watchdog_action ，这样 一个 watchdog 设备会配置到虚拟机上。如果 hw:watchdog_action 设置为 reset，那么虚拟机的操作系统一旦奔溃，watchdog 会将虚拟机自动重启。</p><h3 id="OpenStack计算资源限制"><a href="#OpenStack计算资源限制" class="headerlink" title="OpenStack计算资源限制"></a>OpenStack计算资源限制</h3><p><strong>设置内存</strong></p><p>#内存分配超售比例，默认是 1.5 倍，生产环境不建议开启超售<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ram_allocation_ratio</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>#内存预留量，这部分内存不能被虚拟机使用，以便保证系统的正常运行<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">reserved_host_memory_mb</span> = <span class="number">10240</span>      //如预留<span class="number">10</span>GB</span><br></pre></td></tr></table></figure></p><p><strong>设置CPU</strong></p><p>在虚拟化资源使用上，我们可以通过nova来控制，OpenStack提供了一些配置，我们可以很容易的做到，修改文件nova.conf。</p><p>#虚拟机 vCPU 的绑定范围，可以防止虚拟机争抢宿主机进程的 CPU 资源，建议值是预留前几个物理 CPU<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">vcpu_pin_set</span> = <span class="number">4</span>-<span class="number">31</span></span><br></pre></td></tr></table></figure></p><p>#物理 CPU 超售比例，默认是 16 倍，超线程也算作一个物理 CPU<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">cpu_allocation_ratio</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure></p><h3 id="使用多Region和AZ"><a href="#使用多Region和AZ" class="headerlink" title="使用多Region和AZ"></a>使用多Region和AZ</h3><p>如果，OpenStack云平台需要跨机房或地区部署，可以使用多Region和 Availability Zone（以下简称AZ）的方案。这样，每个机房之间在地理位置上自然隔离，这对上层的应用来说是天然的容灾方法。</p><p><strong>多区域（Region）部署</strong></p><p>OpenStack支持依据地理位置划分为不同的Region，所有的Regino除了共享Keystone、Horizon服务外，每个Region都是一个完整的OpenStack环境，从整体上看，多个区域之间的部署相对独立，但可通过内网专线实现互通（如BGP-EVPN）。其架构如下图所示。</p><p><img src="/images/region-az.png" alt="image"></p><p>部署时只需要部署一套公共的Keystone和Horizon服务，其它服务按照单Region方式部署即可，通过Endpoint指定Region。用户在请求任何资源时必须指定具体的区域。采用这种方式能够把分布在不同的区域的资源统一管理起来，各个区域之间可以采取不同的部署架构甚至不同的版本。其优点如下：</p><ul><li>部署简单，每个区域部署几乎不需要额外的配置，并且区域很容易实现横向扩展。</li><li>故障域隔离，各个区域之间互不影响。</li><li>灵活自由，各个区域可以使用不同的架构、存储、网络。</li></ul><p>但该方案也存在明显的不足：</p><ul><li>各个区域之间完全隔离，彼此之间不能共享资源。比如在Region A创建的Volume，不能挂载到Region B的虚拟机中。在Region A的资源，也不能分配到Region B中，可能出现Region负载不均衡问题。</li><li>各个区域之间完全独立，不支持跨区域迁移，其中一个区域集群发生故障，虚拟机不能疏散到另一个区域集群中。</li><li>Keystone成为最主要的性能瓶颈，必须保证Keystone的可用性，否则将影响所有区域的服务。该问题可以通过部署多Keystone节点解决。</li></ul><p>OpenStack多Region方案通过把一个大的集群划分为多个小集群统一管理起来，从而实现了大规模物理资源的统一管理，它特别适合跨数据中心并且分布在不同区域的场景，此时根据区域位置划分Region，比如北京和上海。而对于用户来说，还有以下好处:</p><ul><li>用户能根据自己的位置选择离自己最近的区域，从而减少网络延迟,加快访问速度。</li><li>用户可以选择在不同的Region间实现异地容灾。当其中一个Region发生重大故障时，能够快速把业务迁移到另一个Region中。</li></ul><p><strong>多Availability Zone部署</strong></p><p>如果，只是想在一个机房中部署OpenStack云环境。则只需要使用AZ方案即可。每个AZ有自己独立供电的机架，以及OpenStack计算节点。</p><p>Availability Zone</p><p>一个Region可以被细分为一个或多个物理隔离或逻辑隔离的availability zones（AZ）。启动虚拟机时，可以指定特定的AZ甚至特定AZ中的某一个节点来启动该虚拟机。AZ可以简单理解为一组节点的集合，这组节点具有独立的电力供应设备，比如一个个独立供电的机房，或一个个独立供电的机架都可以被划分成AZ。</p><p>然后将应用的多个虚拟机分别部署在Region的多个AZ上，提高虚拟机的容灾性和可用性。由于，AZ是物理隔离的，所以一个AZ挂了不会影响到其他的AZ。同时，还可以将挂了的AZ上的虚拟机，迁移到其他正常可用的AZ上，类似于异地双活。</p><p>Host Aggregate</p><p>除了AZ，计算节点也可以在逻辑上划分为主机聚合（Host Aggregates简称HA）。主机聚合使用元数据去标记计算节点组。一个计算节点可以同时属于一个主机聚合以及AZ而不会有冲突，它也可以属于多个主机聚合。</p><p>主机聚合的节点具有共同的属性，比如：cpu是特定类型的一组节点，disks是ssd的一组节点，os是linux或windows的一组节点等等。需要注意的是，Host Aggregates是用户不可见的概念，主要用来给nova-scheduler通过某一属性来进行instance的调度，比如讲数据库服务的 instances都调度到具有ssd属性的Host Aggregate中，又或者让某个flavor或某个image的instance调度到同一个Host Aggregates中。</p><p>简单的来说，Region、Availability Zone和Host Aggregate这三者是从大范围到小范围的关系，即前者包含了后者。一个地理区域Region包含多个可用区AZ (availability zone)，同一个AZ中的计算节点又可以根据某种规则逻辑上的组合成一个组。例如在北京有一个Region，成都有一个Region，做容灾之用。同时，在北京Region下，有2个AZ可用区（如酒仙桥机房和石景山机房），每个AZ都有自己独立的网络和供电设备，以及OpenStack计算节点等，如果用户是在北京，那么用户在部署VM的时候选择北京，可以提高用户的访问速度和较好的SLA（服务等级协议）。</p><h3 id="备份你的数据"><a href="#备份你的数据" class="headerlink" title="备份你的数据"></a>备份你的数据</h3><p>如果因为某些原因，没有跨物理机房或地区的Region和AZ。那么OpenStack云平台相关的数据备份，则是必须要做的。比如MySQL数据库等，可以根据实际需求，每隔几小时进行一次备份。而备份的数据，建议存放到其他机器上。</p><h3 id="使用合适的Docker存储"><a href="#使用合适的Docker存储" class="headerlink" title="使用合适的Docker存储"></a>使用合适的Docker存储</h3><p>如果，OpenStack云平台是用kolla容器化部署和管理的。那么选择一个正确、合适的Docker存储，关乎你的平台稳定和性能。</p><p>Docker 使用存储驱动来管理镜像每层内容及可读写的容器层，存储驱动有 devicemapper、aufs、overlay、overlay2、btrfs、zfs 等，不同的存储驱动实现方式有差异，镜像组织形式可能也稍有不同，但都采用栈式存储，并采用 Copy-on-Write(CoW) 策略。且存储驱动采用热插拔架构，可动态调整。那么，存储驱动那么多，该如何选择合适的呢？大致可从以下几方面考虑：</p><ul><li>若内核支持多种存储驱动，且没有显式配置，Docker 会根据它内部设置的优先级来选择。优先级为 aufs &gt; btrfs/zfs &gt; overlay2 &gt; overlay &gt; devicemapper。若使用 devicemapper 的话，在生产环境，一定要选择 direct-lvm, loopback-lvm 性能非常差。</li><li>选择会受限于 Docker 版本、操作系统、系统版本等。例如，aufs 只能用于 Ubuntu 或 Debian 系统，btrfs 只能用于 SLES （SUSE Linux Enterprise Server, 仅 Docker EE 支持）。</li><li>有些存储驱动依赖于后端的文件系统。例如，btrfs 只能运行于后端文件系统 btrfs 上。</li><li>不同的存储驱动在不同的应用场景下性能不同。例如，aufs、overlay、overlay2 操作在文件级别，内存使用相对更高效，但大文件读写时，容器层会变得很大；devicemapper、btrfs、zfs 操作在块级别，适合工作在写负载高的场景；容器层数多，且写小文件频繁时，overlay2 效率比 overlay更高；btrfs、zfs 更耗内存。</li></ul><p>Docker 容器其实是在镜像的最上层加了一层读写层，通常也称为容器层。在运行中的容器里做的所有改动，如写新文件、修改已有文件、删除文件等操作其实都写到了容器层。存储驱动决定了镜像及容器在文件系统中的存储方式及组织形式。</p><p>在我们的生产环境中，使用的是Centos 7.4系统及其4.15内核版本+Docker 1.13.1版本。因此使用的是overlay2存储。下面对overlay2作一些简单介绍。</p><p><strong>Overlay介绍</strong></p><p>OverlayFS 是一种类似 AUFS 的联合文件系统，但实现更简单，性能更优。OverlayFS 严格来说是 Linux 内核的一种文件系统，对应的 Docker 存储驱动为 overlay 或者 overlay2，overlay2 需 Linux 内核 4.0 及以上，overlay 需内核 3.18 及以上。且目前仅 Docker 社区版支持。条件许可的话，尽量使用 overlay2，与 overlay 相比，它的 inode 利用率更高。</p><p>和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的容器层和镜像层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示：</p><p><img src="/images/overlay2.png" alt="image"></p><p><strong>分析</strong></p><ul><li>从kernel 3.18进入主流Linux内核。设计简单，速度快，比AUFS和Device mapper速度快。在某些情况下，也比Btrfs速度快。是Docker存储方式选择的未来。因为OverlayFS只有两层，不是多层，所以OverlayFS “copy-up”操作快于AUFS。以此可以减少操作延时。</li><li>OverlayFS支持页缓存共享，多个容器访问同一个文件能共享一个页缓存，以此提高内存使用率。</li><li>OverlayFS消耗inode，随着镜像和容器增加，inode会遇到瓶颈。Overlay2能解决这个问题。在Overlay下，为了解决inode问题，可以考虑将/var/lib/docker挂在单独的文件系统上，或者增加系统inode设置。</li></ul><h2 id="使用分布式存储"><a href="#使用分布式存储" class="headerlink" title="使用分布式存储"></a>使用分布式存储</h2><p>如果OpenStack云平台使用开源的分布式存储系统，如Ceph、GlusterFS等。如何保证存储系统的冗余容灾性、可靠性、安全性和性能，便显得尤为重要。这里，以Ceph开源分布式存储为例进行讲解。</p><h3 id="Mon节点和OSD节点部署"><a href="#Mon节点和OSD节点部署" class="headerlink" title="Mon节点和OSD节点部署"></a>Mon节点和OSD节点部署</h3><p>一般地，在生产环境中，至少需要部署有3个Ceph Mon节点（数量最好为奇数）以及多个OSD节点。</p><h3 id="开启CephX认证"><a href="#开启CephX认证" class="headerlink" title="开启CephX认证"></a>开启CephX认证</h3><p>同时，开启CephX认证方式，以提高数据存储的安全性，防范被攻击。如下所示。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/ceph/ceph.conf </span></span><br><span class="line">[global]</span><br><span class="line">fsid = e10d7336-23e8-4dac-a07a-d012d9208ae1</span><br><span class="line">mon_initial_members = computer1, computer2, computer3</span><br><span class="line">mon_host = 172.17.51.54,172.17.51.55,172.17.51.56</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">………</span><br></pre></td></tr></table></figure></p><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>如果Ceph节点少于90台，建议Ceph公共网络（即Public Network）使用千兆网络，集群网络（即Cluster Network）使用万兆网络。如果Ceph节点多于90台，且业务负载较高，则尽量都使用万兆网络，在重要的环境上，Ceph公共网络和集群网络，都应该单独分开。需要注意的是，Ceph存储节点使用的网卡，必须要做网卡Bond，防止网卡因故障而导致网络中断。</p><h3 id="使用Cache-Tier"><a href="#使用Cache-Tier" class="headerlink" title="使用Cache Tier"></a>使用Cache Tier</h3><p>在一个云存储环境中，出于成本的考虑，基本会少量使用SSD硬盘，大量使用SATA硬盘。在OpenStack集成Ceph的云环境中，如何使用SSD和SATA硬盘。一般有两种使用方法。</p><p>第一种：分别创建独立的SSD和SATA存储资源集群。然后，Cinder块存储服务对接这两套Ceph后端存储，这样云平台便可以同时创建和使用SSD介质和SATA介质的云硬盘。</p><p>第二种：使用SSD硬盘创建容量相对较小但性能高的缓存池（Cache tier），SATA硬盘创建容量大的但性能较低的存储池（Storage tier）。</p><p>以第二种方式为例进行讲解。当客户端访问操作数据时，会优先读写cache tier数据(当然要根据cache mode来决定)，如果数据在storage tier 则会提升到cache tier中，在cache tier中会有请求命中算法、缓存刷写算法、缓存淘汰算法等，将热数据提升到cache tier中，将冷数据下放到storage tier中。</p><p>缓存层代理自动处理缓存层和后端存储之间的数据迁移。在使用过程中，我们可以根据自己的需要，来配置迁移规则，主要有两种场景：</p><ul><li>回写模式： 管理员把缓存层配置为 writeback 模式时， Ceph 客户端们会把数据写入缓存层、并收到缓存层发来的 ACK ；写入缓存层的数据会被迁移到存储层、然后从缓存层刷掉。直观地看，缓存层位于后端存储层的“前面”，当 Ceph 客户端要读取的数据位于存储层时，缓存层代理会把这些数据迁移到缓存层，然后再发往 Ceph 客户端。从此， Ceph 客户端将与缓存层进行 I/O 操作，直到数据不再被读写。此模式对于易变数据来说较理想（如照片/视频编辑、事务数据等）。</li><li>只读模式： 管理员把缓存层配置为 readonly 模式时， Ceph 直接把数据写入后端。读取时， Ceph 把相应对象从后端复制到缓存层，根据已定义策略、脏对象会被缓存层踢出。此模式适合不变数据（如社交网络上展示的图片/视频、 DNA 数据、 X-Ray 照片等），因为从缓存层读出的数据可能包含过期数据，即一致性较差。对易变数据不要用 readonly 模式。</li></ul><h3 id="独立使用Pool"><a href="#独立使用Pool" class="headerlink" title="独立使用Pool"></a>独立使用Pool</h3><p>Ceph可以统一OpenStack Cinder块存储服务（cinder-volume、cinder-backup）、Nova计算服务和Glance镜像服务的后端存储。在生产环境上，建议单独创建4个存储资源池（Pool）以分别对应OpenStack的4种服务存储。同时，每个Pool的副本数建议设置为3份，如下表所示。</p><table><thead><tr><th>Openstack服务</th><th style="text-align:center">Ceph存储池</th><th style="text-align:right">认证用户 </th></tr></thead><tbody><tr><td>Cinder-volumes</td><td style="text-align:center">volumes</td><td style="text-align:right">cinder </td></tr><tr><td>Cinder-backups</td><td style="text-align:center">backups</td><td style="text-align:right">cinder </td></tr><tr><td>Nova</td><td style="text-align:center">vms</td><td style="text-align:right">cinder</td></tr><tr><td>Glance</td><td style="text-align:center">images</td><td style="text-align:right">cinder、glance</td></tr></tbody></table><p>最后，Ceph分布式存储部署架构，如下图所示。</p><p><img src="/images/ceph.png" alt="image"></p><h2 id="用户应用层"><a href="#用户应用层" class="headerlink" title="用户应用层"></a>用户应用层</h2><p>在相当多的业务中，都会涉及到服务高可用。而一般的高可用的实现都是通过VIP(Vitrual IP)实现。VIP不像IP一样，对应一个实际的网络接口（网卡），它是虚拟出来的IP地址，所以利用其特性可以实现服务的容错和迁移工作。</p><p>在常见节点中VIP配置非常简单，没有多大的限制。但OpenStack实例中，一个IP对应一个Port设备。并且Neutron 有“Allowed address pairs”限制，该功能要求 Port 的MAC/IP 相互对应，那么该IP才能连通。对Port设备的进行操作可以实现下面几个功能：</p><ul><li>一个Port设备添加多组Allowed address Pairs，允许多个IP通过该Port连通。</li><li>一个IP对应多组MAC地址。</li><li>一个MAC地址对应多个IP</li></ul><p>另外在OpenStack创建的实例中建立VIP并使其能正常工作可以使用下面方法：</p><ul><li>创建VIP的Port设备(防止该VIP被再次分配)</li><li>更新Port设备的Allowed address pairs</li></ul><p>第一步，创建Port设备<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#source</span> admin-openrc<span class="selector-class">.sh</span>   <span class="comment">//导入租户环境变量</span></span><br><span class="line"><span class="selector-id">#openstack</span> network list    <span class="comment">//查看现有网络，从中选择创建port设备的网络</span></span><br><span class="line"><span class="selector-id">#openstack</span> subnet list     <span class="comment">//查看网络下存在子网，从中选择port设备所处子网</span></span><br><span class="line"><span class="selector-id">#openstack</span> port create --network NetWork_Name --fixed-ip subnet=SubNet_Name,\</span><br><span class="line">ip-address=IP Port_Name</span><br><span class="line"><span class="selector-id">#openstack</span> port show Port_Name</span><br></pre></td></tr></table></figure></p><p>此时Port设备已经创建，但该Port设备与需要建立VIP的实例没有任何关系，在该实例上创建VIP也是不能工作的。原因在于下面<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#neutron port-list |grep Instance-IP        <span class="comment">//找到需要配置VIP的实例的Port ID</span></span></span><br></pre></td></tr></table></figure></p><p>查看该Port的详细信息<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-show <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985</span><br></pre></td></tr></table></figure></p><p>此时的allowed_address_pairs为空，就算在该实例中创建VIP，其MAC/IP也是不对应，不能工作的。那么就要进行第二步，即更新Port的allowed_address_pairs信息<br><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-update Port-ID --allowed_address_pair list-<span class="keyword">true</span> <span class="class"><span class="keyword">type</span></span>=dict ip_address=IP</span><br></pre></td></tr></table></figure></p><p>例如<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-update <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985 \</span><br><span class="line">--allowed_address_pairs <span class="type">list</span>=true type=dict ip_address=<span class="number">172.24</span><span class="number">.1</span><span class="number">.202</span></span><br></pre></td></tr></table></figure></p><p>现在再来查看实例Port信息<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-show <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985</span><br></pre></td></tr></table></figure></p><p>此时在虚拟机中创建VIP，就能够正常工作了。</p><h2 id="运维平台建设"><a href="#运维平台建设" class="headerlink" title="运维平台建设"></a>运维平台建设</h2><p>监控是整个运维乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供详实的数据用于追查定位问题。目前业界有很多不错的开源产品可供选择。选择一些开源的监控系统，是一个省时省力，效率最高的方案。</p><p>使用Kolla容器化部署和管理OpenStack云平台，已成为主流趋势。这里，我们以容器化部署和管理OpenStack云平台为背景，聊聊云平台相关的运维平台建设。</p><h3 id="监控目标"><a href="#监控目标" class="headerlink" title="监控目标"></a>监控目标</h3><p>我们先来了解什么是监控、监控的重要性以及监控的目标，当然每个人所在的行业不同、公司不同、业务不同、岗位不同，对监控的理解也不同，但是我们需要注意，监控是需要站在公司的业务角度去考虑，而不是针对某个监控技术的使用。</p><p><strong>监控的目标，包括：</strong></p><p>1）对系统不间断实时监控：实际上是对系统不间断的实时监控(这就是监控)；<br>2）实时反馈系统当前状态：我们监控某个硬件、或者某个系统，都是需要能实时看到当前系统的状态，是正常、异常、或者故障；<br>3）保证服务可靠性安全性：我们监控的目的就是要保证系统、服务、业务正常运行；<br>4）保证业务持续稳定运行：如果我们的监控做得很完善，即使出现故障，能第一时间接收到故障报警，在第一时间处理解决，从而保证业务持续性的稳定运行；</p><p><strong>监控体系分层</strong></p><p>监控有赖于运维各专业条线协同完善，通过将监控体系进行分层、分类，各专业条线再去有重点的丰富监控指标。监控的对象，主要有基础设施硬件类和应用软件类等，如下图所示：</p><p><img src="/images/monitor.png" alt="image"></p><ul><li>硬件设施层：交换机、路由器、负载均衡设备、防火墙、服务器（硬盘、CPU、内存和网卡）等。</li><li>云平台层：日志、数据库、消息队列、操作系统、OpenStack服务、Ceph存储、Docker容器、系统和应用负载等。</li><li>应用层：虚拟机、数据卷、虚拟网卡等。</li></ul><h3 id="监控手段"><a href="#监控手段" class="headerlink" title="监控手段"></a>监控手段</h3><p>通常情况下，随着系统的运行，操作系统会产生系统日志，应用程序会产生应用程序的访问日志、错误日志、运行日志、网络日志，我们可以使用 EFK 来进行日志监控。对于日志监控来说，最常见的需求就是收集、存储、查询、展示。</p><p>除了对日志进行监控外，我们还需要对系统和应用的运行状况进行实时监控。不同的监控目标，有不同的监控手段。OpenStack云资源的监控，如虚拟机、镜像、数据卷、虚拟网卡等，天然的可以由OpenStack自带的Ceilometer+Gnocchi+Aodh等服务来做（PS：ceilometer可以将采集数据交给gnocchi做数据聚合，最后用grafana来出图）。</p><p>如果，OpenStack云平台是基于Kolla容器化部署和运行管理的。那么诸如Docker容器、操作系统负载、存储空间等，又该使用什么来运维监控并告警呢。自然，TPG栈便呼之欲出了（不要问我为啥不用Zabbix）。</p><p>什么是TPIG栈。即由Telegraf + Influxdb + Grafana + Prometheus组合成的一套运维监控工具集合。它们之间的关系是。<br>Prometheus/Telegraf(收集数据) —-&gt; Influxdb(保存数据) —-&gt; Grafana(显示数据)</p><p>说明：<br>Prometheus和Telegraf不是必须同时部署使用的，你可以根据自己的需要，选择二者都部署，也可以二者选其一。</p><p>如下几种开源工具或方案，Kolla社区都是默认支持的。最重要的是，如何去使用、完善它们。</p><ul><li>日志收集和分析处理的开源方案有EFK栈：fluentd/filebeat + elasticsearch +kibana</li><li>性能采集和分析处理的开源方案有TPIG栈：telegraf + influxdb + grafana + Prometheus</li></ul><p><strong>监控方法</strong></p><p>了解监控对象：我们要监控的对象你是否了解呢？比如硬盘的IOPS？<br>对象性能指标：我们要监控这个东西的什么属性？比如 CPU 的使用率、负载、用户态、内核态、上下文切换。<br>报警阈值定义：怎么样才算是故障，要报警呢？比如 CPU 的负载到底多少算高，用户态、内核态分别跑多少算高？<br>故障处理流程：收到了故障报警，我们怎么处理呢？有什么更高效的处理流程吗？</p><p><strong>监控流程</strong></p><ul><li>数据采集：通过telegraf/Prometheus等对系统和应用进行数据采集；</li><li>数据存储：监控数据存储在MySQL、influxdb上，也可以存储在其他数据库中；</li><li>数据分析：当我们事后需要分析故障时，EFK栈 能给我们提供图形以及时间等相关信息，方面我们确定故障所在；</li><li>数据展示：web 界面展示；</li><li>监控报警：电话报警、邮件报警、微信报警、短信报警、报警升级机制等（无论什么报警都可以）；</li><li>报警处理：当接收到报警，我们需要根据故障的级别进行处理，比如:重要紧急、重要不紧急等。根据故障的级别，配合相关的人员进行快速处理；</li></ul><h3 id="监控告警"><a href="#监控告警" class="headerlink" title="监控告警"></a>监控告警</h3><p>当监控的对象超过了某一阈值或者某一服务出现了异常时，便自动发送邮件、短信或微信给相关人员进行告警。</p><p><strong>建立集中监控平台</strong></p><p>在一体化运维体系中，监控平台贯穿所有环节，它起到了生产系统涉及的软硬件环境实时运行状况的“监”，监控平台事件驱动的特性也为一体化运维体系起到神经网络驱动的作用，进而进行了“控”，另外，监控平台优质的运维数据可以作为运维大数据分析的数据源，实现运维数据采集的角色。为了提高投入效率，减少重复投入，需要建立集中监控平台实现统一展示、统一管理，支持两地三中心建设，具备灵活的扩展性，支持运维大数据分析。</p><p><strong>指标权重与阀值分级</strong></p><p>需要重点强调一下监控指标的指标权重、阀值分级与上升机制问题，做监控的人知道“监”的最重要目标是不漏报，为了不漏报在实际实施过程中会出现监控告警过多的困难。如何让运维人员在不漏处理监控事件，又能快速解决风险最高的事件，则需要监控的指标需要进行指标权重、阀值分级与上升机制：</p><p>1）指标权重：</p><p>监控指标的权重是为了定义此项监控指标是否为必须配置，比如应用软件服务、端口监听是一个应用可用性的重要指标，权重定义为一级指标；对于批量状态，则由于不少应用系统并没有批量状态，则定义为二级指标。通常来说一级指标将作为监控覆盖面的底线，通过设置好权重，一是为了让运维人员知道哪些监控指标必须确保覆盖，同时加以引入KPI考核；二是为了让监控平台建设人员有侧重的优化，实现一级指标的自动配置，无需运维人员手工配置。</p><p>2）阀值分级与上升机制：</p><p>有监控指标，就需要针对监控指标定义阀值，监控阀值的设立需要有分级机制，以分通知、预警、告警三级为例：通知需要运维人员关注，比如“交易系统登录数2000，登录成功率95%，平时登录数基线500，登录成功率96%”，由于登录成功率并未明显下降，可能是由于业务作了业务推广，运维人员只需关注当前应用运行状态再做判断；预警代表监控事件需要运维人员处理，但重要性略低，比如“CPU使用率71%，增长趋势非突增”，管理员受理到这个预警可以先设置为一个维护期，待当天找个时间集中处理；告警则必须马上处理的事件，比如“交易成功率为10%，平时为90%”这类监控事件己反映出交易运行问题。</p><p>对于升级，是指一个预警当长时间未处理时，需要有一个上升机制，转化为告警，以督办运维人员完成监控事件的处理。</p><p><strong>事件分级标准</strong></p><p>前面提到了事件分级的问题，分级是将事件当前紧急程度进行标识显示，事件升级是对于低级的事件当达到一定的程度，比如处理时间过长，则需要进行升级。我们将监控事件等级事件级别分为通知、预警、故障三种：</p><ul><li>通知：指一般的通知信息类事件。</li><li>预警：指已经出现异常，即将要引起生产故障的事件。</li><li>故障：指已经发生问题，并且已经影响到生产流程的事件，如果需要进一步细化故障级别，可以分为一般故障和紧急故障：一般故障不需要紧急处理的故障，紧急故障需要管理员紧急处理的故障。事件细分的粒度需根据各企业团队的管理要求而定。</li></ul><p><strong>事件应急响应</strong></p><p>运维最基本的指标就是保证系统的可用性，应急恢复的时效性是系统可用性的关键指标。通常来讲应急恢复的方法有不少，比如：</p><ul><li>服务整体性能下降或异常，可以考虑重启服务；</li><li>应用做过变更，可以考虑是否需要回切变更；</li><li>资源不足，可以考虑应急扩容；</li><li>应用性能问题，可以考虑调整应用参数、日志参数；</li><li>数据库繁忙，可以考虑通过数据库快照分析，优化SQL；</li><li>应用功能设计有误，可以考虑紧急关闭功能菜单；</li><li>还有很多……</li></ul><h3 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h3><p>上面，我们了解到了监控目标、监控手段、监控告警、监控方法和流程之后，我们也更需要知道监控的核心是什么。即<br>1）发现问题：当系统发生故障报警，我们会收到故障报警的信息 ；<br>2）定位问题：故障邮件一般都会写某某主机故障、具体故障的内容，我们需要对报警内容进行分析，比如一台服务器连不上：我们就需要考虑是网络问题、还是负载太高导致长时间无法连接，又或者某开发触发了防火墙禁止的相关策略等等，我们就需要去分析故障具体原因；<br>3）解决问题：当然我们了解到故障的原因后，就需要通过故障解决的优先级去解决该故障；<br>4）总结问题：当我们解决完重大故障后，需要对故障原因以及防范进行总结归纳，避免以后重复出现；</p><p><strong>最后</strong></p><p>关于，如何具体的使用EFK栈和TPG栈监控和采集OpenStack云平台的Log日志和性能数据实现一体化的运维监控告警，将在后面进行专题分享。</p><p>参考链接：<br><a href="http://www.yunweipai.com/archives/13243.html" target="_blank" rel="noopener">http://www.yunweipai.com/archives/13243.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;想一想，从事OpenStack杂七杂八的事儿，至今正好三年半了。做过QA测试（手动的、自动的）、CI（gerrit、jenkins、gitl
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>如何获取Kolla的OpenStack镜像</title>
    <link href="http://yoursite.com/2018/05/07/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96Kolla%E7%9A%84OpenStack%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2018/05/07/如何获取Kolla的OpenStack镜像/</id>
    <published>2018-05-07T15:16:22.000Z</published>
    <updated>2018-05-16T15:09:41.308Z</updated>
    
    <content type="html"><![CDATA[<p>由于，OpenStack社区自Queens版本起，便不再提供将打包好的kolla openstack镜像放在<br><a href="http://tarballs.openstack.org/kolla/images/" target="_blank" rel="noopener">该链接上</a></p><p>所以，我们要获取Kolla的OpenStack镜像，就只能依靠自己手动获取。有如下几种方法。</p><h2 id="从官方源下载镜像"><a href="#从官方源下载镜像" class="headerlink" title="从官方源下载镜像"></a>从官方源下载镜像</h2><p>安装kolla-ansible<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install kolla-ansible/</span></span><br></pre></td></tr></table></figure></p><p>编辑globals.yml文件，设置相关参数<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># vim /etc/kolla/globals.yml</span></span><br><span class="line"><span class="symbol">kolla_install_type:</span> <span class="string">"source"</span></span><br><span class="line"><span class="symbol">kolla_base_distro:</span> <span class="string">"centos"</span></span><br><span class="line"><span class="symbol">openstack_release:</span> <span class="string">"queens"</span></span><br></pre></td></tr></table></figure></p><p>下载镜像<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible -i all-in-one bootstrap-servers</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible pull</span></span><br></pre></td></tr></table></figure></p><h2 id="手动构建镜像"><a href="#手动构建镜像" class="headerlink" title="手动构建镜像"></a>手动构建镜像</h2><p>下载kolla项目<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla.git -b stable/queens</span></span><br></pre></td></tr></table></figure></p><p>生成kolla-build.conf文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pip install tox</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> kolla/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tox -e genconfig</span></span><br></pre></td></tr></table></figure></p><p>构建openstack镜像<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//构建基于centos系统的source源码安装的openstack镜像</span><br><span class="line"># kolla-<span class="keyword">build </span>-t source -<span class="keyword">b </span>centos</span><br><span class="line"></span><br><span class="line">//或者，构建基于centos系统的<span class="keyword">binary二进制包安装的openstack镜像</span></span><br><span class="line"><span class="keyword"># </span>kolla-<span class="keyword">build </span>-t <span class="keyword">binary </span>-<span class="keyword">b </span>centos</span><br></pre></td></tr></table></figure></p><h2 id="自动化拉取kolla镜像"><a href="#自动化拉取kolla镜像" class="headerlink" title="自动化拉取kolla镜像"></a>自动化拉取kolla镜像</h2><p>由于OpenStack社区，已经开始正式将kolla镜像托管在DockerHub上。所以，我们还可以从Docker Hub上直接拉取kolla镜像，由于openstack镜像少则几十，多则上百，因此，这里我编写了一个bash脚本，用于自动化拉取queens版本的kolla镜像。</p><p>为了加快从docker hub上拉取镜像，这里，配置上阿里云的镜像加速器。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat /etc/docker/daemon.json </span></span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"registry-mirrors"</span>: [<span class="string">"https://a5aghnme.mirror.aliyuncs.com"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>重启docker服务，生效<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload &amp;&amp; systemctl restart docker</span></span><br></pre></td></tr></table></figure></p><p><strong>拉取queens版本的kolla镜像</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 01_pull_kolla.sh </span></span><br><span class="line"><span class="meta">#!/usr/bin/bash</span></span><br><span class="line"></span><br><span class="line">image_tag=queens    <span class="comment">#该变量，你可以根据自己需要进行修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete images</span></span><br><span class="line">docker images | awk <span class="string">'&#123;print $3&#125;'</span> | xargs docker rmi -f</span><br><span class="line"></span><br><span class="line"><span class="comment"># pull public images</span></span><br><span class="line"><span class="keyword">for</span> public_images <span class="keyword">in</span> memcached kolla-toolbox cron mongodb mariadb rabbitmq keepalived haproxy chrony iscsid tgtd</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$public_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull monitor manage images</span></span><br><span class="line"><span class="comment"># it is recommended to use telegraf + influxdb + grafana + collectd + Prometheus</span></span><br><span class="line"><span class="keyword">for</span> monitor_images <span class="keyword">in</span> collectd telegraf grafana influxdb prometheus-server prometheus-haproxy-exporter prometheus-node-exporter prometheus-mysqld-exporter</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$monitor_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull log manage images</span></span><br><span class="line"><span class="keyword">for</span> log_images <span class="keyword">in</span> fluentd elasticsearch kibana</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$log_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull nova</span></span><br><span class="line"><span class="keyword">for</span> nova <span class="keyword">in</span> nova-compute nova-consoleauth nova-ssh nova-placement-api nova-api nova-compute-ironic nova-consoleauth nova-serialproxy nova-scheduler nova-novncproxy nova-conductor nova-libvirt</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$nova</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull keystone</span></span><br><span class="line"><span class="keyword">for</span> keystone <span class="keyword">in</span> keystone keystone-fernet keystone-ssh</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$keystone</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull freezer</span></span><br><span class="line">docker pull kolla/centos-source-freezer-api:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull glance </span></span><br><span class="line"><span class="keyword">for</span> glance <span class="keyword">in</span> glance-api glance-registry</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$glance</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull cinder</span></span><br><span class="line"><span class="keyword">for</span> cinder <span class="keyword">in</span> cinder-volume cinder-api cinder-backup cinder-scheduler</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$cinder</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull neutron</span></span><br><span class="line"><span class="keyword">for</span> neutron <span class="keyword">in</span> neutron-server neutron-lbaas-agent neutron-dhcp-agent neutron-l3-agent neutron-openvswitch-agent neutron-metadata-agent neutron-server-opendaylight opendaylight</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$neutron</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull openvswitch</span></span><br><span class="line"><span class="keyword">for</span> openvswitch <span class="keyword">in</span> openvswitch-db-server openvswitch-vswitchd neutron-openvswitch-agent</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$openvswitch</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull ceilometer</span></span><br><span class="line"><span class="keyword">for</span> ceilometer <span class="keyword">in</span> ceilometer-api ceilometer-compute ceilometer-notification ceilometer-central</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$ceilometer</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull gnocchi</span></span><br><span class="line"><span class="keyword">for</span> gnocchi <span class="keyword">in</span> gnocchi-metricd gnocchi-api gnocchi-statsd</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$gnocchi</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull aodh</span></span><br><span class="line"><span class="keyword">for</span> aodh <span class="keyword">in</span> aodh-evaluator aodh-api aodh-listener aodh-notifier</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$aodh</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull heat</span></span><br><span class="line"><span class="keyword">for</span> heat <span class="keyword">in</span> heat-api heat-api-cfn heat-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$heat</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull horizon</span></span><br><span class="line">docker pull kolla/centos-source-horizon:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull murano</span></span><br><span class="line"><span class="keyword">for</span> murano <span class="keyword">in</span> murano-api murano-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$murano</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull magnum</span></span><br><span class="line"><span class="keyword">for</span> magnum <span class="keyword">in</span> magnum-api magnum-conductor</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$magnum</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull senlin</span></span><br><span class="line"><span class="keyword">for</span> senlin <span class="keyword">in</span> senlin-api senlin-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$senlin</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull sahara</span></span><br><span class="line"><span class="keyword">for</span> sahara <span class="keyword">in</span> sahara-engine sahara-api</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$sahara</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull trove</span></span><br><span class="line"><span class="keyword">for</span> trove <span class="keyword">in</span> trove-api trove-taskmanager trove-conductor</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$trove</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull swift</span></span><br><span class="line"><span class="keyword">for</span> swift <span class="keyword">in</span> swift-rsyncd swift-proxy-server swift-object-expirer swift-object swift-account swift-container swift-base</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$swift</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull ironic</span></span><br><span class="line"><span class="keyword">for</span> ironic <span class="keyword">in</span> ironic-conductor ironic-pxe ironic-api ironic-inspector</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$ironic</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">docker pull kolla/centos-source-dnsmasq:pike</span><br><span class="line">docker tag kolla/centos-source-dnsmasq:pike kolla/centos-source-dnsmasq:queens</span><br><span class="line">docker rmi -f kolla/centos-source-dnsmasq:pike</span><br><span class="line"></span><br><span class="line"><span class="comment"># pull cloudkitty</span></span><br><span class="line"><span class="keyword">for</span> cloudkitty <span class="keyword">in</span> cloudkitty-api cloudkitty-processor panko-api</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$cloudkitty</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull kuryr</span></span><br><span class="line">docker pull kolla/centos-source-kuryr-libnetwork:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># save images</span></span><br><span class="line">images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line">docker save -o kolla_queens_images.tar <span class="variable">$images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clean pull's images</span></span><br><span class="line">docker images | awk <span class="string">'&#123;print $3&#125;'</span> | xargs docker rmi -f</span><br></pre></td></tr></table></figure></p><p><strong>将镜像push到本地Registry</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 02_push_kolla.sh </span></span><br><span class="line"><span class="meta">#!/usr/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load images</span></span><br><span class="line">docker load --input kolla_queens_images.tar</span><br><span class="line"></span><br><span class="line">registry=172.17.51.27:4000    <span class="comment">#请将该registry地址，改为你自己环境的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tag images</span></span><br><span class="line">images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> images_tag <span class="keyword">in</span> <span class="variable">$images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker tag <span class="variable">$images_tag</span>:queens <span class="variable">$registry</span>/<span class="variable">$images_tag</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete old's images</span></span><br><span class="line">delete_images=`docker images | grep <span class="string">'^kolla'</span> | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> delete_images1 <span class="keyword">in</span> <span class="variable">$delete_images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker rmi <span class="variable">$delete_images1</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># push images</span></span><br><span class="line">push_images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> push_images1 <span class="keyword">in</span> <span class="variable">$push_images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker push <span class="variable">$push_images1</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>综合，比较以上三种方法的优缺点。这里，推荐使用第三种方法，速度更快，也更便捷。<br>PS：脚本写得有点搓，不喜可喷。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于，OpenStack社区自Queens版本起，便不再提供将打包好的kolla openstack镜像放在&lt;br&gt;&lt;a href=&quot;http://tarballs.openstack.org/kolla/images/&quot; target=&quot;_blank&quot; rel=&quot;noop
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/categories/OpenStack/Kolla/"/>
    
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>如何设置OpenStack节点Swap分区</title>
    <link href="http://yoursite.com/2018/05/07/%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEOpenStack%E8%8A%82%E7%82%B9Swap%E5%88%86%E5%8C%BA/"/>
    <id>http://yoursite.com/2018/05/07/如何设置OpenStack节点Swap分区/</id>
    <published>2018-05-07T13:02:51.000Z</published>
    <updated>2018-05-07T14:09:46.724Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Swap分区介绍"><a href="#Swap分区介绍" class="headerlink" title="Swap分区介绍"></a>Swap分区介绍</h2><p>Swap分区，即交换分区。它的功能就是在物理内存不够的情况下，操作系统先把内存中暂时不用的数据，存到硬盘的交换空间，腾出内存来让别的程序运行，当程序需要用到交换空间内的数据的时候，操作系统再将数据从交换分区恢复到物理内存中。这样，系统总是在物理内存不够时，才进行Swap交换。                </p><h2 id="如何设置Swap分区大小"><a href="#如何设置Swap分区大小" class="headerlink" title="如何设置Swap分区大小"></a>如何设置Swap分区大小</h2><p>以上是SWAP 交换分区的作用。 实际上，我们更关注的应该是SWAP分区的大小问题。 设置多大才是最优的。如下，提供了两种方案。</p><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><p>在Linux系统中，我们可以参照Red Hat公司为RHEL 7推荐的SWAP空间的大小划分原则，在你没有其他特别需求时，可以作为很好的参考依据。</p><ul><li>内存小于2GB，推荐2倍于内存的swap空间；</li><li>内存2GB~8GB，推荐和内存大小一样的swap空间；</li><li>内存8GB~64GB，推荐至少4GB的swap空间；</li><li>内存大于64GB，推荐至少4GB的swap空间。</li></ul><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-swapspace" target="_blank" rel="noopener">原文链接</a></p><p>实际上，系统中交换分区的大小并不取决于物理内存的量，而是取决于系统中内存的负荷，所以在安装系统时要根据具体的业务来设置SWAP的值。</p><p>在OpenStack中，默认的CPU超配比例是1:16，内存超配比例是1:1.5。当宿主机使用swap交换分区来为虚拟机分配内存的时候，则虚拟机的性能将急速下降。生产环境上不建议开启内存超售（建议配置比例1:1）。另外，建议设置nova.conf文件中的reserved_host_memory_mb 参数，即内存预留量（建议至少预留4GB），保证该部分内存不能被虚拟机使用。</p><h3 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h3><p>系统在什么情况下才会使用Swap？实际上，并不是等所有的物理内存都消耗完毕之后，才去使用swap的空间，什么时候使用是由swappiness 参数值控制的。<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat <span class="regexp">/proc/</span>sys<span class="regexp">/vm/</span>swappiness</span><br><span class="line"><span class="number">60</span></span><br></pre></td></tr></table></figure></p><p>该值默认值是60。</p><ul><li>swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间。</li><li>swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。</li></ul><p>由于，现在服务器的内存一般是上百GB，所以我们可以把这个参数值设置的低一些（如10-30之间），让操作系统尽可能的使用物理内存，降低系统对swap的使用，从而提高宿主机系统和虚拟机的性能。</p><p>永久性修改<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'vm.swappiness=10'</span> &gt;&gt;/etc/sysctl.conf</span></span><br></pre></td></tr></table></figure></p><p>保存，重启就生效了。</p><p>查看系统当前SWAP 空间大小<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> free –h</span></span><br></pre></td></tr></table></figure></p><p><strong>小结</strong></p><p>为了保证主机系统和应用程序的稳定运行（内存不足或泄露，易导致系统或应用崩溃），建议在实际使用过程中，服务器仍然需要创建一定的Swap分区。其分区大小可以结合以上两种方案进行设置，已达到最佳效果。</p><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><p><strong>1.释放SWAP 空间</strong></p><p>假设我们的系统出现了性能问题，我们通过vmstat命令看到有大量的swap，而我们的物理内存又很充足，那么我们可以手工把swap 空间释放出来。让进程去使用物理内存，从而提高性能。</p><p>我们对swap 空间的释放，可以通过关闭swap分区，再启动swap 分区来实现。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vmstat 1 5      // 1表示每隔1秒采集一次服务器状态，5表示只采集5次</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> free -h</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> swapon -s      //显示交换分区的使用状况</span></span><br></pre></td></tr></table></figure></p><p>关闭swap 交换分区：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapoff /dev/sda2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> swapon -s</span></span><br></pre></td></tr></table></figure></p><p>启用swap分区：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapon /dev/sda2</span></span><br></pre></td></tr></table></figure></p><p>验证状态：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapon -s</span></span><br></pre></td></tr></table></figure></p><p>Swap分区的拓展和缩小：<a href="https://www.e-learn.cn/content/linux/339010" target="_blank" rel="noopener">https://www.e-learn.cn/content/linux/339010</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Swap分区介绍&quot;&gt;&lt;a href=&quot;#Swap分区介绍&quot; class=&quot;headerlink&quot; title=&quot;Swap分区介绍&quot;&gt;&lt;/a&gt;Swap分区介绍&lt;/h2&gt;&lt;p&gt;Swap分区，即交换分区。它的功能就是在物理内存不够的情况下，操作系统先把内存中暂时不用的数
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Kolla中配置OpenStack虚机网络vxlan和vlan共存</title>
    <link href="http://yoursite.com/2018/05/03/Kolla%E4%B8%AD%E9%85%8D%E7%BD%AEOpenStack%E8%99%9A%E6%9C%BA%E7%BD%91%E7%BB%9Cvxlan%E5%92%8Cvlan%E5%85%B1%E5%AD%98/"/>
    <id>http://yoursite.com/2018/05/03/Kolla中配置OpenStack虚机网络vxlan和vlan共存/</id>
    <published>2018-05-03T12:59:32.000Z</published>
    <updated>2018-05-07T12:55:48.347Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack Neutron网络服务定义了四种网络模式：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = <span class="built_in">local</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = vlan   </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = gre</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = vxlan</span></span><br></pre></td></tr></table></figure></p><p>这里，本文以vlan、vxlan为例，阐述如何实现OpenStack虚机网络（亦称租户网络、业务网络）同时支持vxlan和vlan两种网络。</p><p><strong>说明</strong></p><ul><li>环境：Openstack queens版本</li><li>部署工具：kolla-ansible</li></ul><p>在kolla-ansible部署节点的/etc/kolla/globals.yml文件中，配置网卡。如下所示。</p><p><img src="/images/globals.png" alt="image"></p><ul><li>eth0：openstack管理网络；vlan 51，交换机端口设置为Access模式</li><li>eth1：虚机网络(vxlan)；vlan 52，交换机端口设置为Access模式</li><li>eth2：外部网络兼虚机网络(vlan)；vlan网段53-54，交换机端口设置为trunk模式，主机不配置IP地址</li></ul><h2 id="在所有网络节点上，操作如下"><a href="#在所有网络节点上，操作如下" class="headerlink" title="在所有网络节点上，操作如下"></a>在所有网络节点上，操作如下</h2><p>修改文件/etc/kolla/neutron-server/ml2_conf.ini</p><p><img src="/images/network1.png" alt="image"> </p><p>修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini</p><p><img src="/images/network2.png" alt="image"> </p><p>重启neutron容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker restart neutron_server neutron_openvswitch_agent</span></span><br></pre></td></tr></table></figure></p><p>在网络节点上，查看br-ex网桥设置情况，如下。</p><p><img src="/images/networkbrex.png" alt="image"> </p><h2 id="在所有计算节点上，操作如下"><a href="#在所有计算节点上，操作如下" class="headerlink" title="在所有计算节点上，操作如下"></a>在所有计算节点上，操作如下</h2><p>修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini</p><p><img src="/images/compute.png" alt="image"></p><p>创建一个br-ex外部网桥，并关联到主机的eth2物理网卡上。这样，当计算节点上的虚拟机使用vlan网络时，便可以直接通过qbr-&gt;br-int-&gt;br-ex-&gt;eth2连接到外网。（vlan网络的三层路由，建议使用物理路由器，这样性能和稳定性更好，而不需要通过网络节点上的L3 vRouter虚拟路由）。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># docker exec -<span class="keyword">u</span> root -it neutron_openvswitch_agent ovs-vsctl <span class="built_in">add</span>-<span class="keyword">br</span> <span class="keyword">br</span>-<span class="keyword">ex</span></span><br><span class="line"># docker exec -<span class="keyword">u</span> root -it neutron_openvswitch_agent ovs-vsctl <span class="built_in">add</span>-port <span class="keyword">br</span>-<span class="keyword">ex</span> eth2</span><br></pre></td></tr></table></figure></p><p>最后，重启相关容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker restart neutron_openvswitch_agent</span></span><br></pre></td></tr></table></figure></p><p>在计算节点上，查看br-ex网桥设置情况，如下。</p><p><img src="/images/computebrex.png" alt="image"></p><p>创建一个vlan id为53的网段<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span> <span class="comment">neutron</span> <span class="comment">net</span><span class="literal">-</span><span class="comment">create</span> <span class="comment">vlan</span><span class="literal">-</span><span class="comment">53</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">shared</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:physical_network</span> <span class="comment">physnet1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:network_type</span> <span class="comment">vlan</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:segmentation_id</span> <span class="comment">53</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#</span> <span class="comment">neutron</span> <span class="comment">subnet</span><span class="literal">-</span><span class="comment">create</span> <span class="comment">vlan</span><span class="literal">-</span><span class="comment">53</span> <span class="comment">172</span><span class="string">.</span><span class="comment">17</span><span class="string">.</span><span class="comment">53</span><span class="string">.</span><span class="comment">0/24</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">name</span> <span class="comment">provider</span><span class="literal">-</span><span class="comment">53</span><span class="literal">-</span><span class="comment">subnet</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gateway</span> <span class="comment">172</span><span class="string">.</span><span class="comment">17</span><span class="string">.</span><span class="comment">53</span><span class="string">.</span><span class="comment">1</span></span><br></pre></td></tr></table></figure></p><p>查看创建的网络，如下。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># neutron net-<span class="type">list</span></span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br><span class="line">| id                                   | name           | tenant_id                        | subnets                                             |</span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br><span class="line">| <span class="number">5</span>d9c4874-e03b<span class="number">-4</span>bde-aee0<span class="number">-947</span>d7dde4860 | vlan<span class="number">-53</span>        | <span class="number">48</span>fbadff0ab84229b429166babbe488f | <span class="number">9</span>bade37c-ff44<span class="number">-4004</span><span class="number">-8e82</span><span class="number">-20</span>d61348fdc0 <span class="number">172.17</span><span class="number">.53</span><span class="number">.0</span>/<span class="number">24</span> |</span><br><span class="line">| <span class="number">7</span>b0152da-a975<span class="number">-4</span>dbf-b35b<span class="number">-437951</span>c66efa | tenant_network | <span class="number">48</span>fbadff0ab84229b429166babbe488f | a45516a4<span class="number">-4</span>ce9<span class="number">-4</span>c2e<span class="number">-8052</span><span class="number">-8</span>c71eae0e219 <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span>    |</span><br><span class="line">| <span class="number">9630</span>cf8b<span class="number">-4072</span><span class="number">-415</span>b-a9a9<span class="number">-99</span>ff815748f8 | public_network | <span class="number">48</span>fbadff0ab84229b429166babbe488f | a98f8c80<span class="number">-78</span>de<span class="number">-43</span>ba-af52-d86c19fc59ef <span class="number">172.17</span><span class="number">.54</span><span class="number">.0</span>/<span class="number">24</span> |</span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br></pre></td></tr></table></figure></p><p>最后，创建一个虚拟机并使用该vlan网络。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># nova boot --flavor <span class="number">1</span>Gmem_1cpu --image centos7 --nic net-id=<span class="number">5</span>d9c4874-e03b<span class="number">-4</span>bde-aee0<span class="number">-947</span>d7dde4860 test_vm</span><br><span class="line"></span><br><span class="line"># nova <span class="type">list</span> | grep test_vm</span><br><span class="line">| f506129b<span class="number">-610</span>f<span class="number">-4e2</span>d<span class="number">-886</span>b<span class="number">-5</span>d791cdcb282 | test_vm | <span class="literal">ACTIVE</span> | -  | Running | vlan<span class="number">-53</span>=<span class="number">172.17</span><span class="number">.53</span><span class="number">.7</span></span><br></pre></td></tr></table></figure></p><p>测试虚拟机网络通信<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -c 4 172.17.53.7</span></span><br><span class="line">PING 172.17.53.7 (172.17.53.7) 56(84) bytes of data.</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=1 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.421 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=2 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.503 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=3 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.543 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=4 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.469 ms</span><br></pre></td></tr></table></figure></p><p><strong>br-int和br-ex说明</strong></p><ul><li>br-int</li></ul><p>br-int是OpenVswitch中的集成网桥，类似于一个二层的交换机。上面挂载了大量的agent来提供各种网络服务，另外负责对发往br-ex的流量，实现local vlan转化为外部vlan。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ovs-ofctl dump-flows br-int  </span></span><br><span class="line">NXST_FLOW reply (<span class="attribute">xid</span>=0x4):  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=147294.121s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=224, <span class="attribute">n_bytes</span>=33961, <span class="attribute">idle_age</span>=13, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=3,in_port=4,dl_vlan=1 <span class="attribute">actions</span>=mod_vlan_vid:101,NORMAL  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=603538.84s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=19, <span class="attribute">n_bytes</span>=2234, <span class="attribute">idle_age</span>=18963, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=2,in_port=4 <span class="attribute">actions</span>=drop  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=603547.134s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=31901, <span class="attribute">n_bytes</span>=6419756, <span class="attribute">idle_age</span>=13, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=1 <span class="attribute">actions</span>=NORMAL</span><br></pre></td></tr></table></figure></p><ul><li>br-ex</li></ul><p>br-ex是OpenVswitch中的一个外部网桥，要做的事情很简单，只需要正常转发数据流量即可。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ovs-ofctl dump-flows br-ex  </span></span><br><span class="line">NXST_FLOW reply (<span class="attribute">xid</span>=0x4):  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=6770.969s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=5411, <span class="attribute">n_bytes</span>=306944, <span class="attribute">idle_age</span>=0, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=0 <span class="attribute">actions</span>=NORMAL</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;OpenStack Neutron网络服务定义了四种网络模式：&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;spa
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>如何删除Registry中kolla-ansible的镜像</title>
    <link href="http://yoursite.com/2018/04/30/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4Registry%E4%B8%ADkolla-ansible%E7%9A%84%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2018/04/30/如何删除Registry中kolla-ansible的镜像/</id>
    <published>2018-04-30T15:16:19.000Z</published>
    <updated>2018-05-03T13:16:53.453Z</updated>
    
    <content type="html"><![CDATA[<p>出于某些情况，如释放磁盘空间、旧镜像删除等原因，需要我们删除本地Registry仓库中的镜像。本篇文章，将讲解如何在OpenStack环境的kolla-ansible中，删除本地Registry中的镜像。</p><h2 id="Registry中的镜像管理"><a href="#Registry中的镜像管理" class="headerlink" title="Registry中的镜像管理"></a>Registry中的镜像管理</h2><p>查看Registry仓库中现有的镜像：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> -XGET http:<span class="comment">//172.17.51.51:4000/v2/_catalog</span></span><br></pre></td></tr></table></figure></p><p>查看Registry仓库中指定的镜像，如这里的centos-source-magnum-conductor。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> -XGET http:<span class="comment">//172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/tags/list</span></span><br></pre></td></tr></table></figure></p><h2 id="如何删除私有-registry-中的镜像"><a href="#如何删除私有-registry-中的镜像" class="headerlink" title="如何删除私有 registry 中的镜像"></a>如何删除私有 registry 中的镜像</h2><p>首先，在默认情况下，docker registry 是不允许删除镜像的，需要在配置文件config.yml中启用。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#vim /etc/docker/registry/config.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">log:</span></span><br><span class="line"><span class="attr">  fields:</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line"><span class="attr">  cache:</span></span><br><span class="line"><span class="attr">    blobdescriptor:</span> <span class="string">inmemory</span></span><br><span class="line"><span class="attr">  filesystem:</span></span><br><span class="line"><span class="attr">    rootdirectory:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">  delete:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="attr">  addr:</span> <span class="string">:5000</span></span><br><span class="line"><span class="attr">  headers:</span></span><br><span class="line"><span class="attr">    X-Content-Type-Options:</span> <span class="string">[nosniff]</span></span><br><span class="line"><span class="attr">health:</span></span><br><span class="line"><span class="attr">  storagedriver:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    interval:</span> <span class="number">10</span><span class="string">s</span></span><br><span class="line"><span class="attr">    threshold:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>修改后，需要重启registry容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">docker restart registry</span></span><br></pre></td></tr></table></figure></p><p>使用API接口 GET /v2/&lt;镜像名&gt;/manifests/<tag> 来取得要删除的镜像:Tag所对应的 digest。比如，要删除kolla/centos-source-magnum-conductor:queens镜像，那么取得 digest 的命令是：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> --<span class="selector-tag">header</span> <span class="string">"Accept: application/vnd.docker.distribution.manifest.v2+json"</span> -I -X HEAD http:<span class="comment">//172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/queens</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">Content-Length: <span class="number">8666</span></span><br><span class="line">Content-Type: application/vnd<span class="selector-class">.docker</span><span class="selector-class">.distribution</span><span class="selector-class">.manifest</span><span class="selector-class">.v2</span>+json</span><br><span class="line">Docker-Content-Digest: sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868</span><br><span class="line">Docker-Distribution-Api-Version: registry/<span class="number">2.0</span></span><br><span class="line">Etag: <span class="string">"sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868"</span></span><br><span class="line">X-Content-Type-Options: nosniff</span><br><span class="line">Date: Sat, <span class="number">28</span> Apr <span class="number">2018</span> <span class="number">02</span>:<span class="number">44</span>:<span class="number">46</span> GMT</span><br></pre></td></tr></table></figure></tag></p><p>得到 Docker-Content-Digest:<br><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sha<span class="number">256</span>:e<span class="number">94</span><span class="keyword">c</span><span class="number">4</span>d<span class="number">08520</span>a<span class="number">7</span>f<span class="number">77</span>cbfa<span class="number">0</span><span class="keyword">c</span><span class="number">2</span>d<span class="number">314</span>bc<span class="number">9281</span>d<span class="number">07874</span>b<span class="number">8</span><span class="keyword">c</span><span class="number">7</span>d<span class="number">9337</span>ad<span class="number">5</span>f<span class="number">541832</span>f<span class="number">7</span>d<span class="number">868</span></span><br></pre></td></tr></table></figure></p><p>然后调用API接口 DELETE /v2/&lt;镜像名&gt;/manifests/<digest> 来删除镜像。比如：<br><figure class="highlight ldif"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#curl -I -X DELETE http://172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868</span></span><br><span class="line"><span class="attribute">HTTP/1.1 202 Accepted</span></span><br><span class="line"><span class="attribute">Docker-Distribution-Api-Version</span>: registry/2.0</span><br><span class="line"><span class="attribute">X-Content-Type-Options</span>: nosniff</span><br><span class="line"><span class="attribute">Date</span>: Sat, 28 Apr 2018 03:34:31 GMT</span><br><span class="line"><span class="attribute">Content-Length</span>: 0</span><br><span class="line"><span class="attribute">Content-Type</span>: text/plain; charset=utf-8</span><br></pre></td></tr></table></figure></digest></p><p>至此，镜像已从 registry 中标记删除，外界访问 pull 不到了。但是 registry 的本地空间并未释放，需要垃圾收集才会释放。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">docker <span class="built_in">exec</span> registry bin/registry garbage-collect /etc/docker/registry/config.yml</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;出于某些情况，如释放磁盘空间、旧镜像删除等原因，需要我们删除本地Registry仓库中的镜像。本篇文章，将讲解如何在OpenStack环境的kolla-ansible中，删除本地Registry中的镜像。&lt;/p&gt;
&lt;h2 id=&quot;Registry中的镜像管理&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
</feed>
