<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>起风了</title>
  
  <subtitle>xuchao&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-26T14:02:52.393Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>XuChao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Ceph安装和离线源制作(mimic版本)</title>
    <link href="http://yoursite.com/2019/03/22/Ceph%E7%A6%BB%E7%BA%BF%E6%BA%90%E5%88%B6%E4%BD%9C-mimic%E7%89%88%E6%9C%AC/"/>
    <id>http://yoursite.com/2019/03/22/Ceph离线源制作-mimic版本/</id>
    <published>2019-03-22T11:48:52.000Z</published>
    <updated>2019-03-26T14:02:52.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>系统：Centos 7（系统最小化安装）<br>版本：Ceph mimic</p><h2 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h2><p>配置主机名hostname、hosts、关闭firewalld、ssh无密码登录、ntp时间同步等，过程略。</p><p>保存下载rpm包，安装或升级的同时保留RPM包。yum 默认情况下，升级或者安装后会删除下载的rpm包。不过，我们也可以设置不删除下载的rpm包。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim/etc/yum.conf</span><br><span class="line">[main]</span><br><span class="line">cachedir=/var/cache/yum</span><br><span class="line">keepcache=0</span><br></pre></td></tr></table></figure></p><p>将 keepcache=0 修改为 keepcache=1， 安装或者升级后，在目录 /var/cache/yum 下就会有下载的 rpm 包了。</p><p>配置源，使用阿里源<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rm -f <span class="string">/etc/yum.repos.d/</span>*</span><br><span class="line">wget -O <span class="string">/etc/yum.repos.d/CentOS-Base.repo</span> http:<span class="string">//mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">wget -O <span class="string">/etc/yum.repos.d/epel.repo</span> http:<span class="string">//mirrors.aliyun.com/repo/epel-7.repo</span></span><br></pre></td></tr></table></figure></p><p>删除阿里内网地址<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="selector-tag">i</span> <span class="string">'/aliyuncs.com/d'</span> /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span><span class="comment">/*.repo</span></span><br></pre></td></tr></table></figure></p><p>创建ceph源<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">echo '</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">'&gt;/etc/yum.repos.d/ceph.repo</span><br></pre></td></tr></table></figure></p><p>生成缓存<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all <span class="meta">&amp;&amp; yum makecache</span></span><br></pre></td></tr></table></figure></p><p>指定安装版本的源<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">CEPH_DEPLOY_REPO_URL</span>=https://mirrors.aliyun.com/ceph/rpm-mimic/el7</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CEPH_DEPLOY_GPG_URL</span>=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br></pre></td></tr></table></figure></p><h2 id="安装Ceph"><a href="#安装Ceph" class="headerlink" title="安装Ceph"></a>安装Ceph</h2><p>ceph-node1：Ceph mon+osd节点<br>ceph-node2：Ceph mon+osd节点<br>ceph-node3；Ceph mon+osd节点</p><p>安装ceph-deploy配置工具<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> -y ceph-deploy</span><br></pre></td></tr></table></figure></p><p>创建配置目录<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="string">/etc/ceph</span> &amp;&amp; <span class="keyword">cd</span> <span class="string">/etc/ceph/</span></span><br></pre></td></tr></table></figure></p><p>初始化Mon配置<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy new --public-network <span class="number">192.168</span><span class="number">.111</span><span class="number">.0</span>/<span class="number">24</span> ceph-node&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure></p><p>配置网络,单网卡忽略 修改冗余份数为2，日志大小2G<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">echo '</span><br><span class="line">mon_clock_drift_allowed = 2    </span><br><span class="line">osd_journal_size = 4086</span><br><span class="line">osd_pool_default_pg_num = 128</span><br><span class="line">osd_pool_default_pgp_num = 128</span><br><span class="line">osd pool default size = 2</span><br><span class="line">osd pool default min size = 1</span><br><span class="line">rbd_default_features = 1</span><br><span class="line">client_quota = true</span><br><span class="line">'&gt;&gt;./ceph.conf</span><br></pre></td></tr></table></figure></p><p>安装Ceph<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install --release mimic ceph-admin ceph-<span class="keyword">node</span><span class="title">&#123;1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure></p><p>初始化monitor和key<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy <span class="comment">--overwrite-conf mon create-initial</span></span><br></pre></td></tr></table></figure></p><p>分发拷贝配置及密钥<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod <span class="number">644</span> /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">ceph-deploy admin ceph-admin ceph-<span class="keyword">node</span><span class="title">&#123;1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure></p><p>创建存储节点，清空磁盘<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk zap ceph-node1 <span class="regexp">/dev/</span>vdb</span><br><span class="line">ceph-deploy disk zap ceph-node2 <span class="regexp">/dev/</span>vdb</span><br><span class="line">ceph-deploy disk zap ceph-node3 <span class="regexp">/dev/</span>vdb</span><br></pre></td></tr></table></figure></p><p>查看可用磁盘<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk <span class="built_in">list</span> ceph-node1</span><br><span class="line">ceph-deploy disk <span class="built_in">list</span> ceph-node2</span><br><span class="line">ceph-deploy disk <span class="built_in">list</span> ceph-node3</span><br></pre></td></tr></table></figure></p><p>创建osd<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy <span class="params">--overwrite-conf</span> osd create ceph-node1 <span class="params">--data</span> <span class="string">/dev/vdb</span> </span><br><span class="line">ceph-deploy <span class="params">--overwrite-conf</span> osd create ceph-node2 <span class="params">--data</span> <span class="string">/dev/vdb</span> </span><br><span class="line">ceph-deploy <span class="params">--overwrite-conf</span> osd create ceph-node3 <span class="params">--data</span> <span class="string">/dev/vdb</span></span><br></pre></td></tr></table></figure></p><p>创建 ceph mgr 管理进程服务，自12版本启用<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy <span class="comment">--overwrite-conf mgr create ceph-node1</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf mgr create ceph-node2</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf mgr create ceph-node3</span></span><br></pre></td></tr></table></figure></p><p>创建mon<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy <span class="comment">--overwrite-conf mon create ceph-node1</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf admin ceph-node1</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf mon create ceph-node2</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf admin ceph-node2</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf mon create ceph-node3</span></span><br><span class="line">ceph-deploy <span class="comment">--overwrite-conf admin ceph-node3</span></span><br></pre></td></tr></table></figure></p><p>取消Cephx认证（可选）<br>把下列配置加入 Ceph 配置文件的 [global] 段下即可禁用 cephx 认证<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auth cluster required = none</span><br><span class="line">auth<span class="built_in"> service </span>required = none</span><br><span class="line">auth<span class="built_in"> client </span>required = none</span><br></pre></td></tr></table></figure></p><p>修改后，需要重启 Ceph服务，在每个ceph节点上都需要执行<br><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart ceph-mon.<span class="keyword">target</span></span><br><span class="line">systemctl restart ceph-osd.<span class="keyword">target</span></span><br></pre></td></tr></table></figure></p><p>启用dashboard (在mon节点)<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mgr <span class="class"><span class="keyword">module</span> <span class="title">enable</span> <span class="title">dashboard</span></span></span><br></pre></td></tr></table></figure></p><p>设置dashboard的ip和端口<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph <span class="built_in">config</span>-key <span class="built_in">put</span> mgr/dashboard/server_addr ceph-node1</span><br><span class="line">ceph <span class="built_in">config</span>-key <span class="built_in">put</span> mgr/dashboard/server_port <span class="number">8443</span></span><br><span class="line">systemctl restart ceph-mgr@ceph-node1</span><br></pre></td></tr></table></figure></p><p>生成并安装自签名证书<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ceph dashboard <span class="keyword">create</span>-<span class="keyword">self</span>-signed-cert</span><br><span class="line"><span class="keyword">Self</span>-signed certificate created</span><br></pre></td></tr></table></figure></p><p>创建具有管理员角色的用户<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ceph dashboard <span class="keyword">set</span>-login-credentials <span class="comment">admin admin</span></span><br><span class="line">Username <span class="comment">and password updated</span></span><br></pre></td></tr></table></figure></p><p>登录Web界面<br><img src="/images/ceph-dashboard.png" alt="image"></p><h2 id="制作repo文件"><a href="#制作repo文件" class="headerlink" title="制作repo文件"></a>制作repo文件</h2><p>拷贝安装下载的ceph rpm包到指定文件夹<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find <span class="meta-keyword">/var/</span>cache<span class="meta-keyword">/yum/</span> -name *.rpm | xargs -i <span class="class">cp </span>&#123;&#125; ceph_mimic_rpms/</span><br></pre></td></tr></table></figure></p><p>这里要用到createrepo命令，如果没有此命令需要先安装它<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> createrepo -y</span><br></pre></td></tr></table></figure></p><p>生成一个repodata文件夹<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">createrepo</span> ceph_mimic_rpms/</span><br></pre></td></tr></table></figure></p><p>之后就可以正常离线安装ceph。如果机器无法访问外网，需要事先从<a href="https://download.ceph.com/keys/release.asc" target="_blank" rel="noopener">https://download.ceph.com/keys/release.asc</a> 下载对应的release.asc文件，上传到集群的每一个节点上，执行如下命令：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm --<span class="keyword">import</span> <span class="string">'./release.asc'</span></span><br></pre></td></tr></table></figure></p><p>rpm包更新<br>在使用过程中，可能还需要添加其他rpm包，把rpm包丢到ceph_mimic_rpms/目录下，我们不用重新制作repodata文件，只需要update一下。<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createrepo <span class="comment">--update ceph_mimic_rpms/</span></span><br></pre></td></tr></table></figure></p><h2 id="本机使用ceph离线源"><a href="#本机使用ceph离线源" class="headerlink" title="本机使用ceph离线源"></a>本机使用ceph离线源</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span>/backup</span><br><span class="line">mv /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span><span class="comment">/*.repo  /etc/yum.repos.d/backup</span></span><br></pre></td></tr></table></figure><p>在文件 /etc/yum.repos.d/ceph-deploy.repo中添加以下内容，保存<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[LocalCeph]</span></span><br><span class="line"><span class="attr">name</span>=LocalCeph</span><br><span class="line"><span class="attr">baseurl</span>=file:///opt/ceph_mimic_rpms    //ceph包存放目录</span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="number">0</span></span><br><span class="line"><span class="attr">enabled</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>执行下面命令生效<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum clean <span class="keyword">all</span></span><br><span class="line">yum makecache</span><br><span class="line">yum <span class="keyword">list</span> | <span class="keyword">grep</span> ceph</span><br></pre></td></tr></table></figure></p><p>最后，离线安装ceph，过程同上。</p><h2 id="其他机器使用ceph离线源"><a href="#其他机器使用ceph离线源" class="headerlink" title="其他机器使用ceph离线源"></a>其他机器使用ceph离线源</h2><p>在ceph源机器上安装nginx，createrepo，reposync<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> nginx createrepo yum-utils -y</span><br></pre></td></tr></table></figure></p><p>在nginx根目录建立文件夹<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="regexp">/usr/</span>share<span class="regexp">/nginx/</span>html<span class="regexp">/yum/</span>x86_64<span class="regexp">/ceph/</span>RPMS</span><br></pre></td></tr></table></figure></p><p>启动nginx<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="builtin-name">enable</span> nginx &amp;&amp; systemctl restart nginx</span><br></pre></td></tr></table></figure></p><p>将下载的ceph包放在/usr/share/nginx/html/yum/x86_64/ceph/RPMS目录下</p><p>自动生成repodata<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createrepo -p <span class="regexp">/usr/</span>share<span class="regexp">/nginx/</span>html<span class="regexp">/yum/</span>x86_64<span class="regexp">/ceph/</span></span><br></pre></td></tr></table></figure></p><p>在client端创建repo文件ceph-deploy.repo<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Ceph]</span></span><br><span class="line"><span class="attr">name</span>=Ceph</span><br><span class="line"><span class="attr">baseurl</span>=http://server IP/yum/x<span class="number">86_64</span>/ceph</span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="number">0</span></span><br><span class="line"><span class="attr">enabled</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>验证<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum clean <span class="keyword">all</span></span><br><span class="line">yum makecache</span><br><span class="line">yum <span class="keyword">list</span> | <span class="keyword">grep</span> ceph</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;p&gt;系统：Centos 7（系统最小化安装）&lt;br&gt;版本：Ceph mimic&lt;/p&gt;
&lt;h2 id=&quot;系统配置&quot;&gt;&lt;a href=&quot;#系统配置
      
    
    </summary>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/Ceph/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>使用cert-manager实现Ingress https</title>
    <link href="http://yoursite.com/2019/03/14/%E4%BD%BF%E7%94%A8cert-manager%E5%AE%9E%E7%8E%B0Ingress-https/"/>
    <id>http://yoursite.com/2019/03/14/使用cert-manager实现Ingress-https/</id>
    <published>2019-03-14T11:21:36.000Z</published>
    <updated>2019-03-16T12:47:33.762Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是https"><a href="#什么是https" class="headerlink" title="什么是https"></a>什么是https</h2><p>超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。</p><p>为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS，为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。</p><h2 id="什么是cert-manager"><a href="#什么是cert-manager" class="headerlink" title="什么是cert-manager"></a>什么是cert-manager</h2><p>cert-manager 是一个云原生证书管理开源项目，用于在 Kubernetes 集群中提供 HTTPS 证书并自动续期，支持 Let’s Encrypt, HashiCorp Vault 这些免费证书的签发。在Kubernetes集群中，我们可以通过 Kubernetes Ingress 和 Let’s Encrypt 实现外部服务的自动化 HTTPS。</p><p>在Kubernetes集群中使用 HTTPS 协议，需要一个证书管理器、一个证书自动签发服务，主要通过 Ingress 来发布 HTTPS 服务，因此需要Ingress Controller并进行配置，启用 HTTPS 及其路由。<br><img src="/images/cert-manager.png" alt="image"></p><p><strong>环境依赖</strong></p><ul><li>本文使用 Helm 安装，所以请确保 Helm 已安装，且版本最好&gt;2.10</li><li>集群必须已经装有 Ingress Controller</li><li>外部客户端配置hosts，IP 指向 Ingress Controller 对外暴露的地址（如果IP是公网地址并做了域名解析，则无需配置）</li></ul><h2 id="部署cert-manager"><a href="#部署cert-manager" class="headerlink" title="部署cert-manager"></a>部署cert-manager</h2><h3 id="使用helm安装cert-manager"><a href="#使用helm安装cert-manager" class="headerlink" title="使用helm安装cert-manager"></a>使用helm安装cert-manager</h3><p>安装 CustomResourceDefinition资源<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https:<span class="regexp">//</span>raw.githubusercontent.com<span class="regexp">/jetstack/</span>cert-manager<span class="regexp">/release-0.7/</span>deploy<span class="regexp">/manifests/</span><span class="number">00</span>-crds.yaml</span><br></pre></td></tr></table></figure></p><p>创建cert-manager namespace<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="keyword">create</span> <span class="keyword">namespace</span> cert-manager</span><br></pre></td></tr></table></figure></p><p>标记cert-Manager命名空间以禁用资源验证<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="selector-tag">label</span> namespace cert-manager certmanager<span class="selector-class">.k8s</span><span class="selector-class">.io</span>/disable-validation=true</span><br></pre></td></tr></table></figure></p><p>添加 Jetstack Helm repository<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo <span class="keyword">add</span> jetstack https:<span class="comment">//charts.jetstack.io</span></span><br></pre></td></tr></table></figure></p><p>更新本地Helm chart repository<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">helm repo update</span></span><br></pre></td></tr></table></figure></p><p>使用Helm chart安装cert-manager<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">helm install \</span><br><span class="line">  -<span class="ruby">-name cert-manager \</span></span><br><span class="line"><span class="ruby">  --namespace cert-manager \</span></span><br><span class="line"><span class="ruby">  --version v<span class="number">0</span>.<span class="number">7.0</span> \</span></span><br><span class="line"><span class="ruby">  jetstack/cert-manager</span></span><br></pre></td></tr></table></figure></p><p>查看cert-manager部署结果<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods --namespace cert-manager</span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">cert-manager<span class="number">-5658</span>b7db79<span class="number">-824</span>lt              <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">11</span>h</span><br><span class="line">cert-manager-cainjector<span class="number">-768</span>fd47f68-ls6zh   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">11</span>h</span><br><span class="line">cert-manager-webhook<span class="number">-5</span>b4bc6b547<span class="number">-8</span>qk2v      <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">11</span>h</span><br></pre></td></tr></table></figure></p><h3 id="创建ClusterIssuer"><a href="#创建ClusterIssuer" class="headerlink" title="创建ClusterIssuer"></a>创建ClusterIssuer</h3><p>我们需要先创建一个签发机构，cert-manager 给我们提供了 Issuer 和 ClusterIssuer 这两种用于创建签发机构的自定义资源对象，Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书，这里以 ClusterIssuer 为例创建一个签发机构：<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat issuer.yaml </span></span><br><span class="line"><span class="symbol">apiVersion:</span> certmanager.k8s.io/v1alpha1</span><br><span class="line"><span class="symbol">kind:</span> ClusterIssuer</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> letsencrypt-prod</span><br><span class="line"><span class="symbol">spec:</span></span><br><span class="line"><span class="symbol">  acme:</span></span><br><span class="line"><span class="symbol">    server:</span> https:<span class="comment">//acme-v02.api.letsencrypt.org/directory</span></span><br><span class="line"><span class="symbol">    email:</span> xxxx@<span class="number">126.</span>com</span><br><span class="line"><span class="symbol">    privateKeySecretRef:</span></span><br><span class="line"><span class="symbol">      name:</span> letsencrypt-prod</span><br><span class="line"><span class="symbol">    http01:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></p><p>说明：</p><ul><li>metadata.name 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它</li><li>spec.acme.email 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期</li><li>spec.acme.server 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行</li><li>spec.acme.privateKeySecretRef 指示此签发机构的私钥将要存储到哪个 Secret 对象中，名称不重要</li><li>spec.acme.http01 这里指示签发机构使用 HTTP-01 的方式进行 acme 协议 (还可以用 DNS 方式，acme 协议的目的是证明这台机器和域名都是属于你的，然后才准许给你颁发证书)</li></ul><p>部署<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f issuer.yaml</span></span><br></pre></td></tr></table></figure></p><p>查看clusterissuer创建结果<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get clusterissuer</span></span><br><span class="line"><span class="attribute">NAME</span>               AGE</span><br><span class="line">letsencrypt-prod   <span class="number">11m</span></span><br></pre></td></tr></table></figure></p><h3 id="创建Certificate"><a href="#创建Certificate" class="headerlink" title="创建Certificate"></a>创建Certificate</h3><p>有了签发机构，接下来我们就可以生成免费证书了，cert-manager 给我们提供了 Certificate 这个用于生成证书的自定义资源对象，它必须局限在某一个 namespace 下，证书最终会在这个 namespace 下以 Secret 的资源对象存储，创建一个 Certificate 对象：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cert.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">certmanager.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Certificate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">dashboard-imroc-io</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  secretName:</span> <span class="string">dashboard-imroc-io</span></span><br><span class="line"><span class="attr">  issuerRef:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">letsencrypt-prod</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">ClusterIssuer</span></span><br><span class="line"><span class="attr">  dnsNames:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">istio.kiali.com</span></span><br><span class="line"><span class="attr">  acme:</span></span><br><span class="line"><span class="attr">    config:</span></span><br><span class="line"><span class="attr">    - http01:</span></span><br><span class="line"><span class="attr">        ingressClass:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">      domains:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">istio.kiali.com</span></span><br></pre></td></tr></table></figure></p><p>说明：</p><ul><li>spec.secretName 指示证书最终存到哪个 Secret 中</li><li>spec.issuerRef.kind 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局</li><li>spec.issuerRef.name 我们创建的签发机构的名称 (ClusterIssuer.metadata.name)</li><li>spec.dnsNames 指示该证书的可以用于哪些域名</li><li>spec.acme.config.http01.ingressClass 使用 HTTP-01 方式校验该域名和机器时，cert-manager 会尝试创建Ingress 对象来实现该校验，如果指定该值，会给创建的 Ingress 加上 kubernetes.io/ingress.class 这个 annotation，如果我们的 Ingress Controller 是 traefik Ingress Controller，指定这个字段可以让创建的 Ingress 被 traefik Ingress Controller 处理。</li><li>spec.acme.config.http01.domains 指示该证书的可以用于哪些域名</li></ul><p>执行部署命令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f cert.yaml</span></span><br></pre></td></tr></table></figure></p><p>查看certificate创建结果<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get certificate -n istio-<span class="keyword">system</span></span><br><span class="line">NAME                 AGE</span><br><span class="line">dashboard-imroc-io   <span class="number">54</span>s</span><br></pre></td></tr></table></figure></p><p>执行上述步骤，如有问题，可使用如下命令排查原因<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl describe -n istio-system certificate dashboard-imroc-io</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl describe clusterissuer letsencrypt-prod</span></span><br></pre></td></tr></table></figure></p><p>查看生成的secret 结果<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get secret -n istio-system | grep dashboard-imroc-<span class="built_in">io</span></span><br><span class="line">dashboard-imroc-<span class="built_in">io</span>                              kubernetes.<span class="built_in">io</span>/tls                     <span class="number">3</span>      <span class="number">2</span>m32s</span><br></pre></td></tr></table></figure></p><h2 id="测试Ingress使用https"><a href="#测试Ingress使用https" class="headerlink" title="测试Ingress使用https"></a>测试Ingress使用https</h2><p>创建一个nginx<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat test-nginx.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">443</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">https</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">"traefik"</span></span><br><span class="line">    <span class="string">kubernetes.io/tls-acme:</span> <span class="string">"true"</span></span><br><span class="line"><span class="string">certmanager.k8s.io/cluster-issuer:</span> <span class="string">"letsencrypt-prod"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">test.nginx.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">  tls:</span></span><br><span class="line"><span class="attr">  - secretName:</span> <span class="string">dashboard-imroc-io</span></span><br><span class="line"><span class="attr">    hosts:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">test.nginx.com</span></span><br></pre></td></tr></table></figure></p><p>需要注意的是上面我们添加的两个annotations非常重要，这个将告诉 Cert Manager 去生成证书，然后由于我们这里要使用 HTTPS，所以我们需要添加一个 tls 证书，而证书就是通过k8sui-tls这个 Secret 对象来提供的，要注意的是这个 Secret 对象并不是我们手动创建的，而是 Cert Manager 自动创建的证书对应的对应。然后直接创建这个资源对象即可：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f <span class="built_in">test</span>-nginx.yaml</span></span><br></pre></td></tr></table></figure></p><p>创建完成后隔一会儿我们可以看到会多出现一个随机名称的 Ingress 对象，这个 Ingress 对象就是用来专门验证证书的：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get ingress -n istio-system</span></span><br><span class="line">NAME                        HOSTS                   <span class="built_in"> ADDRESS </span>  PORTS     AGE</span><br><span class="line">cm-acme-http-solver-z562f   test.nginx.com                     80        62s</span><br></pre></td></tr></table></figure></p><p>我们可以通过 Traefik 的 Dashboard 可以观察到这一变化，验证成功后，这个 Ingress 对象也自动删除了：</p><p>这个时候我们可以去describe下我们的 Ingress 对象，查看有无报错<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl describe ingress my-nginx</span></span><br></pre></td></tr></table></figure></p><p>同样我们可以去查看 Cert manager 的 Pod 日志信息：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs -f cert-manager<span class="number">-5658</span>b7db79<span class="number">-824</span>lt --namespace cert-manager</span><br></pre></td></tr></table></figure></p><p>最后，我们来打开浏览器使用https访问服务</p><p>到这里我们就完成了使用Let’s Encrypt实现Kubernetes Ingress自动化 HTTPS。</p><p><strong>参考资料</strong><br>项目地址：<a href="https://github.com/jetstack/cert-manager" target="_blank" rel="noopener">https://github.com/jetstack/cert-manager</a><br>文档地址：<a href="https://cert-manager.readthedocs.io" target="_blank" rel="noopener">https://cert-manager.readthedocs.io</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是https&quot;&gt;&lt;a href=&quot;#什么是https&quot; class=&quot;headerlink&quot; title=&quot;什么是https&quot;&gt;&lt;/a&gt;什么是https&lt;/h2&gt;&lt;p&gt;超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="cert-manager" scheme="http://yoursite.com/tags/cert-manager/"/>
    
  </entry>
  
  <entry>
    <title>浅谈Kubernetes生产架构</title>
    <link href="http://yoursite.com/2019/03/02/%E6%B5%85%E8%B0%88Kubernetes%E7%94%9F%E4%BA%A7%E6%9E%B6%E6%9E%84/"/>
    <id>http://yoursite.com/2019/03/02/浅谈Kubernetes生产架构/</id>
    <published>2019-03-02T12:55:43.000Z</published>
    <updated>2019-03-25T12:28:37.849Z</updated>
    
    <content type="html"><![CDATA[<p><strong>注意</strong><br>本文，只是笔者针对Kubernetes生产环境运行的一些关于架构设计和实现方案的总结，内容很粗糙，同时也会不断完善。</p><p>首先，我们来梳理下Kubernetes生产架构，其设计适用于绝大多数环境。如下图所示。<br><img src="/images/k8s-jiagou.png" alt="image"></p><p>在该架构中，我们可以将其分为四层，如下：</p><ul><li>Client层：即Kubernetes集群外部用户、客户端等；</li><li>服务访问层：即由Traefik ingress实现服务发现、负载均衡和路由规则定义等；</li><li>业务应用层：即基于Kubernetes平台构建和运行企业业务应用，如CI/CD持续集成、微服务项目、监控告警和日志管理、私有镜像仓库等服务；</li><li>基础设施层：即由Kubernetes容器管理平台和Ceph数据持久化存储等系统组成的基础设施服务。</li></ul><p>下面，我们分别来谈谈各层的具体实现方案。</p><h2 id="基础设施层"><a href="#基础设施层" class="headerlink" title="基础设施层"></a>基础设施层</h2><p><strong>Kubernetes平台</strong></p><ul><li>部署管理：Kubernetes平台除了直接使用公有云如阿里云、AWS等云服务提供商的K8s服务外，我们还可以自己部署和管理等，如使用Kubespray工具。</li><li>网络通信：在容器和容器之间、容器和主机网络方面，可以使用Calico或Flannel等方案。</li><li>HA高可用：Kubernetes节点分为Master和Node两种类型节点，前者负责运行集群相关的控制管理服务，而后者负责运行Pod容器。在多Node节点模式下，由于Kubernetes Pod具有天然的容灾冗余HA高可用实现，因此，我们并不需要关心Node节点的HA高可用，而只需关心Master节点的HA即可，Master节点的HA高可用，通过多Master节点+HAProxy方案实现即可。从Kubernetes 1.12版本起，kube-proxy服务默认使用ipvs实现，取消了之前的iptables。这有助于提升K8s大规模集群环境下的性能和稳定性。</li><li>Docker和操作系统优化：在生产环境中，Docker和操作系统版本应当使用较新的release版本。并且，主机操作系统应当做一定程度的优化配置，如关闭swap内存交换分区，预留一定的CPU核数和内存资源给宿主机使用等。</li></ul><p><strong>Ceph/NFS数据存储</strong><br>Kubernetes平台的数据持久化存储，可以使用Ceph、NFS等存储方案。其中，Ceph适用于有其技术背景或大容量存储需求的公司；而NFS适用于存储容量需求相对较小，无专业存储技术背景的公司。</p><h2 id="业务应用层"><a href="#业务应用层" class="headerlink" title="业务应用层"></a>业务应用层</h2><ul><li>镜像管理：使用Harbor私有镜像仓库服务；</li><li>日志管理：使用Elasticsearch、Filebeat 和 Kibana技术栈；</li><li>监控告警管理：使用Cadvisor、Prometheus和Grafana技术栈；</li><li>微服务架构：使用Service Mesh服务网格中的Istio方案；</li><li>Devops：使用Gitlab、Jenkins等持续集成工具；</li><li>单体应用：无状态类服务使用deployment，有状态类服务则使用Statefulset，如果关联的服务较多且复杂则使用Helm。</li><li>规划好Namespace：应当做到每个namespace专属用于某类型的应用，如monitor namespace统一管理诸如监控告警和日志管理方面的pod、service、pvc、ingress等资源。这样，可以较为方便的管理和区分K8s上的各种应用。</li></ul><h2 id="服务访问层"><a href="#服务访问层" class="headerlink" title="服务访问层"></a>服务访问层</h2><p>外部客户端访问K8s集群内的服务、负载均衡和路由规则定义使用Traefik Ingress实现。此外，应当实现Ingress服务HA高可用，可以想象在K8s集群中，大量的出入口流量都进过Ingress，其负载是非常大的，其重要程度不言而喻，因此实现HA就非常重要。ingress controller节点（无论是基于nginx还是traefik实现）应当至少为2个节点，并在这些节点上，部署Keepalived和HAproxy共同维护一个VIP地址，将其提供给ingress使用。</p><p>架构如下图所示。<br><img src="/images/ingress-ha.png" alt="image"></p><p>在该架构中，Ingress节点一般使用独立的服务器，即只做将集群外部流量接入到集群内部。除了使用external Ip来暴露ingress的Service到集群外部，还可以使用hostNetwork，如果是公有云，还可以使用LoadBalance。这样Ingress Controller将监听节点的80和443端口，通过热备的形式部署多个ingress节点，并在每个节点上部署Keepalived，多个节点共同维护一个VIP，实现Ingress服务的高可用。</p><p>如上图所示，部署两个ingress节点172.16.10.11和172.16.10.12。公网ip映射或转发到内网的VIP地址172.16.10.10上（如果VIP本身是公网IP则可以不用转发）。ingress controller的副本数replicaCount为2，将被调度到node1和node2这两个节点上。同时，使用pod反亲和性禁止ingress pod调度在同一个节点上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;br&gt;本文，只是笔者针对Kubernetes生产环境运行的一些关于架构设计和实现方案的总结，内容很粗糙，同时也会不断完善。&lt;/p&gt;
&lt;p&gt;首先，我们来梳理下Kubernetes生产架构，其设计适用于绝大多数环境。如下图所示。&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Istio微服务架构部署和使用</title>
    <link href="http://yoursite.com/2019/03/01/Kubernetes-Istio%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/03/01/Kubernetes-Istio微服务架构部署和使用/</id>
    <published>2019-03-01T13:55:02.000Z</published>
    <updated>2019-03-15T16:16:14.122Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Istio"><a href="#什么是Istio" class="headerlink" title="什么是Istio"></a>什么是Istio</h2><p>Istio是Service Mesh（服务网格）的主流实现方案。该方案降低了与微服务架构相关的复杂性，并提供了负载均衡、服务发现、流量管理、断路器、监控、故障注入和智能路由等功能特性。</p><p>其中，Sidecar模式是一种将应用功能从应用本身剥离出来作为单独进程的方式。该模式允许我们向应用无侵入添加多种功能，避免了为满足第三方组件需求而向应用添加额外的配置代码。从某种意义上来说，服务对于网络是无感知的，只知道所附加的sidecar代理，它将网络依赖抽象成了Sidecar。</p><p><strong>在Service Mesh中，我们需要了解Data Plane和Control Plane两个概念：</strong></p><ul><li>Data Plane：作用是处理网格内服务间的通信，并完成服务发现、负载均衡、流量管理、健康检查等功能；</li><li>Control Plane：作用是管理和配置策略用于路由流量，同时也在运行期执行策略。</li></ul><p><strong>Istio核心组件</strong></p><ul><li>Envoy：Istio 使用 Envoy调解服务网格中所有服务的入站和出站流量。属于数据平面。</li><li>Mixer：负责在服务网格上执行访问控制和使用策略，以及收集从Envoy和其他服务自动监控到的数据。</li><li>Pilot：为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。属于控制平面。</li><li>Citadel：提供访问控制和用户身份认证功能。</li></ul><p><strong>Istio可视化管理组件</strong></p><ul><li><p>Vistio：用于近乎实时地监控应用程序和集群之间的网络流量。<br>可以参考：<a href="https://www.yangcs.net/posts/vistio-visualize-your-istio-mesh-using-netflixs-vizceral/" target="_blank" rel="noopener">https://www.yangcs.net/posts/vistio-visualize-your-istio-mesh-using-netflixs-vizceral/</a></p></li><li><p>Kiali：提供可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。<br>可以参考：<a href="https://jimmysong.io/istio-handbook/setup/istio-observability-tool-kiali.html" target="_blank" rel="noopener">https://jimmysong.io/istio-handbook/setup/istio-observability-tool-kiali.html</a></p></li><li><p>jaeger：用于展示istio微服务调用链关系，以及微服务工作状态监测。注意，在生产环境中，你应当使用Elasticsearch或cassandra持久化存储jaeger数据。<br>可以参考：<a href="https://blog.csdn.net/ywq935/article/details/80599297" target="_blank" rel="noopener">https://blog.csdn.net/ywq935/article/details/80599297</a><br><a href="https://mathspanda.github.io/2018/09/19/jaeger-deploy/" target="_blank" rel="noopener">https://mathspanda.github.io/2018/09/19/jaeger-deploy/</a><br><a href="https://blog.frognew.com/2017/12/opentracing-jaeger-3.html" target="_blank" rel="noopener">https://blog.frognew.com/2017/12/opentracing-jaeger-3.html</a></p></li></ul><p>其中，Kiali、Jaeger、prometheus、grafana管理工具，将和Istio一并部署。</p><h2 id="使用Helm部署Istio"><a href="#使用Helm部署Istio" class="headerlink" title="使用Helm部署Istio"></a>使用Helm部署Istio</h2><p><strong>依赖环境</strong><br>Helm &gt; 2.10<br>Kubernetes &gt; 1.9</p><p>下载并解压缩istio的发布包<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="regexp">//gi</span>thub.com<span class="regexp">/istio/i</span>stio<span class="regexp">/releases/</span>download<span class="regexp">/1.0.6/i</span>stio-<span class="number">1.0</span>.<span class="number">6</span>-linux.tar.gz</span><br><span class="line">tar -zxvf istio-<span class="number">1.0</span>.<span class="number">6</span>-linux.tar.gz</span><br><span class="line">cd istio-<span class="number">1.0</span>.<span class="number">6</span></span><br></pre></td></tr></table></figure></p><p>Istio的Chart在istio-1.0.6/install/kubernetes/helm目录中，这个Chart包含了下面的代码文件<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># tree install/kubernetes/helm/istio</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"><span class="number">31</span> directories, <span class="number">139</span> files</span><br></pre></td></tr></table></figure></p><p>如果安装的Helm版本高于2.10，就不再需要手动使用kubectl安装Istio的CRD。反之，则需要执行如下命令安装<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f install<span class="regexp">/kubernetes/</span>helm<span class="regexp">/istio/</span>templates<span class="regexp">/crds.yaml</span></span><br></pre></td></tr></table></figure></p><p>查看安装的CRD<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="builtin-name">get</span> CustomResourceDefinition</span><br></pre></td></tr></table></figure></p><p>通过各个组件在vaule file的enabled flag启用或禁用，下面创建名称为istio.yaml的vaule file，将几个默认禁用的组件也启用<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tracing:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">servicegraph:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">kiali:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">grafana:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>首先，创建名称为kiali的secret。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> -n <span class="string">'admin'</span> | base64</span></span><br><span class="line">YWRtaW4=</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> -n <span class="string">'1f2d1e2e67df'</span> | base64</span></span><br><span class="line">MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></table></figure><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat &lt;&lt;EOF | kubectl apply -f -</span></span><br><span class="line"><span class="symbol">apiVersion:</span> v1</span><br><span class="line"><span class="symbol">kind:</span> Secret</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> kiali</span><br><span class="line"><span class="symbol">  namespace:</span> istio-system</span><br><span class="line"><span class="symbol">  labels:</span></span><br><span class="line"><span class="symbol">    app:</span> kiali</span><br><span class="line"><span class="symbol">type:</span> Opaque</span><br><span class="line"><span class="symbol">data:</span></span><br><span class="line"><span class="symbol">  username:</span> YWRtaW4=</span><br><span class="line"><span class="symbol">  passphrase:</span> MWYyZDFlMmU2N2Rm</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>执行helm安装命令<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm <span class="keyword">install</span> <span class="keyword">install</span>/kubernetes/helm/istio <span class="comment">--name istio --namespace istio-system -f istio.yaml</span></span><br></pre></td></tr></table></figure></p><p>安装完成后确认各个组件的Pod正常运行<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pod -n istio-system</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">grafana<span class="number">-59</span>b8896965<span class="number">-5</span>f9j2                  <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-citadel<span class="number">-6</span>f444d9999-s9jrc            <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-egressgateway<span class="number">-6</span>d79447874-ssbc4      <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-galley<span class="number">-685</span>bb48846-mvf5w             <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-grafana-post-install<span class="number">-6</span>m256          <span class="number">0</span>/<span class="number">1</span>     Completed   <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-ingressgateway<span class="number">-5</span>b64fffc9f-mrl9t     <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-pilot<span class="number">-8645</span>f5655b-k6fcz              <span class="number">2</span>/<span class="number">2</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-policy<span class="number">-547</span>d64b8d7<span class="number">-6</span>dgkp             <span class="number">2</span>/<span class="number">2</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-sidecar-injector<span class="number">-5</span>d8dd9448d-zfdsb   <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-telemetry-c5488fc49-qwwcv           <span class="number">2</span>/<span class="number">2</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">istio-tracing<span class="number">-6</span>b994895fd<span class="number">-4</span>vjfx            <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">kiali<span class="number">-5</span>f9ffff7cf-jqk8p                    <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">prometheus<span class="number">-76</span>b7745b64-xjzmm               <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br><span class="line">servicegraph-cb9b94c-mlhjm                <span class="number">1</span>/<span class="number">1</span>     Running     <span class="number">0</span>          <span class="number">23</span>m</span><br></pre></td></tr></table></figure></p><p>Istio 以一个项目的形式部署到 Kubernetes 集群中。我们可以看到，部署好的 pods 中，除了有 istio-citadel、istio-egressgateway、istio-ingressgateway、istio-pilot 等 Istio 本身的功能组件，还集成了微服务相关的监控工具，如：grafana、jaeger-agent、kiali、prometheus。正是这些功能丰富且强大的监控工具，帮助 Istio实现了微服务的可视化管理。</p><h2 id="运行示例Bookinfo"><a href="#运行示例Bookinfo" class="headerlink" title="运行示例Bookinfo"></a>运行示例Bookinfo</h2><p>您可以部署自己的应用或者示例应用程序如 Bookinfo。 注意：应用程序必须使用 HTTP/1.1 或 HTTP/2.0 协议来传递 HTTP 流量，因为 HTTP/1.0 已经不再支持。</p><p>如果运行 Pod 的 namespace 被标记为 istio-injection=enabled 的话，Istio-Initializer 会向应用程序的 Pod 中自动注入 Envoy 容器：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl label <span class="keyword">namespace</span> &lt;<span class="keyword">namespace</span>&gt; istio-injection=enabled</span><br><span class="line">kubectl <span class="keyword">create</span> -n &lt;<span class="keyword">namespace</span>&gt; -f &lt;your-app-spec&gt;.yaml</span><br></pre></td></tr></table></figure></p><p>如果您没有安装 Istio-initializer-injector 的话，您必须使用 istioctl kube-inject 命令在部署应用之前向应用程序的 Pod 中手动注入 Envoy 容器：<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f &lt;<span class="comment">(istioctl kube-inject -f &lt;your-app-spec&gt;.yaml)</span></span><br></pre></td></tr></table></figure></p><p>Bookinfo 应用由四个单独的微服务构成，用来演示多种 Istio 特性，包含：</p><ul><li>productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。</li><li>details ：这个微服务包含了书籍的信息。</li><li>reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。</li><li>ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。</li></ul><p>reviews 微服务有 3 个版本：</p><ul><li>v1 版本不会调用 ratings 服务。</li><li>v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。</li><li>v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。</li></ul><p>下图展示了这个应用的端到端架构。<br><img src="/images/bookinfo-arch.png" alt="image"></p><p>运行示例bookinfo，并开启Sidecar自动注入。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl label namespace default istio-injection=enabled</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml</span></span><br></pre></td></tr></table></figure></p><p>访问productpage<br><a href="http://172.16.0.180:31380/productpage" target="_blank" rel="noopener">http://172.16.0.180:31380/productpage</a><br><img src="/images/bookinfo-istio.png" alt="image"></p><p>31380端口可以通过命令获取<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n istio-system <span class="builtin-name">get</span> svc istio-ingressgateway -o <span class="attribute">jsonpath</span>=<span class="string">'&#123;.spec.ports[0].nodePort&#125;'</span></span><br></pre></td></tr></table></figure></p><h2 id="使用Ingress暴露管理服务"><a href="#使用Ingress暴露管理服务" class="headerlink" title="使用Ingress暴露管理服务"></a>使用Ingress暴露管理服务</h2><p>完成Istio的安装后，可以看到安装的组件除了Istio架构中的数据平面和控制平面的各个核心组件，还部署了Prometheus、Grafana、Jaeger、Kiali等辅助组件。 在云原生生态中，我们已经对这些组件很熟悉了。</p><ul><li>Prometheus：监控系统，收集Istio的监控数据</li><li>Grafana：监控信息的图表展现，Istio部署的Grafana为我们内置了各个组件相关的Dashboard</li><li>Jaeger：分布式跟踪系统，Istio中集成Jaeger可以对基于Istio的微服务实现调用链跟踪、依赖分析，为性能优化和故障排查提供支持</li><li>kiali：kiali作为Istio的可视化管理工具，可以认为是Istio的UI，可以展现服务的网络拓扑、服务的容错情况(超时、重试、短路等)、分布式跟踪等</li></ul><p>这些辅助组件都有自己的web界面，这里我们使用ingress的方式将这些组件暴露到集群外，以便在集群外部访问。Istio支持使用自带的istio-ingressgateway将服务暴露到集群外部，这个和Kubernetes中暴露Ingress Controller类似，有很多种方式，如NodePort，LoadBalancer，或直接开启hostNetwork: true等等。为了便于统一管理K8s集群中的服务暴露，笔者更倾向使用Traefik Ingress。</p><p>使用Ingress暴露istio服务<br>编写ingress yaml文件，如下<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat istio-ingress.yaml </span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jaeger-query</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">istio.jaeger-query.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">jaeger-query</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">16686</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">prometheus</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">istio.prometheus.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">prometheus</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">9090</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">grafana</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">istio.grafana.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">grafana</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kiali</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">istio.kiali.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">kiali</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">20001</span></span><br></pre></td></tr></table></figure></p><p>执行部署命令<br><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">apply</span> -f istio-ingress.yaml</span><br></pre></td></tr></table></figure></p><p>外部客户端，配置hosts地址解析，如下<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span> <span class="selector-tag">istio</span><span class="selector-class">.prometheus</span><span class="selector-class">.com</span></span><br><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span>  <span class="selector-tag">istio</span><span class="selector-class">.jaeger-query</span><span class="selector-class">.com</span></span><br><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span>  <span class="selector-tag">istio</span><span class="selector-class">.grafana</span><span class="selector-class">.com</span></span><br><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span>  <span class="selector-tag">istio</span><span class="selector-class">.kiali</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p><strong>访问jaeger</strong><br>浏览器访问Jaeger之前可以多次刷新productpage页面以便产生访问请求等。选择productpage.default可以查看整个调用链。使用istio.jaeger-query.com域名访问，结果展示：<br><img src="/images/jaeger-istio.png" alt="image"></p><p><strong>访问kiali</strong><br>使用域名istio.kiali.com访问kiali页面。用户名admin，密码1f2d1e2e67df。<br><img src="/images/kiali-istio.png" alt="image"></p><p><strong>访问prometheus</strong><br>使用域名istio.prometheus.com访问prometheus页面。<br><img src="/images/prometheus-istio.png" alt="image"></p><p><strong>访问grafana</strong><br>使用域名istio.grafana.com访问prometheus页面。<br><img src="/images/grafana-istio.png" alt="image"></p><h2 id="Istio-对-Pod-和服务的要求"><a href="#Istio-对-Pod-和服务的要求" class="headerlink" title="Istio 对 Pod 和服务的要求"></a>Istio 对 Pod 和服务的要求</h2><p>要成为服务网格的一部分，Kubernetes 集群中的 Pod 和服务必须满足以下几个要求：</p><ul><li>需要给端口正确命名：服务端口必须进行命名。端口名称只允许是&lt;协议&gt;[-&lt;后缀&gt;-]模式；</li><li>Pod必须关联到 Kubernetes服务：如果一个 Pod 属于多个服务，这些服务不能再同一端口上使用不同协议，例如 HTTP 和 TCP。</li><li>Deployment应带有app以及version标签：每个 Deployment 都应该有一个有意义的 app 标签和一个用于标识 Deployment 版本的 version 标签。Istio 会用 app 和 version 标签来给监控指标数据加入上下文信息。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文实践了使用istio官方提供的helm chart在Kubernetes上部署Istio 1.0.6的过程，并使用traefik ingress将Istio集成的Prometheus、Grafana、Jaeger、Kiali等辅助组件暴露到集群外部，并对进入集群的流量进行管理。</p><p>在生产环境中，如果是基于公有云，如阿里云、AWS等运行Istio，建议Ingress的IP地址使用ELB地址；如果是自建的平台，则建议使用HAproxy+Keepalived提供的VIP地址，作为Ingress的IP地址，实现高可用。</p><p>如果Ingress服务，需要暴露在公网，应当使用CA认证机构颁发的证书https化（如使用cert-manager）。此外建议使用NFS、Ceph等方案实现Istio监控以及微服务应用的数据持久化存储。</p><p><strong>istio参考资料</strong><br><a href="https://istio.io/zh/docs/" target="_blank" rel="noopener">https://istio.io/zh/docs/</a><br><a href="https://jimmysong.io/istio-handbook/" target="_blank" rel="noopener">https://jimmysong.io/istio-handbook/</a><br><a href="http://www.servicemesher.com/" target="_blank" rel="noopener">http://www.servicemesher.com/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是Istio&quot;&gt;&lt;a href=&quot;#什么是Istio&quot; class=&quot;headerlink&quot; title=&quot;什么是Istio&quot;&gt;&lt;/a&gt;什么是Istio&lt;/h2&gt;&lt;p&gt;Istio是Service Mesh（服务网格）的主流实现方案。该方案降低了与微服务架构相关
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="Istio" scheme="http://yoursite.com/tags/Istio/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes traefik ingress使用</title>
    <link href="http://yoursite.com/2019/03/01/Kubernetes-traefik-ingress%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/03/01/Kubernetes-traefik-ingress使用/</id>
    <published>2019-03-01T12:33:08.000Z</published>
    <updated>2019-03-11T13:41:08.740Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Traefik介绍"><a href="#Traefik介绍" class="headerlink" title="Traefik介绍"></a>Traefik介绍</h2><p>简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。</p><p>Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。</p><p>traefik整体架构如下所示。<br><img src="/images/traefik-arch.png" alt="image"></p><p><strong>Traefik主要特性详解</strong></p><ul><li><p>自动熔断<br>在集群中，当某一个服务大量出现请求错误，或者请求响应时间过久，或者返回500+错误状态码时，我们希望可以主动剔除该服务，也就是不在将请求转发到该服务上，而这一个过程是自动完成，不需要人工执行。Traefik 通过配置很容易就能帮我们实现，Traefik 可以通过定义策略来主动熔断服务。</p><p>  NetworkErrorRatio() &gt; 0.5：监测服务错误率达到50%时，熔断。<br>  LatencyAtQuantileMS(50.0) &gt; 50：监测延时大于50ms时，熔断。<br>  ResponseCodeRatio(500, 600, 0, 600) &gt; 0.5：监测返回状态码为[500-600]在[0-600]区间占比超过50%时，熔断。</p></li><li><p>负载均衡策略<br>Traefik 提供两种负载均衡策略支持。一种是 wrr（加权轮训调度算法），一种是 drr（动态加权循环调度算法）。</p><p> wrr是默认的负载均衡策略，新创建的 service 权重都是一样为1，这样的话，请求会平均分给每个服务，但是这样很多时候会出现资源分配不均衡的问题，比如由于集群中每个机器配置不一样，而且服务消耗不一样，假设 A 资源使用率已经很高，而 B 属于空闲状态，如果还是均摊到每个服务的话，会加重 A 的负荷，这时候因该有一种策略能够主动识别并分担更多流量到 B 才对。</p><p> drr 就更加智能，它是一种动态加权轮训调度方式，它会记录一段时间内转发到 A 的请求数，跟转发到 B 的请求数对比，转发数量多，说明处理速度快，响应时间快。如果 A 处理请求速度比 B 快，那么就会调整 A 的权重，接下来的一段时间，就会转发更多请求给 A，相应的 B 的转发就少一些。整个过程都在不断的调整权重，实现请求的合理分配，从而达到资源使用最大化。 </p></li></ul><h2 id="部署Traefik-ingress"><a href="#部署Traefik-ingress" class="headerlink" title="部署Traefik ingress"></a>部署Traefik ingress</h2><p>创建ingress-rbac.yaml，将用于service account验证。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ingress</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">ingress</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure></p><p>创建Depeloyment部署traefik，如文件名为deployment.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">ingress</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">1000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">3000</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">2000</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          hostPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">8580</span></span><br><span class="line"><span class="attr">          hostPort:</span> <span class="number">8580</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--web</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--web.address=:8580</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--kubernetes</span></span><br></pre></td></tr></table></figure></p><p>注意我们这里用的是Deploy类型，没有限定该pod运行在哪个主机上。Traefik的端口是8580。</p><p>编写Traefik UI的ingress部署文件，如文件名为traefik-ui.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">traefik-web-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8580</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">traefik-web-ui</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">traefik.ui.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">traefik-web-ui</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="string">web</span></span><br></pre></td></tr></table></figure></p><p>backend中要配置default namespace中启动的service名字。path就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。</p><p>配置完成后就可以启动treafik ingress了。<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># kubectl <span class="built_in">create</span> -f .</span><br><span class="line">deployment.extensions/traefik-ingress-lb created</span><br><span class="line">serviceaccount/ingress created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.<span class="built_in">io</span>/ingress created</span><br><span class="line">service/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-ingress created</span><br></pre></td></tr></table></figure></p><p>查看是否部署成功<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system | grep traefik</span></span><br><span class="line">traefik-ingress-lb-57786f6c44-cwr96        1/1     Running   0          2m27s</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get ingress -o wide --all-namespaces   </span></span><br><span class="line">NAMESPACE     NAME             HOSTS           <span class="built_in"> ADDRESS </span>  PORTS   AGE</span><br><span class="line">kube-system   traefik-web-ui   traefik.ui.com             80      12s</span><br></pre></td></tr></table></figure></p><p>在客户端配置hosts域名解析，如下<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span> <span class="selector-tag">traefik</span><span class="selector-class">.ui</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>172.16.0.180是traefik pod所在的K8s节点，通过域名traefik.ui.com访问将可以看到dashboard。<br><img src="/images/traefik-ui.png" alt="image"></p><p>左侧黄色部分列出的是所有的rule，右侧绿色部分是所有的backend。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>下面模拟部署一个程序，以Nginx 为例，并使用drr动态轮训加权策略。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-deployment.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx-pod</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">nginx:1.15.5</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">traefik.ingress.kubernetes.io/load-balancer-method:</span> <span class="string">drr</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="attr">        namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nginx-pod</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-ingress</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">k8s.nginx.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure></p><p>创建nginx<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">apply</span> -f nginx-deployment.yaml </span><br><span class="line">kubectl <span class="built_in">get</span> pods</span><br></pre></td></tr></table></figure></p><p>同样，修改客户端的hosts文件。在其中加入<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span> <span class="selector-tag">traefik</span><span class="selector-class">.ui</span><span class="selector-class">.com</span></span><br><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span> <span class="selector-tag">k8s</span><span class="selector-class">.nginx</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>所有访问这些地址的流量都会发送给172.16.0.180这台主机，就是我们启动traefik的主机。Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。<br><img src="/images/traefik-ui-nginx.png" alt="image"></p><p>在外部客户端，访问nginx应用的ingress地址<br><a href="http://k8s.nginx.com/" target="_blank" rel="noopener">http://k8s.nginx.com/</a></p><p><img src="/images/nginx-ui.png" alt="image"></p><p>在K8s集群节点上访问测试<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># curl -x 172.16.0.180:80 http:<span class="comment">//k8s.nginx.com</span></span></span><br></pre></td></tr></table></figure></p><h2 id="ingress配置同域名不同路径代理web应用"><a href="#ingress配置同域名不同路径代理web应用" class="headerlink" title="ingress配置同域名不同路径代理web应用"></a>ingress配置同域名不同路径代理web应用</h2><p>很多使用我们不想配置太多的域名来区别应用，使用同域名分路径的方式来区别应用就简洁方便很多。ingress也提供了相关的配置。</p><p>假设两个应用tomcat-test1和tomcat-test2。这里可配置域名tomcat.test.k8s，通过路径test1、test2来分别代理两个tomcat应用。其中，分路径配置需添加配置：traefik.frontend.rule.type: PathPrefixStrip<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi ingress-tomcat.yaml </span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tomcat-test-web</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line">    <span class="string">traefik.frontend.rule.type:</span> <span class="string">PathPrefixStrip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">tomcat.test.k8s</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/test1/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">tomcat-test1</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/test2/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">tomcat-test2</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f ingress-tomcat.yaml                               </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl describe ingress tomcat-test-web</span></span><br></pre></td></tr></table></figure><p>从describe信息和ui界面上可以看到，tomcat.test.k8s分别有了/test1/和/test2/的域名代理以及相对应的后端，可以修改hosts测试一下分路径是否生效：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span>  <span class="selector-tag">tomcat</span><span class="selector-class">.test</span><span class="selector-class">.k8s</span></span><br></pre></td></tr></table></figure></p><p>测试访问<br><img src="/images/test-ui.png" alt="image"></p><h2 id="在线修改资源配置"><a href="#在线修改资源配置" class="headerlink" title="在线修改资源配置"></a>在线修改资源配置</h2><p>如果需要在线修改部署的资源，如deployment、service或ingress等，可以使用kubectl edit命令。如修改deployment。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get deploy</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl edit deploy nginx-pod</span></span><br></pre></td></tr></table></figure></p><p>或者直接修改yaml文件后，执行kubectl apply命令更新即可生效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h2&gt;&lt;p&gt;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的UR
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="traefik" scheme="http://yoursite.com/tags/traefik/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Helm使用</title>
    <link href="http://yoursite.com/2019/03/01/Kubernetes-Helm%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/03/01/Kubernetes-Helm使用/</id>
    <published>2019-03-01T11:58:21.000Z</published>
    <updated>2019-03-13T13:00:18.683Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Helm"><a href="#什么是Helm" class="headerlink" title="什么是Helm"></a>什么是Helm</h2><p>在没使用helm之前，向kubernetes部署应用，我们要依次部署deployment、svc等，步骤较繁琐。况且随着很多项目微服务化，复杂的应用在容器中部署以及管理显得较为复杂，helm通过打包的方式，支持发布的版本管理和控制，很大程度上简化了Kubernetes应用的部署和管理.</p><p>Helm本质就是让K8s的应用管理（Deployment,Service等)可配置，能动态生成。通过动态生成K8s资源清单文件（deployment.yaml，service.yaml）。然后调用Kubectl自动执行K8s资源部署。</p><p><strong>Helm和charts的主要作用</strong></p><ul><li>应用程序封装</li><li>版本管理</li><li>依赖检查</li><li>便于应用程序分发</li></ul><p><strong>组成</strong><br>helm客户端</p><ul><li>制作、拉取、查找和验证 Chart</li><li>安装服务端Tiller</li><li>指示服务端Tiller做事，比如根据chart创建一个Release</li></ul><p>helm服务端 tiller</p><ul><li>安装在Kubernetes集群内的一个应用， 用来执行客户端发来的命令，管理Release</li></ul><h2 id="安装Helm客户端"><a href="#安装Helm客户端" class="headerlink" title="安装Helm客户端"></a>安装Helm客户端</h2><p>下载期望的版本<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># wget -c https:<span class="comment">//storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz</span></span></span><br></pre></td></tr></table></figure></p><p>解压<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">tar</span> <span class="selector-tag">-zxvf</span> <span class="selector-tag">helm-v2</span><span class="selector-class">.12</span><span class="selector-class">.3-linux-amd64</span><span class="selector-class">.tar</span><span class="selector-class">.gz</span></span><br></pre></td></tr></table></figure></p><p>在解压后的文件夹中找到Helm命令所在位置, 将它移动到期望位置<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span></span><br></pre></td></tr></table></figure></p><h2 id="安装helm服务端tiller"><a href="#安装helm服务端tiller" class="headerlink" title="安装helm服务端tiller"></a>安装helm服务端tiller</h2><p>执行命令<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># helm init --upgrade --tiller-image registry.<span class="keyword">cn</span>-beijing.aliyuncs.<span class="keyword">com</span>/minminmsn/tiller:v2.<span class="number">12.3</span> --stable-repo-url http<span class="variable">s:</span>//kubernetes.oss-<span class="keyword">cn</span>-hangzhou.aliyuncs.<span class="keyword">com</span>/charts</span><br></pre></td></tr></table></figure></p><p>确认服务端tiller<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods -n kube-system |grep tiller</span><br><span class="line">tiller-deploy<span class="number">-99</span>dcdbf5f-ddwbg              <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">4</span>m28s</span><br></pre></td></tr></table></figure></p><p>确认客户端和服务端连接成功。如果只显示了客户端版本，说明没有连上服务端。 它会自动去K8s上kube-system命名空间下查找是否有Tiller的Pod在运行。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">helm</span> <span class="selector-tag">version</span></span><br><span class="line"><span class="selector-tag">Client</span>: &amp;<span class="selector-tag">version</span><span class="selector-class">.Version</span>&#123;<span class="attribute">SemVer</span>:<span class="string">"v2.12.3"</span>, GitCommit:<span class="string">"20adb27c7c5868466912eebdf6664e7390ebe710"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line"><span class="selector-tag">Server</span>: &amp;<span class="selector-tag">version</span><span class="selector-class">.Version</span>&#123;<span class="attribute">SemVer</span>:<span class="string">"v2.12.3"</span>, GitCommit:<span class="string">"20adb27c7c5868466912eebdf6664e7390ebe710"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure></p><p>查找helm仓库中可用chart，如查找mysql<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm search mysql</span></span><br></pre></td></tr></table></figure></p><p>默认安装的 tiller 权限很小，我们执行下面的脚本给它加最大权限，这样方便我们可以用 helm 部署应用到任意 namespace 下:<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl <span class="keyword">create</span> serviceaccount --<span class="keyword">namespace</span>=kube-<span class="keyword">system</span> tiller</span><br><span class="line"></span><br><span class="line"># kubectl <span class="keyword">create</span> clusterrolebinding tiller-<span class="keyword">cluster</span>-rule --clusterrole=<span class="keyword">cluster</span>-admin --serviceaccount=kube-<span class="keyword">system</span>:tiller</span><br><span class="line"></span><br><span class="line"># kubectl patch deploy --<span class="keyword">namespace</span>=kube-<span class="keyword">system</span> tiller-deploy -p <span class="string">'&#123;"spec":&#123;"template":&#123;"spec":&#123;"serviceAccount":"tiller"&#125;&#125;&#125;&#125;'</span></span><br></pre></td></tr></table></figure></p><h2 id="创建自己的chart"><a href="#创建自己的chart" class="headerlink" title="创建自己的chart"></a>创建自己的chart</h2><p>我们创建一个名为mychart的chart，看一看chart的文件结构。<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ helm create mongodb</span><br><span class="line">$ tree mongodb</span><br><span class="line">mongodb</span><br><span class="line">├── Chart.yaml <span class="comment">#Chart本身的版本和配置信息</span></span><br><span class="line">├── charts <span class="comment">#依赖的chart</span></span><br><span class="line">├── templates <span class="comment">#配置模板目录</span></span><br><span class="line">│   ├── NOTES.txt <span class="comment">#helm提示信息</span></span><br><span class="line">│   ├── _helpers.tpl <span class="comment">#用于修改kubernetes objcet配置的模板</span></span><br><span class="line"><span class="params">|     |</span>—— ingress.yaml  <span class="comment">#用于服务暴露或访问</span></span><br><span class="line">│   ├── deployment.yaml <span class="comment">#kubernetes Deployment object</span></span><br><span class="line">│   └── service.yaml <span class="comment">#kubernetes Serivce</span></span><br><span class="line">└── values.yaml <span class="comment">#kubernetes object configuration</span></span><br></pre></td></tr></table></figure></p><p>如此，我们可以按需编辑自动生成的yaml文件。templates目录下的yaml文件中的变量是从values.yaml文件中获取的。</p><p>使用命令验证chart配置。该输出中包含了模板的变量配置与最终渲染的yaml文件。<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span> <span class="comment">helm</span> <span class="comment">install</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dry</span><span class="literal">-</span><span class="comment">run</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">debug</span> <span class="comment">mongodb</span></span><br></pre></td></tr></table></figure></p><h2 id="部署到kubernetes"><a href="#部署到kubernetes" class="headerlink" title="部署到kubernetes"></a>部署到kubernetes</h2><p>在mongodb目录下执行下面的命令将nginx部署到kubernetes集群上。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm install .</span></span><br></pre></td></tr></table></figure></p><p>查看部署的release<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">helm</span> <span class="selector-tag">list</span></span><br><span class="line"><span class="selector-tag">NAME</span>         <span class="selector-tag">REVISION</span><span class="selector-tag">UPDATED</span>                 <span class="selector-tag">STATUS</span>  <span class="selector-tag">CHART</span>        <span class="selector-tag">NAMESPACE</span></span><br><span class="line"><span class="selector-tag">garish-gopher</span>1       <span class="selector-tag">Wed</span> <span class="selector-tag">Feb</span> 27 11<span class="selector-pseudo">:24</span><span class="selector-pseudo">:36</span> 2019<span class="selector-tag">DEPLOYED</span><span class="selector-tag">mongodb-0</span><span class="selector-class">.1</span><span class="selector-class">.0</span><span class="selector-tag">default</span></span><br></pre></td></tr></table></figure></p><h2 id="打包分享"><a href="#打包分享" class="headerlink" title="打包分享"></a>打包分享</h2><p>我们可以修改Chart.yaml中的helm chart配置信息，然后使用下列命令将chart打包成一个压缩文件。<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># helm <span class="keyword">package</span> <span class="title">.</span></span><br></pre></td></tr></table></figure></p><p>打包出mongodb-0.1.0.tgz文件。</p><p><strong>依赖</strong><br>我们可以在requirements.yaml中定义应用所依赖的chart，例如定义对mariadb的依赖：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">dependencies</span>:</span><br><span class="line">- <span class="attribute">name</span>: mariadb</span><br><span class="line">  <span class="attribute">version</span>: <span class="number">0.6</span>.<span class="number">0</span></span><br><span class="line">  <span class="attribute">repository</span>: <span class="attribute">https</span>:<span class="comment">//kubernetes-charts.storage.googleapis.com</span></span><br></pre></td></tr></table></figure></p><p>使用helm lint .命令可以检查依赖和模板配置是否正确。</p><h2 id="使用第三方chat库"><a href="#使用第三方chat库" class="headerlink" title="使用第三方chat库"></a>使用第三方chat库</h2><p>添加fabric8库<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># helm repo add fabric8 https:<span class="comment">//fabric8.io/helm</span></span></span><br></pre></td></tr></table></figure></p><p>搜索fabric8提供的工具（主要就是fabric8-platform工具包，包含了CI、CD的全套工具）<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm search fabric8</span></span><br></pre></td></tr></table></figure></p><p>我们在前面打包的chart可以通过HTTP server的方式提供。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">helm</span> <span class="selector-tag">serve</span> <span class="selector-tag">--address</span> 172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span><span class="selector-pseudo">:8879</span></span><br><span class="line"><span class="selector-tag">Regenerating</span> <span class="selector-tag">index</span>. <span class="selector-tag">This</span> <span class="selector-tag">may</span> <span class="selector-tag">take</span> <span class="selector-tag">a</span> <span class="selector-tag">moment</span>.</span><br><span class="line"><span class="selector-tag">Now</span> <span class="selector-tag">serving</span> <span class="selector-tag">you</span> <span class="selector-tag">on</span> 172<span class="selector-class">.16</span><span class="selector-class">.0</span><span class="selector-class">.180</span><span class="selector-pseudo">:8879</span></span><br></pre></td></tr></table></figure></p><p>访问<a href="http://172.16.0.180:8879可以看到刚刚安装的chart。" target="_blank" rel="noopener">http://172.16.0.180:8879可以看到刚刚安装的chart。</a><br><img src="/images/helm-k8s.png" alt="image"></p><p><strong>解决本地chart依赖</strong><br>打开另外一个终端，在本地当前chart配置的目录下，将该repo加入到repo list中。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm repo add <span class="built_in">local</span> http://172.16.0.180:8879</span></span><br></pre></td></tr></table></figure></p><p>在浏览器中访问<a href="http://172.16.0.180:8879，可以看到所有本地的chart。" target="_blank" rel="noopener">http://172.16.0.180:8879，可以看到所有本地的chart。</a></p><p>然后下载依赖到本地。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm dependency update</span></span><br></pre></td></tr></table></figure></p><p>这样所有的chart都会下载到本地的charts目录下。</p><p>设置helm命令自动补全<br>为了方便helm命令的使用，helm提供了自动补全功能，如果使用bash请执行：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> &lt;(helm completion bash)</span></span><br></pre></td></tr></table></figure></p><p><strong>Example: 安装Mysql</strong><br>执行命令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm repo update</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> helm install stable/mysql</span></span><br><span class="line">Released smiling-penguin</span><br></pre></td></tr></table></figure></p><p>每次安装都有一个Release被创建， 所以一个Chart可以在同一个集群中被安装多次，每一个都是独立管理和升级的。其中 stable/mysql是Chart名， smiling-penguid 是Release名，后面管理Release时都是用的这个名字。</p><p>在使用一个Chart前，查看它的默认配置，然后使用配置文件覆盖它的默认设置<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm inspect values stable/mariadb</span></span><br></pre></td></tr></table></figure></p><p>使用一个YAML文件，内含要覆盖Chart的配置值。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'&#123;mariadbUser: user0, mariadbDatabase: user0db&#125;'</span> &gt; config.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> helm install -f config.yaml stable/mariadb</span></span><br></pre></td></tr></table></figure></p><p>values.yaml中的值可以被部署release时用到的参数–values YAML_FILE_PATH 或 –set key1=value1, key2=value2覆盖掉， 比如<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> helm install --<span class="built_in">set</span> image.tag=<span class="string">'latest'</span> .</span></span><br></pre></td></tr></table></figure></p><p>优先级： –set设置的值会覆盖–value设置的值， –value设置的值会覆盖 values.yaml中定义的值</p><h2 id="helm一些常用命令"><a href="#helm一些常用命令" class="headerlink" title="helm一些常用命令"></a>helm一些常用命令</h2><p><strong>Charts:</strong><br>helm search 查找可用的Charts<br>helm inspect 查看指定Chart的基本信息<br>helm install 根据指定的Chart 部署一个Release到K8s<br>helm create 创建自己的Chart<br>helm package 打包Chart，一般是一个压缩包文件</p><p><strong>release:</strong><br>helm list 列出已经部署的Release<br>helm delete [RELEASE] 删除一个Release. 并没有物理删除， 出于审计需要，历史可查。<br>helm status [RELEASE] 查看指定的Release信息，即使使用helm delete命令删除的Release.<br>helm upgrade 升级某个Release<br>helm rollback [RELEASE] [REVISION] 回滚Release到指定发布版本<br>helm get values [RELEASE] 查看Release的配置文件值<br>helm ls –deleted 列出已经删除的Release</p><p><strong>repo:</strong><br>helm repo list<br>helm repo add [RepoName] [RepoUrl]<br>helm repo update</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是Helm&quot;&gt;&lt;a href=&quot;#什么是Helm&quot; class=&quot;headerlink&quot; title=&quot;什么是Helm&quot;&gt;&lt;/a&gt;什么是Helm&lt;/h2&gt;&lt;p&gt;在没使用helm之前，向kubernetes部署应用，我们要依次部署deployment、svc等，
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="Helm" scheme="http://yoursite.com/tags/Helm/"/>
    
  </entry>
  
  <entry>
    <title>使用client-go自定义开发Kubernetes</title>
    <link href="http://yoursite.com/2019/02/13/%E4%BD%BF%E7%94%A8client-go%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%80%E5%8F%91Kubernetes/"/>
    <id>http://yoursite.com/2019/02/13/使用client-go自定义开发Kubernetes/</id>
    <published>2019-02-13T14:39:09.000Z</published>
    <updated>2019-03-01T12:54:06.261Z</updated>
    
    <content type="html"><![CDATA[<p><strong> 1. 安装client-go </strong></p><p>client-go 安装很简单，前提是本机已经安装并配置好了 Go 环境，安装之前，我们需要先查看下其版本针对 k8s 版本 兼容性列表，针对自己本机安装的 k8s 版本选择对应的 client-go 版本，当然也可以默认选择最新版本，来兼容所有。</p><p>client-go 安装方式有多种，比如 go get、Godep、Glide 方式。如果我们本地没有安装 Godep 和 Glide 依赖管理工具的话，可以使用最简单的 go get 下载安装。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">go</span> <span class="built_in">get</span> k8s.io/client-<span class="built_in">go</span>/...</span><br></pre></td></tr></table></figure></p><p>执行该命令将会自动将 k8s.io/client-go 下载到本机 $GOPATH，默认下载的源码中只包含了大部分依赖，并将其放在 k8s.io/client-go/vendor 路径，但是如果想成功运行的话，还需要另外两个依赖库 k8s.io/client-go/vendor 和 glog，所以还需要接着执行如下命令。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go <span class="builtin-name">get</span> -u k8s.io/apimachinery/<span class="built_in">..</span>.</span><br></pre></td></tr></table></figure></p><p>说明一下，为什么要使用 -u 参数来拉取最新的该依赖库呢？那是因为最新的 client-go 库只能保证跟最新的 apimachinery 库一起运行。</p><p><strong> 2. 在k8s集群外操作资源示例 </strong><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"># cat main.<span class="keyword">go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"flag"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/kubernetes"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/clientcmd"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 配置 k8s 集群外 kubeconfig 配置文件</span></span><br><span class="line"><span class="keyword">var</span> kubeconfig *<span class="keyword">string</span></span><br><span class="line">kubeconfig = flag.String(<span class="string">"kubeconfig"</span>, <span class="string">"/etc/kubernetes/admin.conf"</span>, <span class="string">"absolute path to the kubeconfig file"</span>)</span><br><span class="line">flag.Parse()</span><br><span class="line"></span><br><span class="line"><span class="comment">//在 kubeconfig 中使用当前上下文环境，config 获取支持 url 和 path 方式</span></span><br><span class="line">config, err := clientcmd.BuildConfigFromFlags(<span class="string">""</span>, *kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据指定的 config 创建一个新的 clientset</span></span><br><span class="line">clientset, err := kubernetes.NewForConfig(config)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="comment">// 通过实现 clientset 的 CoreV1Interface 接口列表中的 PodsGetter 接口方法 Pods(namespace string)返回 PodInterface</span></span><br><span class="line"><span class="comment">// PodInterface 接口拥有操作 Pod 资源的方法，例如 Create、Update、Get、List 等方法</span></span><br><span class="line"><span class="comment">// 注意：Pods() 方法中 namespace 不指定则获取 Cluster 所有 Pod 列表</span></span><br><span class="line">pods, err := clientset.CoreV1().Pods(<span class="string">""</span>).List(metav1.ListOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"There are %d pods in the k8s cluster\n"</span>, <span class="built_in">len</span>(pods.Items))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取指定 namespace 中的 Pod 列表信息</span></span><br><span class="line">namespace := <span class="string">"default"</span></span><br><span class="line">pods, err = clientset.CoreV1().Pods(namespace).List(metav1.ListOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"\nThere are %d pods in namespaces %s\n"</span>, <span class="built_in">len</span>(pods.Items), namespace)</span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods.Items &#123;</span><br><span class="line">fmt.Printf(<span class="string">"Name: %s, Status: %s, CreateTime: %s\n"</span>, pod.ObjectMeta.Name, pod.Status.Phase, pod.ObjectMeta.CreationTimestamp)</span><br><span class="line">&#125;</span><br><span class="line">time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">prettyPrint</span><span class="params">(maps <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">lens := <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k, _ := <span class="keyword">range</span> maps &#123;</span><br><span class="line"><span class="keyword">if</span> lens &lt;= <span class="built_in">len</span>(k) &#123;</span><br><span class="line">lens = <span class="built_in">len</span>(k)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> key, values := <span class="keyword">range</span> maps &#123;</span><br><span class="line">spaces := lens - <span class="built_in">len</span>(key)</span><br><span class="line">v := <span class="string">""</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; spaces; i++ &#123;</span><br><span class="line">v += <span class="string">" "</span></span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"%s: %s%v\n"</span>, key, v, values)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">homeDir</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> h := os.Getenv(<span class="string">"HOME"</span>); h != <span class="string">""</span> &#123;</span><br><span class="line"><span class="keyword">return</span> h</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> os.Getenv(<span class="string">"USERPROFILE"</span>) <span class="comment">// windows</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>执行程序<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># go run main.go</span></span><br><span class="line">There are 15 pods <span class="keyword">in</span> the k8s cluster</span><br><span class="line"></span><br><span class="line">There are 2 pods <span class="keyword">in</span> namespaces default</span><br><span class="line">Name: podinfo-7b8c9bc5c9-64g8k, Status: Running, CreateTime: 2019-01-10 22:40:18 +0800 CST</span><br><span class="line">Name: podinfo-7b8c9bc5c9-bx7ml, Status: Running, CreateTime: 2019-01-10 22:40:18 +0800 CST</span><br><span class="line">There are 15 pods <span class="keyword">in</span> the k8s cluster</span><br></pre></td></tr></table></figure></p><p><strong> 3.在k8s集群内操作资源示例 </strong><br>除以上方法外，还可以在 k8s 集群内运行客户端操作资源类型。既然是在 k8s 集群内运行，那么就需要将编写的代码放到镜像内，然后在 k8s 集群内以 Pod 方式运行该镜像容器。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># cat main2.<span class="keyword">go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/kubernetes"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/rest"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 通过集群内部配置创建 k8s 配置信息，通过 KUBERNETES_SERVICE_HOST 和 KUBERNETES_SERVICE_PORT 环境变量方式获取</span></span><br><span class="line"><span class="comment">// 若集群使用 TLS 认证方式，则默认读取集群内部 tokenFile 和 CAFile</span></span><br><span class="line"><span class="comment">// tokenFile  = "/var/run/secrets/kubernetes.io/serviceaccount/token"</span></span><br><span class="line"><span class="comment">// rootCAFile = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"</span></span><br><span class="line">config, err := rest.InClusterConfig()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据指定的 config 创建一个新的 clientset</span></span><br><span class="line">clientset, err := kubernetes.NewForConfig(config)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="comment">// 通过实现 clientset 的 CoreV1Interface 接口列表中的 PodsGetter 接口方法 Pods(namespace string)返回 PodInterface</span></span><br><span class="line"><span class="comment">// PodInterface 接口拥有操作 Pod 资源的方法，例如 Create、Update、Get、List 等方法</span></span><br><span class="line"><span class="comment">// 注意：Pods() 方法中 namespace 不指定则获取 Cluster 所有 Pod 列表</span></span><br><span class="line">pods, err := clientset.CoreV1().Pods(<span class="string">""</span>).List(metav1.ListOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"There are %d pods in the k8s cluster\n"</span>, <span class="built_in">len</span>(pods.Items))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取指定 namespace 中的 Pod 列表信息</span></span><br><span class="line">namespce := <span class="string">"default"</span></span><br><span class="line">pods, err = clientset.CoreV1().Pods(namespce).List(metav1.ListOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"\nThere are %d pods in namespaces %s\n"</span>, <span class="built_in">len</span>(pods.Items), namespce)</span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods.Items &#123;</span><br><span class="line">fmt.Printf(<span class="string">"Name: %s, Status: %s, CreateTime: %s\n"</span>, pod.ObjectMeta.Name, pod.Status.Phase, pod.ObjectMeta.CreationTimestamp)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有的 Namespaces 列表信息</span></span><br><span class="line">ns, err := clientset.CoreV1().Namespaces().List(metav1.ListOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">nss := ns.Items</span><br><span class="line">fmt.Printf(<span class="string">"\nThere are %d namespaces in cluster\n"</span>, <span class="built_in">len</span>(nss))</span><br><span class="line"><span class="keyword">for</span> _, ns := <span class="keyword">range</span> nss &#123;</span><br><span class="line">fmt.Printf(<span class="string">"Name: %s, Status: %s, CreateTime: %s\n"</span>, ns.ObjectMeta.Name, ns.Status.Phase, ns.CreationTimestamp)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该示例主要演示如何在 k8s 集群内操作 Pod 和 Namespaces 资源类型，包括获取集群所有 Pod 列表数量，获取指定 Namespace 中的 Pod 列表信息，获取集群内所有 Namespace 列表信息。这里，该方式获取 k8s 集群配置的方式跟上边方式不同，它通过集群内部创建的 k8s 配置信息，通过 KUBERNETES_SERVICE_HOST 和 KUBERNETES_SERVICE_PORT 环境变量方式获取，来跟 k8s 建立连接，进而来操作其各个资源类型。如果 k8s 开启了 TLS 认证方式，那么默认读取集群内部指定位置的 tokenFile 和 CAFile。</p><p>编译一下，看下是否通过。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> go build main2.go</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ls</span></span><br><span class="line">main2  main2.go</span><br></pre></td></tr></table></figure></p><p>接下来，在同级目录创建一个 Dockerfile 文件如下<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> debian</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./main2 /opt</span></span><br><span class="line"><span class="bash">ENTRYPOINT /opt/main2</span></span><br></pre></td></tr></table></figure></p><p>构建docker镜像<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ls</span></span><br><span class="line">Dockerfile  main2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker build -t client-go/<span class="keyword">in</span>-cluster:1.0 .</span></span><br></pre></td></tr></table></figure></p><p>因为本机 k8s 默认开启了 RBAC 认证的，所以需要创建一个 clusterrolebinding 来赋予 default 账户 view 权限。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create clusterrolebinding default-view --clusterrole=view --serviceaccount=default:default</span><br><span class="line">clusterrolebinding<span class="selector-class">.rbac</span><span class="selector-class">.authorization</span><span class="selector-class">.k8s</span><span class="selector-class">.io</span> <span class="string">"default-view"</span> created</span><br></pre></td></tr></table></figure></p><p>最后，在 Pod 中运行该镜像即可，可以使用 yaml 方式或运行 kubectl run 命令来创建。<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># kubectl run --rm -i client-go-in-cluster-demo --image=client-go/in-cluster:1.0 --image-pull-policy=Never</span><br><span class="line"></span><br><span class="line">There are 3 pods in namespaces default</span><br><span class="line">Name: client-go-in-cluster-demo<span class="string">-58</span>d9b5bd79<span class="string">-7</span>w5ds, Status: Running, CreateTime: 2019<span class="string">-02</span><span class="string">-13</span> 14:25:38 <span class="string">+0000</span> UTC</span><br><span class="line">Name: podinfo<span class="string">-7</span>b8c9bc5c9<span class="string">-64</span>g8k, Status: Running, CreateTime: 2019<span class="string">-01</span><span class="string">-10</span> 14:40:18 <span class="string">+0000</span> UTC</span><br><span class="line">Name: podinfo<span class="string">-7</span>b8c9bc5c9-bx7ml, Status: Running, CreateTime: 2019<span class="string">-01</span><span class="string">-10</span> 14:40:18 <span class="string">+0000</span> UTC</span><br><span class="line"></span><br><span class="line">There are 5 namespaces in cluster</span><br><span class="line">Name: custom-metrics, Status: Active, CreateTime: 2019<span class="string">-01</span><span class="string">-10</span> 09:01:52 <span class="string">+0000</span> UTC</span><br><span class="line">Name: default, Status: Active, CreateTime: 2019<span class="string">-01</span><span class="string">-05</span> 09:18:02 <span class="string">+0000</span> UTC</span><br><span class="line">Name: kube-public, Status: Active, CreateTime: 2019<span class="string">-01</span><span class="string">-05</span> 09:18:02 <span class="string">+0000</span> UTC</span><br><span class="line">Name: kube-system, Status: Active, CreateTime: 2019<span class="string">-01</span><span class="string">-05</span> 09:18:02 <span class="string">+0000</span> UTC</span><br><span class="line">Name: monitoring, Status: Active, CreateTime: 2019<span class="string">-01</span><span class="string">-08</span> 15:00:41 <span class="string">+0000</span> UTC</span><br><span class="line">There are 16 pods in the k8s cluster</span><br></pre></td></tr></table></figure></p><p>运行正常，简单验证一下吧！<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods -n <span class="section">default</span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">client-go-in-cluster-demo<span class="number">-58</span>d9b5bd79<span class="number">-7</span>w5ds   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">10</span>m</span><br><span class="line">podinfo<span class="number">-7</span>b8c9bc5c9<span class="number">-64</span>g8k                     <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">1</span>          <span class="number">33</span>d</span><br><span class="line">podinfo<span class="number">-7</span>b8c9bc5c9-bx7ml                     <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">1</span>          <span class="number">33</span>d</span><br></pre></td></tr></table></figure></p><p><strong> 4. k8s各资源对象CRUD操作 </strong><br>上边演示了，在 k8s 集群内外运行客户端操作资源类型，但是仅仅是 Read 相关读取操作，接下来简单演示下如何进行 Create、Update、Delete 操作。创建 main.go 文件如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"># cat main3.<span class="keyword">go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"flag"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">apiv1 <span class="string">"k8s.io/api/core/v1"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/kubernetes"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/clientcmd"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 配置 k8s 集群外 kubeconfig 配置文件</span></span><br><span class="line"><span class="keyword">var</span> kubeconfig *<span class="keyword">string</span></span><br><span class="line">    kubeconfig = flag.String(<span class="string">"kubeconfig"</span>, <span class="string">"/etc/kubernetes/admin.conf"</span>, <span class="string">"absolute path to the kubeconfig file"</span>)</span><br><span class="line">flag.Parse()</span><br><span class="line"></span><br><span class="line"><span class="comment">//在 kubeconfig 中使用当前上下文环境，config 获取支持 url 和 path 方式</span></span><br><span class="line">config, err := clientcmd.BuildConfigFromFlags(<span class="string">""</span>, *kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据指定的 config 创建一个新的 clientset</span></span><br><span class="line">clientset, err := kubernetes.NewForConfig(config)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过实现 clientset 的 CoreV1Interface 接口列表中的 NamespacesGetter 接口方法 Namespaces 返回 NamespaceInterface</span></span><br><span class="line"><span class="comment">// NamespaceInterface 接口拥有操作 Namespace 资源的方法，例如 Create、Update、Get、List 等方法</span></span><br><span class="line">name := <span class="string">"client-go-test"</span></span><br><span class="line">namespacesClient := clientset.CoreV1().Namespaces()</span><br><span class="line">namespace := &amp;apiv1.Namespace&#123;</span><br><span class="line">ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">Name: name,</span><br><span class="line">&#125;,</span><br><span class="line">Status: apiv1.NamespaceStatus&#123;</span><br><span class="line">Phase: apiv1.NamespaceActive,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个新的 Namespaces</span></span><br><span class="line">fmt.Println(<span class="string">"Creating Namespaces..."</span>)</span><br><span class="line">result, err := namespacesClient.Create(namespace)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"Created Namespaces %s on %s\n"</span>, result.ObjectMeta.Name, result.ObjectMeta.CreationTimestamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取指定名称的 Namespaces 信息</span></span><br><span class="line">fmt.Println(<span class="string">"Getting Namespaces..."</span>)</span><br><span class="line">result, err = namespacesClient.Get(name, metav1.GetOptions&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"Name: %s, Status: %s, selfLink: %s, uid: %s\n"</span>,</span><br><span class="line">result.ObjectMeta.Name, result.Status.Phase, result.ObjectMeta.SelfLink, result.ObjectMeta.UID)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除指定名称的 Namespaces 信息</span></span><br><span class="line">fmt.Println(<span class="string">"Deleting Namespaces..."</span>)</span><br><span class="line">deletePolicy := metav1.DeletePropagationForeground</span><br><span class="line"><span class="keyword">if</span> err := namespacesClient.Delete(name, &amp;metav1.DeleteOptions&#123;</span><br><span class="line">PropagationPolicy: &amp;deletePolicy,</span><br><span class="line">&#125;); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">"Deleted Namespaces %s\n"</span>, name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>执行程序<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">go</span> run main3.<span class="keyword">go</span></span><br><span class="line">Creating Namespaces...</span><br><span class="line">Created Namespaces client-<span class="keyword">go</span>-test <span class="keyword">on</span> <span class="number">2019</span>-<span class="number">02</span>-<span class="number">13</span> <span class="number">21</span>:<span class="number">44</span>:<span class="number">52</span> +<span class="number">0800</span> CST</span><br><span class="line">Getting Namespaces...</span><br><span class="line">Name: client-<span class="keyword">go</span>-test, Statu<span class="variable">s:</span> Active, selfLink: /api/v1/namespaces/client-<span class="keyword">go</span>-test, uid: <span class="number">8</span>a2de86e-<span class="number">2</span>f95-<span class="number">11</span>e9-b2e0-a0369f3f0404</span><br><span class="line">Deleting Namespaces...</span><br><span class="line">Deleted Namespaces client-<span class="keyword">go</span>-test</span><br></pre></td></tr></table></figure></p><p>参考资料<br><a href="https://blog.csdn.net/aixiaoyang168/article/details/84752005" target="_blank" rel="noopener">https://blog.csdn.net/aixiaoyang168/article/details/84752005</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt; 1. 安装client-go &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;client-go 安装很简单，前提是本机已经安装并配置好了 Go 环境，安装之前，我们需要先查看下其版本针对 k8s 版本 兼容性列表，针对自己本机安装的 k8s 版本选择对应的 clien
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>编译和运行Kubernetes源码</title>
    <link href="http://yoursite.com/2019/01/26/%E7%BC%96%E8%AF%91%E5%92%8C%E8%BF%90%E8%A1%8CKubernetes%E6%BA%90%E7%A0%81/"/>
    <id>http://yoursite.com/2019/01/26/编译和运行Kubernetes源码/</id>
    <published>2019-01-26T11:51:09.000Z</published>
    <updated>2019-03-01T12:53:39.676Z</updated>
    
    <content type="html"><![CDATA[<p><strong> 为什么要编译源码 </strong><br>Kubernetes是一个非常棒的容器集群管理平台。通常情况下，我们并不需要修改K8s代码即可直接使用。但如果，我们在环境中发现了某个问题/缺陷，或按照特定业务需求需要修改K8s代码时，如定制Kubelet的StopContainer 逻辑、kube-scheduler的pod调度逻辑等。为了让修改生效，那么就需要编译K8s代码了。</p><p>Kubernetes源码编译，大致分为本地二进制可执行文件编译和docker镜像编译两种。由于在我们的环境中，Kubernetes是由Docker容器方式运行的。故此我们需要采用后面一种方式编译，即镜像编译。</p><p>由于Kubernetes每个组件服务的镜像Dockerfile文件是由Kubernetes源码自动生成的，因此，社区并未提供每个组件的镜像Dockerfile文件。编译本地二进制可执行文件很简单，也更直接。而docker镜像编译资料却很少，且碍于某种特殊网络原因，会导致失败。此处，将介绍如何顺利的完成k8s镜像编译。</p><p><strong> 安装依赖 </strong><br>安装Golang<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -<span class="keyword">c</span> http<span class="variable">s:</span>//<span class="keyword">dl</span>.google.<span class="keyword">com</span>/<span class="keyword">go</span>/go1.<span class="number">11.4</span>.linux-amd64.tar.gz -<span class="keyword">P</span> /<span class="keyword">opt</span>/</span><br><span class="line"><span class="keyword">cd</span> /<span class="keyword">opt</span>/</span><br><span class="line">tar -C /usr/local -xzf go1.<span class="number">11.4</span>.linux-amd64.tar.gz </span><br><span class="line"><span class="keyword">echo</span> <span class="string">"export PATH=$PATH:/usr/local/go/bin"</span> &gt;&gt; /etc/<span class="keyword">profile</span> &amp;&amp; <span class="keyword">source</span> /etc/<span class="keyword">profile</span></span><br></pre></td></tr></table></figure></p><p>指定分支，下载 Kubernetes 源代码（默认$GOPATH目录为/root/go/）<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -<span class="selector-tag">p</span> <span class="variable">$GOPATH</span>/src/k8s.io</span><br><span class="line">cd <span class="variable">$GOPATH</span>/src/k8s.io</span><br><span class="line">git clone  https:<span class="comment">//github.com/kubernetes/kubernetes -b release-1.13</span></span><br><span class="line">cd <span class="variable">$GOPATH</span>/src/k8s.io/kubernetes</span><br></pre></td></tr></table></figure></p><p><strong> 本地二进制文件编译Kubernetes（方法一） </strong><br>修改运行平台配置参数（可选）<br>根据自己的运行平台（linux/amd64)修改hack/lib/golang.sh，把KUBE_SERVER_PLATFORMS，KUBE_CLIENT_PLATFORMS和KUBE_TEST_PLATFORMS中除linux/amd64以外的其他平台注释掉，以此来减少编译所用时间。</p><p>编译源码<br>进入Kubernetes根目录下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> kubernetes</span><br></pre></td></tr></table></figure></p><p>KUBE_BUILD_PLATFORMS指定目标平台，WHAT指定编译的组件，通过GOFLAGS和GOGCFLAGS传入编译时参数，如此处编译kubelet 组件。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">KUBE_BUILD_PLATFORMS</span>=linux/amd64 make all <span class="attribute">WHAT</span>=cmd/kubelet <span class="attribute">GOFLAGS</span>=-v <span class="attribute">GOGCFLAGS</span>=<span class="string">"-N -l"</span></span><br></pre></td></tr></table></figure></p><ul><li>如果不指定WHAT，则编译全部。</li><li>make all是在本地环境中进行编译的。</li><li>make release和make quick-release在容器中完成编译、打包成docker镜像。</li><li>编译kubelet这部分代码，也可执行make clean &amp;&amp; make WHAT=cmd/kubelet</li></ul><p>检查编译成果<br>编译过程较长，请耐心等待，编译后的文件在kubernetes/_output里。</p><p>或者进入cmd/kubelet (以kubelet为例子)<br>执行go build -v命令,如果没出错,会生成可执行文件kubelet<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">go</span> <span class="keyword">build </span>-v</span><br></pre></td></tr></table></figure></p><p>生成的可执行文件在当前文件夹下面<br><figure class="highlight hsp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># ls <span class="meta-keyword">cmd</span>/kubelet/</span></span><br><span class="line">app  BUILD  kubelet  kubelet.go  OWNERS</span><br></pre></td></tr></table></figure></p><p><strong> Docker镜像编译Kubernetes（方法二） </strong></p><p>查看kube-cross的TAG版本号<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat ./build/build-image/cross/VERSION</span><br><span class="line">v1<span class="number">.11</span><span class="number">.4</span><span class="number">-1</span></span><br></pre></td></tr></table></figure></p><p>查看debian_iptables_version版本号<br><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># egrep -Rn <span class="string">"debian_iptables_version="</span> ./</span></span><br><span class="line">./build/common.sh:<span class="number">93</span>:  <span class="keyword">local</span> debian_iptables_version=v11<span class="number">.0</span></span><br></pre></td></tr></table></figure></p><p>这里，我使用DockerHub的Auto build功能，来构建K8s镜像。自然将编译需要用到的base镜像，放在了DockerHub上（也算是为促进国内K8s源码docker编译贡献绵薄之力吧！）。<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker pull xiaoxu780/<span class="keyword">pause</span>-amd64:<span class="number">3.1</span></span><br><span class="line">docker pull xiaoxu780/kube-cross:v1<span class="meta">.11</span><span class="meta">.2</span>-<span class="number">1</span></span><br><span class="line">docker pull xiaoxu780/debian-base-amd64:<span class="number">0.4</span><span class="meta">.0</span></span><br><span class="line">docker pull xiaoxu780/debian-iptables-amd64:v11<span class="meta">.0</span></span><br><span class="line">docker pull xiaoxu780/debian-hyperkube-base-amd64:<span class="number">0.12</span><span class="meta">.0</span></span><br><span class="line"></span><br><span class="line">docker tag xiaoxu780/<span class="keyword">pause</span>-amd64:<span class="number">3.1</span> k8s.gcr.io/<span class="keyword">pause</span>-amd64:<span class="number">3.1</span></span><br><span class="line">docker tag xiaoxu780/kube-cross:v1<span class="meta">.11</span><span class="meta">.2</span>-<span class="number">1</span> k8s.gcr.io/kube-cross:v1<span class="meta">.11</span><span class="meta">.4</span>-<span class="number">1</span></span><br><span class="line">docker tag xiaoxu780/debian-base-amd64:<span class="number">0.4</span><span class="meta">.0</span> k8s.gcr.io/debian-base-amd64:<span class="number">0.4</span><span class="meta">.0</span></span><br><span class="line">docker tag xiaoxu780/debian-iptables-amd64:v11<span class="meta">.0</span> k8s.gcr.io/debian-iptables-amd64:v11<span class="meta">.0</span></span><br><span class="line">docker tag xiaoxu780/debian-hyperkube-base-amd64:<span class="number">0.12</span><span class="meta">.0</span> k8s.gcr.io/debian-hyperkube-base-amd64:<span class="number">0.12</span><span class="meta">.0</span></span><br></pre></td></tr></table></figure></p><p>把build/lib/release.sh中的–pull去掉，避免构建镜像继续拉取镜像：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"<span class="variable">$&#123;DOCKER[@]&#125;</span>"</span> build --pull -q -t <span class="string">"<span class="variable">$&#123;docker_image_tag&#125;</span>"</span> <span class="variable">$&#123;docker_build_path&#125;</span> &gt;/dev/<span class="literal">null</span></span><br><span class="line">修改为:</span><br><span class="line"> <span class="string">"<span class="variable">$&#123;DOCKER[@]&#125;</span>"</span> build -q -t <span class="string">"<span class="variable">$&#123;docker_image_tag&#125;</span>"</span> <span class="variable">$&#123;docker_build_path&#125;</span> &gt;/dev/<span class="literal">null</span></span><br></pre></td></tr></table></figure></p><p>编辑文件hack/lib/version.sh<br>将KUBE_GIT_TREE_STATE=”dirty” 改为 KUBE_GIT_TREE_STATE=”clean”，确保版本号干净。</p><p>执行编译命令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> kubernetes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> make clean</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> KUBE_BUILD_PLATFORMS=linux/amd64 KUBE_BUILD_CONFORMANCE=n KUBE_BUILD_HYPERKUBE=n make release-images GOFLAGS=-v GOGCFLAGS=<span class="string">"-N -l"</span></span></span><br></pre></td></tr></table></figure></p><p>其中KUBE_BUILD_PLATFORMS=linux/amd64指定目标平台为linux/amd64，GOFLAGS=-v开启verbose日志，GOGCFLAGS=”-N -l”禁止编译优化和内联，减小可执行程序大小。</p><p>编译的K8s Docker镜像以压缩包的形式发布在_output/release-tars目录中<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls _output/release-images/amd64/</span></span><br><span class="line"><span class="keyword">cloud-controller-manager.tar </span> kube-controller-manager.tar  kube-<span class="keyword">scheduler.tar</span></span><br><span class="line"><span class="keyword">kube-apiserver.tar </span>           kube-proxy.tar</span><br></pre></td></tr></table></figure></p><p><strong> 使用编译镜像 </strong></p><p>等待编译完成后，在_output/release-stage/server/linux-amd64/kubernetes/server/bin/目录下保存了编译生成的二进制可执行程序和docker镜像tar包。如导入kube-apiserver.tar镜像，并更新环境上部署的kube-apiserver镜像。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker load -i kube-apiserver.tar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker tag k8s.gcr.io/kube-apiserver:v1.13.3  registry.com/kube-apiserver:v1.13.3</span></span><br></pre></td></tr></table></figure></p><p>整个编译过程结束后，现在就可以到master节点上，修改/etc/kubernetes/manifests/kube-apiserver.yaml描述文件中的image，修改完立即生效。</p><p>参考资料：<br><a href="https://github.com/kubernetes/kubernetes/tree/master/build/" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/build/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt; 为什么要编译源码 &lt;/strong&gt;&lt;br&gt;Kubernetes是一个非常棒的容器集群管理平台。通常情况下，我们并不需要修改K8s代码即可直接使用。但如果，我们在环境中发现了某个问题/缺陷，或按照特定业务需求需要修改K8s代码时，如定制Kubelet的St
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="源码编译" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>如何实现K8s Pod自定义指标弹性伸缩</title>
    <link href="http://yoursite.com/2019/01/10/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0K8s-Pod%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/"/>
    <id>http://yoursite.com/2019/01/10/如何实现K8s-Pod自定义指标弹性伸缩/</id>
    <published>2019-01-10T13:56:48.000Z</published>
    <updated>2019-03-01T12:53:55.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="弹性伸缩介绍"><a href="#弹性伸缩介绍" class="headerlink" title="弹性伸缩介绍"></a>弹性伸缩介绍</h2><p>自动弹性伸缩(AutoScaling)，是Kubernetes的一大功能和亮点。在OpenStack IaaS云计算中也有类似的服务，即Senlin。即基于资源使用情况自动弹性缩容和扩容工作负载。Kubernetes的自动弹性伸缩有两个维度：</p><ul><li>Cluster Autoscaler：处理K8s集群Node节点伸缩，该功能依赖于IaaS云提供商云主机服务和资源监控服务。</li><li>Horizontal Pod Autoscaler（HPA）：处理Pod自动弹性伸缩副本集，该功能依赖于监控服务采集到的资源指标数据。</li></ul><p>简言之，Cluster Autoscaling和Horizontal Pod Autoscaler（HPA）可用于动态调整计算能力以满足系统SLA的要求。</p><p>通常，扩/缩容都是根据内存或者CPU资源的使用率实现，但是现实中，很多时候扩/缩容的依据通常是业务监控指标。如何根据业务监控指标来进行扩/缩容，将是本文探讨的内容。</p><p>自Kubernetes 1.11版本起，K8s资源采集指标由Resource Metrics API（Metrics Server 实现）和Custom metrics api（Prometheus实现）两种API实现，传统Heapster监控被废弃。前者主要负责采集Node、Pod的核心资源数据，如内存、CPU等；而后者则主要负责自定义指标数据采集，如网卡流量，磁盘IOPS、HTTP请求数、数据库连接数等。</p><h2 id="Custom-Metrics-Server介绍"><a href="#Custom-Metrics-Server介绍" class="headerlink" title="Custom Metrics Server介绍"></a>Custom Metrics Server介绍</h2><p>前面已提过，自heapster被废弃以后，所有的指标数据都从API接口中获取，由此kubernetes将资源指标分为了两种：</p><ul><li>Core metrics(核心指标)：由metrics-server提供API，即 metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。</li><li>Custom Metrics(自定义指标)：由Prometheus Adapter提供API，即 custom.metrics.k8s.io，由此可支持任意Prometheus采集到的自定义指标。</li></ul><p>想让K8s的HPA，获取核心指标以外的其它自定义指标，则必须部署一套prometheus监控系统，让prometheus采集其它各种指标，但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(kube-state-metrics)，将prometheus的metrics数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在Master节点上的kube-apiserver中注册，以便直接通过/apis/来访问。</p><p><img src="/images/k8s-custom-metrics.png" alt="image"></p><h3 id="Custom-Metrics组件介绍"><a href="#Custom-Metrics组件介绍" class="headerlink" title="Custom Metrics组件介绍"></a>Custom Metrics组件介绍</h3><ul><li>node-exporter：prometheus的agent端，收集Node级别的监控数据。</li><li>prometheus：监控服务端，从node-exporter拉取数据并存储为时序数据。</li><li>kube-state-metrics： 将prometheus中可以用PromQL查询到的指标数据转换成k8s对应的数据格式，即转换成Custerom Metrics API接口格式的数据，但是它不能聚合进apiserver中的功能。</li><li>k8s-prometheus-adpater：聚合apiserver，即提供了一个apiserver（cuester-metrics-api），自定义APIServer通常都要通过Kubernetes aggregator聚合到apiserver。</li></ul><h2 id="Custom-Metrics部署"><a href="#Custom-Metrics部署" class="headerlink" title="Custom Metrics部署"></a>Custom Metrics部署</h2><p>获取部署文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/stefanprodan/k8s-prom-hpa</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> k8s-prom-hpa/</span></span><br></pre></td></tr></table></figure></p><p>创建monitoring命名空间<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f ./namespaces.yaml</span></span><br></pre></td></tr></table></figure></p><p>编辑prometheus部署文件<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># vim ./prometheus/prometheus-dep.yaml</span><br><span class="line"><span class="symbol">image:</span> prom/prometheus:v2<span class="meta">.1</span><span class="meta">.0</span></span><br><span class="line">//修改为image: prom/prometheus:v2<span class="meta">.5</span><span class="meta">.0</span></span><br></pre></td></tr></table></figure></p><p>将 Prometheus部署到monitoring命名空间<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f ./prometheus</span></span><br></pre></td></tr></table></figure></p><p>生成由Prometheus adapter所需的TLS证书<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> make certs</span></span><br></pre></td></tr></table></figure></p><p>编辑custom-metrics-apiserve部署文件<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># vim ./custom-metrics-api/custom-metrics-apiserver-deployment.yaml</span></span><br><span class="line">- --prometheus-url=http:<span class="comment">//prometheus.monitoring.svc.cluster.local:9090/      </span></span><br><span class="line"><span class="comment">//将该参数修改为Prometheus的 service域名地址</span></span><br><span class="line"> </span><br><span class="line"><span class="symbol">image:</span> quay.io<span class="meta-keyword">/coreos/</span>k8s-prometheus-adapter-amd64:v0<span class="number">.2</span><span class="number">.0</span></span><br><span class="line"><span class="comment">//修改为image: quay.io/coreos/k8s-prometheus-adapter-amd64:v0.4.1</span></span><br></pre></td></tr></table></figure></p><p>部署Prometheus自定义api适配器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f ./custom-metrics-api</span></span><br></pre></td></tr></table></figure></p><p>列出由prometheus提供的自定义指标<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get --raw <span class="string">"/apis/custom.metrics.k8s.io/v1beta1"</span> | jq .</span></span><br></pre></td></tr></table></figure></p><p>查看创建的namespace<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get namespace</span></span><br><span class="line">NAME             STATUS   AGE</span><br><span class="line">default          Active   10d</span><br><span class="line">kube-public      Active   10d</span><br><span class="line">kube-system      Active   10d</span><br><span class="line">monitoring       Active   6d23h</span><br></pre></td></tr></table></figure></p><p>查看创建的pod<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods -n monitoring</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">custom-metrics-apiserver<span class="number">-855</span>cbf8644<span class="number">-6</span>qhmv   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d21h</span><br><span class="line">prometheus<span class="number">-788</span>f78d959-xs74p                 <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d21h</span><br></pre></td></tr></table></figure></p><p>查看创建的service<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc -n monitoring</span></span><br><span class="line">NAME                      <span class="built_in"> TYPE </span>       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">custom-metrics-apiserver   ClusterIP   10.233.7.116   &lt;none&gt;        443/TCP          2d21h</span><br><span class="line">prometheus                 NodePort    10.233.49.23   &lt;none&gt;        9090:30090/TCP   2d21h</span><br></pre></td></tr></table></figure></p><p>到了这里，我们便可以通过 http://节点IP:30090方式访问Prometheus页面。</p><p><img src="/images/prometheus.png" alt="image"></p><p>查看新创建的api群组<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl api-versions  | grep metrics</span><br><span class="line">custom.metrics.k8s.<span class="built_in">io</span>/v1beta1</span><br><span class="line">metrics.k8s.<span class="built_in">io</span>/v1beta1</span><br></pre></td></tr></table></figure></p><p>列出由prometheus提供的自定义指标<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># yum -y install jq</span><br><span class="line"></span><br><span class="line"># kubectl get --raw <span class="string">"/apis/custom.metrics.k8s.io/v1beta1"</span> | jq .</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"APIResourceList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"groupVersion"</span>: <span class="string">"custom.metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"resources"</span>: [.............]               <span class="comment">//输出信息太多，此处省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>列示Pod 上的Prometheus 适配器所提供的缺省定制指标。<br><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#  kubectl get --raw <span class="string">"/apis/custom.metrics.k8s.io/v1beta1"</span> | <span class="type">jq</span> .  |<span class="type">grep</span> <span class="string">"pods/"</span></span><br></pre></td></tr></table></figure></p><p>获取monitoring命名空间中所有pod的fs_usage_bytes信息<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#  kubectl get --raw <span class="string">"/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes"</span> | jq .</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"MetricValueList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"custom.metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line">    <span class="string">"selfLink"</span>: <span class="string">"/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/%2A/fs_usage_bytes"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"describedObject"</span>: &#123;</span><br><span class="line">        <span class="string">"kind"</span>: <span class="string">"Pod"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"monitoring"</span>,</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"custom-metrics-apiserver-855cbf8644-6qhmv"</span>,</span><br><span class="line">        <span class="string">"apiVersion"</span>: <span class="string">"/__internal"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"metricName"</span>: <span class="string">"fs_usage_bytes"</span>,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-01-15T14:14:22Z"</span>,</span><br><span class="line">      <span class="string">"value"</span>: <span class="string">"233570304"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"describedObject"</span>: &#123;</span><br><span class="line">        <span class="string">"kind"</span>: <span class="string">"Pod"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"monitoring"</span>,</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"prometheus-788f78d959-xs74p"</span>,</span><br><span class="line">        <span class="string">"apiVersion"</span>: <span class="string">"/__internal"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"metricName"</span>: <span class="string">"fs_usage_bytes"</span>,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-01-15T14:14:22Z"</span>,</span><br><span class="line">      <span class="string">"value"</span>: <span class="string">"36864"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>创建一个使用Custom Metrics APIs的Pod，来验证自定义指标弹性伸缩功能。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> k8s-prom-hpa/</span></span><br></pre></td></tr></table></figure></p><p>podinfo 应用暴露了一个自定义的度量指标：http_requests_total。Prometheus adapter（即 custom-metrics-apiserver）删除了 _total 后缀并将该指标标记为 counter metric。</p><p>在default命名空间中创建podinfo服务，podinfo应用程序暴露了一个自定义的指标，即http_requests。<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># kubectl create -f .<span class="regexp">/podinfo/</span>podinfo-svc.yaml,.<span class="regexp">/podinfo/</span>podinfo-dep.yaml</span><br></pre></td></tr></table></figure></p><p>从自定义指标API获取每秒的总请求数:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get --raw <span class="string">"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests"</span> | jq .</span></span><br></pre></td></tr></table></figure></p><p>m代表milli-units，例如，901m 意味着901 milli-requests （就是大约0.9个请求）。</p><p>创建一个HPA，如果请求数超过每秒10，将扩容podinfo这个Pod的副本数。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f ./podinfo/podinfo-hpa-custom.yaml</span></span><br></pre></td></tr></table></figure></p><p>过几秒钟HPA从自定义指标API取得http_requests的值：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get hpa</span><br><span class="line">NAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">podinfo   Deployment/podinfo   <span class="number">899</span>m / <span class="number">10</span>   <span class="number">2</span>         <span class="number">10</span>        <span class="number">2</span>          <span class="number">1</span>m</span><br></pre></td></tr></table></figure></p><p>用每秒25次请求数给podinfo服务加压<br>安装hey<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> go get -u github.com/rakyll/hey</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hey -n 10000 -q 5 -c 5 http://&lt;K8S-IP&gt;:31198/healthz</span></span><br></pre></td></tr></table></figure></p><p>几分钟后，HPA开始扩容该Pod副本数。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe hpa</span></span><br><span class="line">Name:                       podinfo</span><br><span class="line">Namespace:                  default</span><br><span class="line">Reference:                  Deployment/podinfo</span><br><span class="line">Metrics:                    ( current / target )</span><br><span class="line">  <span class="string">"http_requests"</span> on pods:  9059m / 10</span><br><span class="line">Min replicas:               2</span><br><span class="line">Max replicas:               10</span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line"> <span class="built_in"> Type </span>   Reason             Age   <span class="keyword">From</span>                       Message</span><br><span class="line">  ----    ------             ----  ----                       -------</span><br><span class="line">  Normal  SuccessfulRescale  2m    horizontal-pod-autoscaler  New size: 3; reason: pods metric http_requests above target</span><br></pre></td></tr></table></figure></p><p>可看到，在当前压力测试下，Pod副本将自动扩容到三副本，但不会超过10副本这个最大值。同时三副本Pod已经满足当前负载。</p><p>负载测试结束后，HPA向下自动缩容到1个副本Pod。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>    <span class="string">Reason</span>             <span class="string">Age</span>   <span class="string">From</span>                       <span class="string">Message</span></span><br><span class="line"><span class="bullet">  -</span><span class="meta">---</span>    <span class="bullet">------</span>             <span class="bullet">----</span>  <span class="bullet">----</span>                       <span class="bullet">-------</span></span><br><span class="line">  <span class="string">Normal</span>  <span class="string">SuccessfulRescale</span>  <span class="number">5</span><span class="string">m</span>    <span class="string">horizontal-pod-autoscaler</span>  <span class="string">New</span> <span class="attr">size:</span> <span class="number">3</span><span class="string">;</span> <span class="attr">reason:</span> <span class="string">pods</span> <span class="string">metric</span> <span class="string">http_requests</span> <span class="string">above</span> <span class="string">target</span></span><br><span class="line">  <span class="string">Normal</span>  <span class="string">SuccessfulRescale</span>  <span class="number">21</span><span class="string">s</span>   <span class="string">horizontal-pod-autoscaler</span>  <span class="string">New</span> <span class="attr">size:</span> <span class="number">2</span><span class="string">;</span> <span class="attr">reason:</span> <span class="string">All</span> <span class="string">metrics</span> <span class="string">below</span> <span class="string">target</span></span><br></pre></td></tr></table></figure></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>custom metrics大大丰富了K8s pod弹性伸缩的能力，使K8s Pod AutoScaling从资源伸缩向业务应用伸缩方向转变成为现实。限于篇幅，后面将介绍部署Grafana+Influxdb集成Prometheus可视化查看K8s集群资源使用情况。</p><p>此外，在生产环境中，应当使用NFS、hostPath等方式持久化存储Prometheus监控数据。</p><p>参考资料：<br><a href="https://github.com/stefanprodan/k8s-prom-hpa" target="_blank" rel="noopener">https://github.com/stefanprodan/k8s-prom-hpa</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;弹性伸缩介绍&quot;&gt;&lt;a href=&quot;#弹性伸缩介绍&quot; class=&quot;headerlink&quot; title=&quot;弹性伸缩介绍&quot;&gt;&lt;/a&gt;弹性伸缩介绍&lt;/h2&gt;&lt;p&gt;自动弹性伸缩(AutoScaling)，是Kubernetes的一大功能和亮点。在OpenStack Iaa
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="自动弹性伸缩" scheme="http://yoursite.com/tags/%E8%87%AA%E5%8A%A8%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>自动化部署K8s 1.13.1</title>
    <link href="http://yoursite.com/2019/01/04/%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2K8s-1-13-1%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2019/01/04/自动化部署K8s-1-13-1集群/</id>
    <published>2019-01-04T15:43:11.000Z</published>
    <updated>2019-03-01T12:54:18.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>部署Kubernetes除了手动方式外，还有诸如Kubeadm、Kubespray、Breeze、Rancher、kargo等多种自动化方式。工具没有好坏之分，能干事、效率高就行。这里，笔者仍使用Kubespray部署当前K8s最新版本（用着真的很贴身），可自动化部署HA集群、可灵活定制开发、高稳定性等。</p><p>本篇将介绍如何在不用科学上网的背景下，快速自动化部署K8s集群。那么，开始吧！</p><h2 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h2><p>环境说明<br>环境实在有限，只有一台机器，想玩HA集群也没环境啊</p><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>K8s</td><td>172.16.0.180</td><td>Master+node</td></tr></tbody></table><p>环境为Centos 7系统，各节点配置hosts和hostname，如<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/hosts</span><br><span class="line"><span class="number">172.16</span><span class="number">.0</span><span class="number">.180</span>   K8s</span><br></pre></td></tr></table></figure></p><p>关闭防火墙等<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/SELINUX=*/SELINUX=disabled/'</span> /etc/selinux/<span class="built_in">config</span></span><br><span class="line">systemctl disable firewalld &amp;&amp; systemctl <span class="built_in">stop</span> firewalld</span><br></pre></td></tr></table></figure></p><p>Kubernetes 1.8开始要求关闭系统的Swap交换分区，方法如下<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -<span class="selector-tag">a</span> &amp;&amp; echo <span class="string">"vm.swappiness=0"</span> &gt;&gt; /etc/sysctl<span class="selector-class">.conf</span> &amp;&amp; sysctl -<span class="selector-tag">p</span> &amp;&amp; free –h</span><br></pre></td></tr></table></figure></p><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，在各个Docker节点执行下面的命令：<br><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -<span class="keyword">P</span> FORWARD <span class="keyword">ACC</span>EPT</span><br></pre></td></tr></table></figure></p><p>配置ssh key 认证。确保本机也可以 ssh 连接，否则下面部署失败<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -N <span class="string">""</span></span><br><span class="line">cat ~<span class="regexp">/.ssh/id</span>_rsa.pub <span class="meta">&gt;&gt; </span>~<span class="regexp">/.ssh/authorized</span>_keys</span><br></pre></td></tr></table></figure></p><p>更新系统内核为 4.4.x , CentOS 默认为3.10.x<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rpm --import https:<span class="comment">//www.elrepo.org/RPM-GPG-KEY-elrepo.org</span></span><br><span class="line">rpm -Uvh http:<span class="comment">//www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span></span><br><span class="line">yum --enablerepo=elrepo-kernel install -y kernel-<span class="keyword">lt</span> kernel-<span class="keyword">lt</span>-devel </span><br><span class="line">grub2-<span class="keyword">set</span>-default <span class="comment">0</span></span><br></pre></td></tr></table></figure></p><p>重启系统<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">reboot</span></span><br></pre></td></tr></table></figure></p><p>增加内核配置<br><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line"># docker</span><br><span class="line"><span class="built_in">net</span>.bridge.bridge-nf-<span class="keyword">call</span>-iptables = <span class="number">1</span></span><br><span class="line"><span class="built_in">net</span>.bridge.bridge-nf-<span class="keyword">call</span>-ip6tables = <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>使其内核配置生效<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sysctl -p</span></span><br></pre></td></tr></table></figure></p><h2 id="安装kubespray"><a href="#安装kubespray" class="headerlink" title="安装kubespray"></a>安装kubespray</h2><p>安装 centos的epel源<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y <span class="keyword">install</span> epel-<span class="keyword">release</span></span><br></pre></td></tr></table></figure></p><p>make缓存<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all <span class="meta">&amp;&amp; yum makecache</span></span><br></pre></td></tr></table></figure></p><p>安装软件，ansible 必须 &gt;= 2.7<br><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y <span class="keyword">python</span>-pip python34 <span class="keyword">python</span>-netaddr python34-pip ansible git</span><br></pre></td></tr></table></figure></p><p>下载源码，当前kubespray项目的master分支默认安装k8s 1.13.1版本<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/kubernetes-sigs/kubespray</span><br></pre></td></tr></table></figure></p><p>安装 kubespray 依赖，若无特殊说明，后续操作均在~/kubespray目录下执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> kubespray</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>配置kubespray<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cp</span> -rfp inventory/sample inventory/mycluster</span><br></pre></td></tr></table></figure></p><p>修改配置文件 hosts.ini。<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vim inventory/mycluster/hosts.ini</span><br><span class="line">[all]</span><br><span class="line">k8s <span class="attr">ansible_host=</span>k8s <span class="attr">ip=</span><span class="number">172.16</span>.<span class="number">0.180</span></span><br><span class="line"></span><br><span class="line">[kube-<span class="literal">master</span>]</span><br><span class="line">k8s</span><br><span class="line"></span><br><span class="line">[etcd]</span><br><span class="line">k8s</span><br><span class="line"></span><br><span class="line">[kube-<span class="keyword">node</span><span class="title">]</span></span><br><span class="line"><span class="title">k8s</span></span><br><span class="line"></span><br><span class="line">[k8s-cluster:children]</span><br><span class="line">kube-<span class="literal">master</span></span><br><span class="line">kube-<span class="keyword">node</span><span class="title"></span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">[calico-rr</span>]</span><br></pre></td></tr></table></figure></p><p>修改配置文件all.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vim</span> <span class="string">inventory/mycluster/group_vars/all/all.yml</span></span><br><span class="line"><span class="comment"># 修改如下配置:</span></span><br><span class="line"><span class="attr">loadbalancer_apiserver_localhost:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 加载内核模块，否则 ceph, gfs 等无法挂载客户端</span></span><br><span class="line"><span class="attr">kubelet_load_modules:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>默认镜像从 gcr.io/google-containers 下载，由于墙的原因不能下载。这里我将k8s 1.13.1版本所必须的镜像push到了DockerHub上，方便大家下载使用。<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 下载镜像(该步骤可不用执行)</span><br><span class="line">docker pull xiaoxu780/kube-proxy:v1<span class="meta">.13</span><span class="meta">.1</span></span><br><span class="line">docker pull xiaoxu780/kube-controller-manager:v1<span class="meta">.13</span><span class="meta">.1</span></span><br><span class="line">docker pull xiaoxu780/kube-scheduler:v1<span class="meta">.13</span><span class="meta">.1</span></span><br><span class="line">docker pull xiaoxu780/kube-apiserver:v1<span class="meta">.13</span><span class="meta">.1</span></span><br><span class="line">docker pull xiaoxu780/coredns:<span class="number">1.2</span><span class="meta">.6</span></span><br><span class="line">docker pull xiaoxu780/cluster-proportional-autoscaler-amd64:<span class="number">1.3</span><span class="meta">.0</span></span><br><span class="line">docker pull xiaoxu780/kubernetes-dashboard-amd64:v1<span class="meta">.10</span><span class="meta">.0</span></span><br><span class="line">docker pull xiaoxu780/etcd:<span class="number">3.2</span><span class="meta">.24</span></span><br><span class="line">docker pull xiaoxu780/node:v3<span class="meta">.1</span><span class="meta">.3</span></span><br><span class="line">docker pull xiaoxu780/ctl:v3<span class="meta">.1</span><span class="meta">.3</span></span><br><span class="line">docker pull xiaoxu780/kube-controllers:v3<span class="meta">.1</span><span class="meta">.3</span></span><br><span class="line">docker pull xiaoxu780/cni:v3<span class="meta">.1</span><span class="meta">.3</span></span><br><span class="line">docker pull xiaoxu780/<span class="keyword">pause</span>-amd64:<span class="number">3.1</span></span><br></pre></td></tr></table></figure></p><p>修改镜像默认的repo地址，使用Calico三层网络，同时可以指定安装的k8s版本，参数为kube_version。编辑文件<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim inventory<span class="regexp">/mycluster/g</span>roup_vars<span class="regexp">/k8s-cluster/</span>k8s-cluster.yml</span><br><span class="line">kube_image_repo: <span class="string">"gcr.io/google-containers"</span> <span class="regexp">//</span>修改为kube_image_repo: <span class="string">"xiaoxu780"</span></span><br></pre></td></tr></table></figure></p><p>修改配置文件main.yml，使用sed命令批量替换<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="selector-tag">i</span> <span class="string">'s/gcr\.io\/google_containers/xiaoxu780/g'</span> roles/download/defaults/main.yml</span><br><span class="line">sed -<span class="selector-tag">i</span> <span class="string">'s/quay\.io\/coreos/xiaoxu780/g'</span> roles/download/defaults/main.yml</span><br><span class="line">sed -<span class="selector-tag">i</span> <span class="string">'s/quay\.io\/calico/xiaoxu780/g'</span> roles/download/defaults/main.yml</span><br></pre></td></tr></table></figure></p><p>修改代码，使用NodePort方式访问Dashboard。<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim .<span class="meta-keyword">/roles/</span>kubernetes-apps<span class="meta-keyword">/ansible/</span>templates/dashboard.yml.j2</span><br><span class="line"><span class="meta"># ------------------- Dashboard Service ------------------- #</span></span><br><span class="line">……</span><br><span class="line">……</span><br><span class="line"><span class="symbol">      targetPort:</span> <span class="number">8443</span></span><br><span class="line"><span class="symbol">  type:</span> NodePort    <span class="comment">//添加这一行  </span></span><br><span class="line"><span class="symbol">  selector:</span></span><br><span class="line">k8s-app: kubernetes-dashboard</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong></p><p>如果是单节点部署K8s，Kubespray默认会创建2个coredns Pod，但Deployment中又用到了podAntiAffinity，因此会导致其中一个coredns pod pending，所以需要修改代码如下<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vim ./roles/kubernetes-apps/ansible/templates/coredns-deployment.yml.j2</span><br><span class="line"><span class="comment">//注释掉以下几行代码</span></span><br><span class="line">      affinity:</span><br><span class="line">        #podAntiAffinity:</span><br><span class="line">        #  requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        #  - topologyKey: <span class="string">"kubernetes.io/hostname"</span></span><br><span class="line">        #    labelSelector:</span><br><span class="line">        #      matchLabels:</span><br><span class="line">        #        k8s-app: coredns&#123;&#123; coredns_ordinal_suffix | default(<span class="string">''</span>) &#125;&#125;</span><br><span class="line"></span><br><span class="line">或者在spec一行添加代码：</span><br><span class="line">spec:</span><br><span class="line">  replicas: <span class="number">1</span>   <span class="comment">//指定pod为1个副本</span></span><br></pre></td></tr></table></figure></p><h2 id="安装K8s集群"><a href="#安装K8s集群" class="headerlink" title="安装K8s集群"></a>安装K8s集群</h2><h3 id="K8s高可用方案"><a href="#K8s高可用方案" class="headerlink" title="K8s高可用方案"></a>K8s高可用方案</h3><p>Kubernetes的高可用，要解决的核心其实是kube-apiserver组件和etcd的高可用，其它组件在多节点模式下，本身拥有天然容灾性。</p><p><strong>etcd高可用</strong></p><p>etcd本身就支持集群模式，所以啥都不用考虑，只要保证节点数量足够，升级备份之类的事情，kubespray本身就支持多节点etcd部署。由于etcd采用Raft一致性算法，集群规模最好不要超过9个，推荐3，5，7，9个数量。具体看集群规模。如果性能不够，宁可多分配资源，也最好不要超过9个。</p><p><strong>api 高可用</strong></p><p>api的高可用，一般有2种思路。</p><p>各节点自己代理<br>使用这种方式，会在每个Node节点启动一个Nginx代理，然后由这个Nginx代理负载所有的master节点的api。master会访问自己节点下的api（localhost）。这是Kubespray部署的默认方式。</p><p>外置负载均衡<br>利用外部的负载均衡实现，例如阿里云的SLB或自建的HAproxy等。</p><p>将hyperkube和kubeadm包下载到所有k8s节点的/tmp/releases 目录下，为了避免科学上网，此处，我下载存放到了网盘上。<br><a href="https://pan.baidu.com/s/1m2rF1dRXIZh_15OevTDbnA" target="_blank" rel="noopener">https://pan.baidu.com/s/1m2rF1dRXIZh_15OevTDbnA</a></p><p>执行部署命令<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> cluster<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> -v -k</span><br></pre></td></tr></table></figure></p><h3 id="运维经验"><a href="#运维经验" class="headerlink" title="运维经验"></a>运维经验</h3><p>如果需要扩容Work节点，则修改hosts.ini文件，增加新增的机器信息。然后执行下面的命令：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> scale<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> -v -k</span><br></pre></td></tr></table></figure></p><p>将hosts.ini文件中的master和etcd的机器增加到多台，执行部署命令<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> cluster<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> -vvv</span><br></pre></td></tr></table></figure></p><p>刪除节点，如果不指定节点就是刪除整个集群：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> remove-node<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> -v</span><br></pre></td></tr></table></figure></p><p>如果需要卸载，可以执行以下命令：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> reset<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> –vvv</span><br></pre></td></tr></table></figure></p><p>升级K8s集群，选择对应的k8s版本信息，执行升级命令。涉及文件为upgrade-cluster.yml。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook upgrade-cluster<span class="selector-class">.yml</span> -<span class="selector-tag">b</span> -<span class="selector-tag">i</span> inventory/mycluster/hosts<span class="selector-class">.ini</span> -e kube_version=vX<span class="selector-class">.XX</span><span class="selector-class">.XX</span> -vvv</span><br></pre></td></tr></table></figure></p><h2 id="登录Dashboard"><a href="#登录Dashboard" class="headerlink" title="登录Dashboard"></a>登录Dashboard</h2><p>登陆Dashboard 支持 kubeconfig 和 token 两种认证方式，kubeconfig 也依赖 token 字段，所以生成 token这一步是必不可少的。此处，我们获取集群管理员（拥有所有命名空间的 admin 权限）的token。</p><p>查看kubernetes-dashboard暴露的端口，如下所示，这里是31777端口。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl <span class="built_in">get</span> svc --<span class="keyword">all</span>-namespaces | <span class="keyword">grep</span> kubernetes-dashboard</span><br><span class="line">kube-<span class="built_in">system</span>   kubernetes-dashboard   NodePort    <span class="number">10.233</span>.<span class="number">34.183</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">443</span>:<span class="number">31777</span>/TCP            <span class="number">12</span>h</span><br></pre></td></tr></table></figure></p><p>获取admin的token<br><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token</span><br><span class="line">Name:         namespace-controller-token-kmtg<span class="number">7</span></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line">token:      eyJhbGciOiJSUzI<span class="number">1</span>NiIsImtpZCI<span class="number">6</span>IiJ<span class="number">9</span>.eyJpc<span class="number">3</span>MiOiJrdWJlcm<span class="number">5</span>ldGVzL<span class="number">3</span>NlcnZpY<span class="number">2</span>VhY<span class="number">2</span>NvdW<span class="number">50</span>Iiwia<span class="number">3</span>ViZXJuZXRlcy<span class="number">5</span>pby<span class="number">9</span>zZXJ<span class="number">2</span>aWNlYWNjb<span class="number">3</span>VudC<span class="number">9</span>uYW<span class="number">1</span>lc<span class="number">3</span>BhY<span class="number">2</span>UiOiJrdWJlLXN<span class="number">5</span><span class="keyword">c</span><span class="number">3</span>RlbSIsImt<span class="number">1</span>YmVybmV<span class="number">0</span>ZXMuaW<span class="number">8</span>vc<span class="number">2</span>VydmljZWFjY<span class="number">291</span>bnQvc<span class="number">2</span>VjcmV<span class="number">0</span>Lm<span class="number">5</span>hbWUiOiJuYW<span class="number">1</span>lc<span class="number">3</span>BhY<span class="number">2</span>UtY<span class="number">29</span>udHJvbGxlc<span class="keyword">i10</span>b<span class="number">2</span>tlb<span class="keyword">i1</span>rbXRnNyIsImt<span class="number">1</span>YmVybmV<span class="number">0</span>ZXMuaW<span class="number">8</span>vc<span class="number">2</span>VydmljZWFjY<span class="number">291</span>bnQvc<span class="number">2</span>VydmljZS<span class="number">1</span>hY<span class="number">2</span>NvdW<span class="number">50</span>Lm<span class="number">5</span>hbWUiOiJuYW<span class="number">1</span>lc<span class="number">3</span>BhY<span class="number">2</span>UtY<span class="number">29</span>udHJvbGxlciIsImt<span class="number">1</span>YmVybmV<span class="number">0</span>ZXMuaW<span class="number">8</span>vc<span class="number">2</span>VydmljZWFjY<span class="number">291</span>bnQvc<span class="number">2</span>VydmljZS<span class="number">1</span>hY<span class="number">2</span>NvdW<span class="number">50</span>LnVpZCI<span class="number">6</span>ImQwYTI<span class="number">0</span>N<span class="number">2</span>JkLTEwY<span class="number">2</span>EtMTFlOS<span class="number">1</span>iYTFiLWEwMzY<span class="number">5</span>ZjNmMDQwNCIsInN<span class="number">1</span>YiI<span class="number">6</span>InN<span class="number">5</span><span class="keyword">c</span><span class="number">3</span>RlbTpzZXJ<span class="number">2</span>aWNlYWNjb<span class="number">3</span>VudDprdWJlLXN<span class="number">5</span><span class="keyword">c</span><span class="number">3</span>RlbTpuYW<span class="number">1</span>lc<span class="number">3</span>BhY<span class="number">2</span>UtY<span class="number">29</span>udHJvbGxlciJ<span class="number">9</span>.v<span class="number">689</span>nSk_SxDLWk<span class="number">5</span>Mna<span class="number">0</span>t<span class="number">9</span>uITRE<span class="number">1</span>Jy<span class="number">2</span>mstZxeJfZmQmm<span class="number">2</span>UsQ-vIm<span class="number">4</span>ueUNtCoA-PNx<span class="number">49</span>s<span class="number">9</span>hic-Pn<span class="number">6</span>PfqyWQQW_QQ<span class="number">1</span>yLDjjp<span class="number">1</span>wl<span class="number">4</span>J<span class="number">3</span>tdar<span class="number">8</span>fBfuR<span class="number">7</span>Zvm<span class="number">5</span>aKw<span class="number">8</span>kyRhfQzQQZgEKlgBEHaYyKicgVUwEupa<span class="number">3</span>zevXdUTnLH<span class="number">8</span>FudcOdWEwgCflCveHdkRwoy<span class="number">88</span>pYPyL<span class="number">5</span>wh<span class="number">2</span>egEKpeDhzOEztULs<span class="keyword">i2</span>g<span class="number">3</span>tpdlyg_uQIaKJ<span class="number">1</span>OBODJZz<span class="number">5</span>PXVFMYyIk<span class="number">06</span>SyciEOX<span class="number">0</span>YxF<span class="number">3</span>pH_uSlPqg<span class="number">4</span>RxMaeTfPhlWTnFPlIjQ<span class="number">2</span>juK<span class="number">4</span>s<span class="number">0</span>o<span class="number">2</span>Tyg_sftLSXvd<span class="number">3</span>QtOg<span class="number">3</span>tBavRm<span class="number">3</span>pzHISIPbtN<span class="number">7</span>EZAyWZQ</span><br></pre></td></tr></table></figure></p><p>在dashboard登录页面上使用上面输出中的那个非常长的字符串作为token登录，即可以拥有管理员权限操作整个kubernetes集群中的对象。当然您也可以将这串token加到admin用户的kubeconfig文件中，继续使用kubeconfig登录，两种认证方式任您选择。登录dashboard。<br><a href="https://172.16.0.180:31777" target="_blank" rel="noopener">https://172.16.0.180:31777</a></p><p>注意<br>由于这里使用的HTTPS，并未使用证书，因此使用Google等浏览器会终止访问（真想说一句，Google真尼玛操蛋，强制使用HTTPS协议）。建议使用firefox浏览器。</p><h2 id="验证K8s集群"><a href="#验证K8s集群" class="headerlink" title="验证K8s集群"></a>验证K8s集群</h2><p>查看集群状态<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME   STATUS   ROLES         AGE   <span class="keyword">VERSION</span></span><br><span class="line">k8s    Ready    <span class="literal">master</span>,<span class="keyword">node</span>   <span class="title">36m</span>   v1.<span class="number">13.1</span></span><br></pre></td></tr></table></figure></p><p>查看集群Pod状态<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers<span class="number">-687</span>b7cc79c-knj87   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   calico-node<span class="number">-7</span>rj8c                          <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   coredns<span class="number">-5</span>b47d4476c<span class="number">-8</span>wdb7                   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   coredns<span class="number">-5</span>b47d4476c<span class="number">-92</span>wnq                   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   dns-autoscaler<span class="number">-5</span>b547856bc<span class="number">-95</span>cft            <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   kube-apiserver-k8s                         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">36</span>m</span><br><span class="line">kube-system   kube-controller-manager-k8s                <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">36</span>m</span><br><span class="line">kube-system   kube-proxy-cdlzp                           <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br><span class="line">kube-system   kube-scheduler-k8s                         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">36</span>m</span><br><span class="line">kube-system   kubernetes-dashboard-d7978b5cc-lvf6l       <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">35</span>m</span><br></pre></td></tr></table></figure></p><p>查看ipvs<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n</span><br><span class="line"><span class="built_in">IP</span> Virtual Server version <span class="number">1.2</span><span class="meta">.1</span> (size=<span class="number">4096</span>)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  <span class="number">172.16</span><span class="meta">.0</span><span class="meta">.180</span>:<span class="number">32714</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.133</span>:<span class="number">8443</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">172.17</span><span class="meta">.0</span><span class="meta">.1</span>:<span class="number">32714</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.133</span>:<span class="number">8443</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">10.233</span><span class="meta">.0</span><span class="meta">.1</span>:<span class="number">443</span> rr</span><br><span class="line">  -&gt; <span class="number">172.16</span><span class="meta">.0</span><span class="meta">.180</span>:<span class="number">6443</span>            Masq    <span class="number">1</span>      <span class="number">5</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">10.233</span><span class="meta">.0</span><span class="meta">.3</span>:<span class="number">53</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.131</span>:<span class="number">53</span>             Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.134</span>:<span class="number">53</span>             Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">10.233</span><span class="meta">.0</span><span class="meta">.3</span>:<span class="number">9153</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.131</span>:<span class="number">9153</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.134</span>:<span class="number">9153</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">10.233</span><span class="meta">.45</span><span class="meta">.198</span>:<span class="number">443</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.133</span>:<span class="number">8443</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.128</span>:<span class="number">32714</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.133</span>:<span class="number">8443</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">TCP  <span class="number">127.0</span><span class="meta">.0</span><span class="meta">.1</span>:<span class="number">32714</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.133</span>:<span class="number">8443</span>           Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">UDP  <span class="number">10.233</span><span class="meta">.0</span><span class="meta">.3</span>:<span class="number">53</span> rr</span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.131</span>:<span class="number">53</span>             Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span>         </span><br><span class="line">  -&gt; <span class="number">10.233</span><span class="meta">.65</span><span class="meta">.134</span>:<span class="number">53</span>             Masq    <span class="number">1</span>      <span class="number">0</span>          <span class="number">0</span></span><br></pre></td></tr></table></figure></p><p>创建一个Nginx应用的deplpyment。K8s中，针对无状态类服务推荐使用Deployment，有状态类服务则建议使用Statefulset。RC和RS已不支持目前K8s的诸多新特性了。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim nginx-deployment.yaml </span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span> </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-dm</span></span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span> </span><br><span class="line"><span class="attr">    metadata:</span> </span><br><span class="line"><span class="attr">      labels:</span> </span><br><span class="line"><span class="attr">        name:</span> <span class="string">nginx</span> </span><br><span class="line"><span class="attr">    spec:</span> </span><br><span class="line"><span class="attr">      containers:</span> </span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nginx</span> </span><br><span class="line"><span class="attr">          image:</span> <span class="attr">nginx:alpine</span> </span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">              name:</span> <span class="string">http</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-svc</span> </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">    - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      protocol:</span> <span class="string">TCP</span> </span><br><span class="line"><span class="attr">  selector:</span> </span><br><span class="line"><span class="attr">    name:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># kubectl apply -f nginx-deployment.yaml </span><br><span class="line">deployment.apps/nginx-dm created</span><br><span class="line">service/nginx-svc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># kubectl get pods</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-dm<span class="number">-799879696</span>c<span class="number">-9</span>cdgz   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">30</span>s</span><br><span class="line">nginx-dm<span class="number">-799879696</span>c-cwzn5   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">30</span>s</span><br><span class="line">nginx-dm<span class="number">-799879696</span>c-xwjd7   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">30</span>s</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"># kubectl get svc -o wide</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR</span><br><span class="line">kubernetes   ClusterIP   <span class="number">10.233</span><span class="number">.0</span><span class="number">.1</span>      &lt;none&gt;        <span class="number">443</span>/TCP   <span class="number">39</span>m   &lt;none&gt;</span><br><span class="line">nginx-svc    ClusterIP   <span class="number">10.233</span><span class="number">.42</span><span class="number">.172</span>   &lt;none&gt;        <span class="number">80</span>/TCP    <span class="number">65</span>s   name=nginx</span><br><span class="line"></span><br><span class="line"># ipvsadm -L -n</span><br></pre></td></tr></table></figure><p>测试nginx服务是否正常<br><figure class="highlight ldif"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -I 10.233.42.172</span></span><br><span class="line"><span class="attribute">HTTP/1.1 200 OK</span></span><br><span class="line"><span class="attribute">Server</span>: nginx/1.15.8</span><br><span class="line"><span class="attribute">Date</span>: Sat, 05 Jan 2019 09:58:16 GMT</span><br><span class="line"><span class="attribute">Content-Type</span>: text/html</span><br><span class="line"><span class="attribute">Content-Length</span>: 612</span><br><span class="line"><span class="attribute">Last-Modified</span>: Wed, 26 Dec 2018 23:21:49 GMT</span><br><span class="line"><span class="attribute">Connection</span>: keep-alive</span><br><span class="line"><span class="attribute">ETag</span>: "5c240d0d-264"</span><br><span class="line"><span class="attribute">Accept-Ranges</span>: bytes</span><br></pre></td></tr></table></figure></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>K8s从1.11版本起便废弃了Heapster监控组件，取而代之的是metrics-server 和 custom metrics API，后面将陆续完善包括Prometheus+Grafana监控，Kibana+Fluentd日志管理，cephfs-provisioner存储（可能需要重新build kube-controller-manager装上rbd相关的包才能使用Ceph RBD StorageClass），traefik ingress等服务。</p><p>参考资料<br><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;部署Kubernetes除了手动方式外，还有诸如Kubeadm、Kubespray、Breeze、Rancher、kargo等多种自动化方式
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
      <category term="kubespray" scheme="http://yoursite.com/tags/kubespray/"/>
    
  </entry>
  
  <entry>
    <title>使用Scrum管理软件开发项目</title>
    <link href="http://yoursite.com/2018/10/23/%E4%BD%BF%E7%94%A8Scrum%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE/"/>
    <id>http://yoursite.com/2018/10/23/使用Scrum管理软件开发项目/</id>
    <published>2018-10-23T15:39:59.000Z</published>
    <updated>2018-10-23T16:08:40.444Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scrum-价值观"><a href="#Scrum-价值观" class="headerlink" title="Scrum 价值观"></a>Scrum 价值观</h2><p><img src="/images/scrumjiazhi.jpg" alt="image"></p><h2 id="Scrum三种角色"><a href="#Scrum三种角色" class="headerlink" title="Scrum三种角色"></a>Scrum三种角色</h2><ul><li>Product Owner：产品负责人，确定「大家要做什么」。一般由相关的产品经理担任；如果是为客户做项目，PO 一般就是客户负责人。 </li><li>Scrum Master：Scrum的推动者，掌控大节奏的人。 </li><li>Team：一般由多个 developer 组成，开发的主力。</li></ul><h2 id="Scrum执行流程"><a href="#Scrum执行流程" class="headerlink" title="Scrum执行流程"></a>Scrum执行流程</h2><p><img src="/images/scrumgongzuoliu.jpg" alt="image"></p><p>概言之，就是<br>1.产品经理整理出需求列表，做好优先级排序，这些需求可以来自客户或者产品经理的判断<br>2.开个会，和研发团队一起讨论需求列表的工作量，确定要进行Sprint（冲刺开发）的任务<br>3.研发团队进入开发环节，这个期间可以让产品经理修改需求，但是不得延期或修改验收标准。Sprint期间每天要进行站会，汇报工作进展、问题和下一步计划<br>4.再开个会，和产品经理一起验收产品，并确定最终的可发布版本。<br>5.整个团队一起回顾项目，提出改进点，然后进入下一个Sprint</p><p><strong>Scrum敏捷开发执行由Product Backlog、Sprint Backlog、燃尽图/看板和Sprint会议等要素组成。</strong></p><p>说明：Backlog 是 Scrum中的一个专用名词，意思是待办工作事项的集合。</p><h3 id="1-Sprint迭代周期"><a href="#1-Sprint迭代周期" class="headerlink" title="1.Sprint迭代周期"></a>1.Sprint迭代周期</h3><p>整个开发过程由若干个短的迭代周期组成，一个短的迭代周期称为一个Sprint，每个Sprint的长度是2到4周。</p><h3 id="2-组建团队"><a href="#2-组建团队" class="headerlink" title="2.组建团队"></a>2.组建团队</h3><p>Scrum团队，推荐人数在5-10人左右，可以按需组建相应的团队等。如根据所开发的软件系统特点，将全员分成4个小组，分别是管理和产品组、前端组A、后端组B、测试组。</p><h3 id="3-Product-Backlog"><a href="#3-Product-Backlog" class="headerlink" title="3.Product Backlog"></a>3.Product Backlog</h3><p>即产品待办事项列表，也被称为用户故事，是量化的用户需求。即PO、Scrum Master、Team围绕产品创建一个团队需要做的所有事情的列表。然后产品负责人进行筛选并按照优先级进行排序，以商业价值作为排序的主要原则，从Product Backlog中挑选高优先级的需求进行开发，挑选的需求在Sprint计划会议上经过讨论、分析和估算得到相应的任务列表，称它为Sprint backlog。</p><p>Product backlog中列举了本项目应该实现的需求，需求采用了用户故事的方式进行描述，用户故事是一句简短的采用用户熟悉的术语表达的需求，是用户讲给开发人员的故事，不是开发人员讲给用户的故事。 故事可以有标准的格式，称为三段论式故事，哪三段呢？</p><ul><li>用户角色</li><li>需要的功能</li><li>目的</li></ul><p>比如，有这样一个故事：<br>作为一个家庭主妇，我需要一个30平米的餐厅，以便于我可以招待10位朋友来用餐，光线明亮。</p><p>除了用户故事本身之外，还包括用户故事的验收标准，或者叫用户故事的测试要点，这也是由product owner来完成的，product owner可以先完成前三段，在和team member的沟通过程中，逐步丰富完善验收标准。Product backlog在项目进展过程中是会发生变化的，只有product owner有权来修改此文档。可以用EXCEL文件来管理它。</p><p>Product backlog = 用户故事 + 优先级 + 验收标准</p><h3 id="4-Sprint-Backlog"><a href="#4-Sprint-Backlog" class="headerlink" title="4.Sprint Backlog"></a>4.Sprint Backlog</h3><p>即任务列表，是一次迭代中团队需要完成的具体任务，也是开发过程用得最多的Backlog，非常细化。当Scrum团队完成Sprint backlog列表中的所有任务时，本次Sprint结束，进入下一个Sprint迭代周期。可以用Jira类敏捷工具来管理。</p><p>每个月，团队都努力实现列表最顶端的任务，这一部分是他们估计需要一个月完成的工作。把它展开成一个详细的任务列表，叫做Sprint Backlog。这个团队承诺在月底向出资人演示或交付结果。用故事点的方式，即用斐波那契数列的数字（1，2，3，5，8，13，21……）的形式去估算时间。单个用户故事点数不超过8是最理性的状态，超过21则需要拆分。</p><p>如图所示。<br><img src="/images/sprintbacklog.png" alt="image"></p><p>Product Backlog和Sprint Backlog两者区别如图所示。<br><img src="/images/duibi.png" alt="image"> </p><h3 id="5-燃尽图和看板"><a href="#5-燃尽图和看板" class="headerlink" title="5.燃尽图和看板"></a>5.燃尽图和看板</h3><h4 id="5-1燃尽图"><a href="#5-1燃尽图" class="headerlink" title="5.1燃尽图"></a>5.1燃尽图</h4><p>即实时评估完成任务数量和剩余时间的趋势。每天Scrum Master都会记录待完成的剩余点数，而后画在燃尽图上。可以使用Jira系统的燃尽图。<br><img src="/images/ranjingtu.jpg" alt="image"> </p><p>横坐标为工作日期，纵坐标为工作量（任务数），每个点代表了在那一天估计剩余的工作量，通过折线依次连接起所有的点形成估计剩余工作量的趋势线。另外还有一条控制线，为最初的估计工作量到结束日期的连线，一般用不同的颜色画上面的两根线。</p><p>对此图的研判规则如下：<br>1）如果趋势线在控制线以下，说明进展顺利，有比较大的概率按期或提前完工；<br>2）如果趋势线在控制线以上，说明有比较大的概率延期，此时需要关注进度了。</p><p> 注意，趋势线并非一直下行，也有可能上行，即发生了错误的估计或遗漏的任务时，估计剩余的工作量也有可能在某天上升了。</p><p>每天开完15分钟站立会议后，由scrum master根据进展更新燃尽图。第1个点是项目最初的工作量估计值，第2个点是第最初的估计工作量减去第1天已经完成的任务的工作量，依次类推计算后续的点。任务完成的标志是什么呢？准则如下：<br>1）开发人员检测：所有的单元测试用例都通过；<br>2）Product owner或测试人员检测：通过了所有的功能测试；</p><p>燃尽图最好是张贴在白板上，让每个人项目组成员抬头就能看见，这样给大家一个明确的视觉效果，每个人随时都能看到我们离目标有多远。燃尽图可以每天画，表示完成某个迭代的进展趋势，也可以某次迭代结束的时候画，表示完成整个项目的进展趋势，此时横坐标就是迭代的顺序号。</p><h4 id="5-2看板"><a href="#5-2看板" class="headerlink" title="5.2看板"></a>5.2看板</h4><p>让工作透明化除了使用燃尽图之外，还有看板。看板的栏目大致包括待办事项、进行中事项以及已完成事项三个部分。随着迭代进度的推进，由Team每天及时将事项转移到对应看板栏目下。<br><img src="/images/kanban.png" alt="image"> </p><h3 id="6-任务申领"><a href="#6-任务申领" class="headerlink" title="6.任务申领"></a>6.任务申领</h3><p>原则上，奉行Scrum各个组里的成员主动申领任务。</p><h3 id="7-Scrum四种会议"><a href="#7-Scrum四种会议" class="headerlink" title="7.Scrum四种会议"></a>7.Scrum四种会议</h3><p>在SCRUM方法中定义了4种会议活动：<br> <img src="/images/scrumhuiyi.png" alt="image"> </p><ul><li>Sprint planning<br>即Sprint规划会(Sprint Planning)。这是第一场Scrum会议。团队成员、Scrum Master以及产品负责人坐到一起，规划冲刺的内容。PO 向大家介绍排好序的产品待办事项（Production Backlog），然后大家共同思考决定如何推进计划，梳理出 Sprint Backlog 来完成后续的工作。简单点说，就是一个大家理解「需要做什么」，然后讨论「怎么完成」，并形成待办事项列表的会议。</li></ul><p>冲刺周期一般是固定的，不超过一个月，一般是2-4周。团队要从待办事项清单的顶端着手（即从最重要的事项着手），评估一个冲刺阶段能完成多少。</p><ul><li><p>Daily meeting<br>即每日站会，每天团队都面对面地开5～10分钟的会。每日例会后每个人更新自己的任务表，还要每天更新下燃尽图。<br>1）昨天我完成了什么工作？<br>2）今天我打算做什么工作？<br>3）我遇到了哪些困难，打算怎么办，或需要哪些帮助？</p></li><li><p>Sprint review<br>即Sprint评估或展示会议(Sprint Review)。在冲刺结束前，给产品负责人展示成果，也就是展示哪些事项可以挪到“完成事项”那一栏，并接受评价。这是一场公开的会议，任何人都可以是参与者，不仅仅包括产品负责人、ScrumMaster及开发团队，还包括利益相关者、管理人员与客户。团队应该只展示那些符合“完成定义”的事项，也就是全部完成，不需要再做工作就能交付的成果。这个成果或许不是完整的产品，但至少是一项完整的、可以使用的功能。</p></li><li><p>Sprint retrospective<br>即Sprint回顾会议，会议维持在2小时以内，主要总结此次Sprint实施的经验教训，如何在下一个Sprint中发挥。冲刺回顾会要认真分析以下几个问题：<br>1）发生了哪些有待改进的事；<br>2）为什么会发生那件事；<br>3）为什么我们当时忽略了；<br>4）怎样才能加快工作进度。</p></li></ul><p>除去开发活动外这4种会议构成了scrum方法的核心活动。这四种会议的要点如下。<br> <img src="/images/huiyiduibi.jpg" alt="image"> </p><p>最后，敏捷开发在线协作工具，有Jira、Teambition和Worktitle等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Scrum-价值观&quot;&gt;&lt;a href=&quot;#Scrum-价值观&quot; class=&quot;headerlink&quot; title=&quot;Scrum 价值观&quot;&gt;&lt;/a&gt;Scrum 价值观&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/scrumjiazhi.jpg&quot; alt=&quot;im
      
    
    </summary>
    
      <category term="项目管理" scheme="http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="Scrum" scheme="http://yoursite.com/tags/Scrum/"/>
    
      <category term="项目管理" scheme="http://yoursite.com/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>配置Gnocchi使用Ceph后端存储</title>
    <link href="http://yoursite.com/2018/06/10/%E9%85%8D%E7%BD%AEGnocchi%E4%BD%BF%E7%94%A8Ceph%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8/"/>
    <id>http://yoursite.com/2018/06/10/配置Gnocchi使用Ceph后端存储/</id>
    <published>2018-06-10T10:47:46.000Z</published>
    <updated>2018-06-14T13:29:18.742Z</updated>
    
    <content type="html"><![CDATA[<p>1.给gnocchi创建一个专用的ceph pool，用来存放计量数据。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd pool create gnocchi <span class="number">128</span> <span class="number">128</span></span><br></pre></td></tr></table></figure></p><p>gnocchi pool的pg_num需要根据实际的ceph环境确定。</p><p>2.给gnocchi创建一个ceph用户。<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ceph auth get-<span class="keyword">or</span>-<span class="built_in">create</span> client.gnocchi mon <span class="string">'allow r'</span> osd <span class="string">'allow class-read object_prefix rbd_children, allow rwx pool=gnocchi'</span></span><br><span class="line">[client.gnocchi]</span><br><span class="line">key = AQB+<span class="number">5</span>xxbZKKQIhAA7FgJKYBQNjF6dD3FZzJvUQ==</span><br></pre></td></tr></table></figure></p><p>3.保存keyring文件<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">ceph </span><span class="string">auth </span><span class="built_in">get-or-create</span> <span class="string">client.</span><span class="string">gnocchi </span>| <span class="string">tee </span><span class="string">ceph.</span><span class="string">client.</span><span class="string">gnocchi.</span><span class="string">keyring</span></span><br></pre></td></tr></table></figure></p><p>4.在gnocchi容器中创建/etc/ceph目录<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_api mkdir /etc/ceph</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_statsd mkdir /etc/ceph</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_metricd mkdir /etc/ceph</span></span><br></pre></td></tr></table></figure></p><p>5.在ceph节点拷贝相关文件到controller节点<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> scp ceph.client.gnocchi.keyring controller1:/home</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> scp /etc/ceph/ceph.conf controller1:/home</span></span><br></pre></td></tr></table></figure></p><p>6.拷贝gnocchi的keyring文件和ceph配置文件到gnocchi容器的/etc/ceph目录下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.conf gnocchi_api:/etc/ceph/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.client.gnocchi.keyring gnocchi_api:/etc/ceph/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.conf gnocchi_statsd:/etc/ceph/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.client.gnocchi.keyring gnocchi_statsd:/etc/ceph/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.conf gnocchi_metricd:/etc/ceph/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp /home/ceph.client.gnocchi.keyring gnocchi_metricd:/etc/ceph/</span></span><br></pre></td></tr></table></figure></p><p>7.修改文件gnocchi.conf<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/kolla/gnocchi-api/gnocchi.conf </span></span><br><span class="line"><span class="comment"># vim /etc/kolla/gnocchi-metricd/gnocchi.conf </span></span><br><span class="line"><span class="comment"># vim /etc/kolla/gnocchi-statsd/gnocchi.conf </span></span><br><span class="line"></span><br><span class="line">添加如下内容</span><br><span class="line">[storage]</span><br><span class="line">driver = ceph</span><br><span class="line">ceph_username = gnocchi</span><br><span class="line">ceph_keyring = /etc/ceph/ceph.client.gnocchi.keyring</span><br><span class="line">ceph_conffile = /etc/ceph/ceph.conf</span><br><span class="line">ceph_secret = AQB+5xxbZKKQIhAA7FgJKYBQNjF6dD3FZzJvUQ==</span><br><span class="line">ceph_pool = gnocchi</span><br></pre></td></tr></table></figure></p><p>初始化数据库<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -it gnocchi_api gnocchi-upgrade</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -it gnocchi_metricd gnocchi-upgrade</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -it gnocchi_statsd gnocchi-upgrade</span></span><br></pre></td></tr></table></figure></p><p>8.设置权限<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_api chown -R gnocchi:gnocchi /etc/ceph/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_metricd chown -R gnocchi:gnocchi /etc/ceph/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it gnocchi_statsd chown -R gnocchi:gnocchi /etc/ceph/</span></span><br></pre></td></tr></table></figure></p><p>9.重启服务生效<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker restart gnocchi_api gnocchi_metricd gnocchi_statsd</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.给gnocchi创建一个专用的ceph pool，用来存放计量数据。&lt;br&gt;&lt;figure class=&quot;highlight lsl&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>安装OpenStack Zaqar服务</title>
    <link href="http://yoursite.com/2018/05/26/%E5%AE%89%E8%A3%85OpenStack-Zaqar%E6%9C%8D%E5%8A%A1/"/>
    <id>http://yoursite.com/2018/05/26/安装OpenStack-Zaqar服务/</id>
    <published>2018-05-26T07:41:52.000Z</published>
    <updated>2018-06-03T09:11:40.403Z</updated>
    
    <content type="html"><![CDATA[<p>关于什么是Zaqar，有什么作用。国内已有介绍读者可以自行Google查阅。若在此再阐述，已显多余。由于安装Zaqar服务官方文档还有坑且国内无资料，故这里就写一写吧。</p><p><strong>依赖服务</strong></p><ul><li>一个基本的OpenStack正常运行环境</li><li>MongoDB（必须大于或等于2个节点，否则会报错）</li><li>Memcache</li></ul><p><strong>说明</strong></p><p>本环境，MongoDB和memcache已有kolla-ansible事先安装好。所以，这里只需要安装配置zaqar服务即可。</p><p>1.创建keystone认证信息<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> admin-openrc.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> useradd zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack user create --domain default --password-prompt zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack role add --project service --user zaqar admin</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack service create --name zaqar --description <span class="string">"Messaging"</span> messaging</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging public http://172.17.223.21:8888</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging internal http://172.17.223.21:8888</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack endpoint create --region RegionOne messaging admin http://172.17.223.21:8888</span></span><br></pre></td></tr></table></figure></p><p>2.安装zaqar，这里使用queens版本<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://git.openstack.org/openstack/zaqar.git -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install . -r ./requirements.txt --upgrade --<span class="built_in">log</span> /tmp/zaqar-pip.log</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install --upgrade pymongo gevent uwsgi</span></span><br></pre></td></tr></table></figure></p><p>3.创建zaqar配置目录<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/zaqar</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> oslopolicy-sample-generator --config-file etc/oslo-config-generator/zaqar-policy-generator.conf</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp etc/zaqar.policy.yaml.sample /etc/zaqar/policy.yaml</span></span><br></pre></td></tr></table></figure></p><p>4.创建zaqar的log文件<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># touch /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br><span class="line"># chown zaqar:zaqar /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br><span class="line"># chmod <span class="number">600</span> /<span class="built_in">var</span>/<span class="built_in">log</span>/zaqar-server.<span class="built_in">log</span></span><br></pre></td></tr></table></figure></p><p>5.创建/srv/zaqar目录<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir /srv/zaqar</span></span><br><span class="line"><span class="comment"># vim /srv/zaqar/zaqar_uwsgi.py</span></span><br><span class="line"><span class="keyword">from</span> keystonemiddleware <span class="keyword">import</span> auth_token</span><br><span class="line"><span class="keyword">from</span> zaqar.transport.wsgi <span class="keyword">import</span> app</span><br><span class="line">app = auth_token.AuthProtocol(app.app, &#123;&#125;)</span><br></pre></td></tr></table></figure></p><p><strong>说明</strong></p><p>注意，下面的参数“listen = 1024”，超过了系统的默认值128，会报错。因此需要修改默认值，如这里的2048。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># echo "net.core.somaxconn=2048" | sudo tee --append /etc/sysctl.conf</span></span><br><span class="line"><span class="comment"># echo 2048 &gt; /proc/sys/net/core/somaxconn</span></span><br><span class="line"><span class="comment"># echo 2048 &gt; /proc/sys/net/ipv4/tcp_max_syn_backlog</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim /srv/zaqar/uwsgi.ini</span></span><br><span class="line"><span class="section">[uwsgi]</span></span><br><span class="line"><span class="attr">http</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">8888</span></span><br><span class="line"><span class="attr">pidfile</span> = /var/run/zaqar.pid</span><br><span class="line"><span class="attr">gevent</span> = <span class="number">2000</span></span><br><span class="line"><span class="attr">gevent-monkey-patch</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">listen</span> = <span class="number">1024</span></span><br><span class="line"><span class="attr">enable-threads</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">chdir</span> = /srv/zaqar</span><br><span class="line"><span class="attr">module</span> = zaqar_uwsgi:app</span><br><span class="line"><span class="attr">workers</span> = <span class="number">4</span></span><br><span class="line"><span class="attr">harakiri</span> = <span class="number">60</span></span><br><span class="line"><span class="attr">add-header</span> = Connection: close</span><br></pre></td></tr></table></figure></p><p>6.创建pid文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> touch /var/run/zaqar.pid</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chown zaqar:zaqar /var/run/zaqar.pid</span></span><br></pre></td></tr></table></figure></p><p>7.编辑zaqar配置文件<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/zaqar/zaqar.conf</span></span><br><span class="line"><span class="section">[DEFAULT]</span></span><br><span class="line"><span class="attr">pooling</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">admin_mode</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">debug</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">log_file</span> = /var/log/zaqar-server.log</span><br><span class="line"><span class="comment">;auth_strategy = keystone</span></span><br><span class="line"><span class="section">[keystone_authtoken]</span></span><br><span class="line"><span class="attr">auth_uri</span> = http://<span class="number">172.17</span>.<span class="number">223.20</span>:<span class="number">5000</span></span><br><span class="line"><span class="attr">auth_url</span> = http://<span class="number">172.17</span>.<span class="number">223.20</span>:<span class="number">35357</span></span><br><span class="line"><span class="attr">auth_type</span> = password</span><br><span class="line"><span class="attr">project_domain_id</span> = default</span><br><span class="line"><span class="attr">user_domain_id</span> = default</span><br><span class="line"><span class="attr">project_name</span> = service</span><br><span class="line"><span class="attr">username</span> = zaqar</span><br><span class="line"><span class="attr">password</span> = zaqar</span><br><span class="line"><span class="attr">memcache_security_strategy</span> = ENCRYPT</span><br><span class="line"><span class="attr">memcache_secret_key</span> = wRJgCSJjqUT5JWlGVyvSygovRiyXFgJg7kYz1sXX</span><br><span class="line"><span class="attr">memcached_servers</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">11211</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">11211</span></span><br><span class="line"><span class="section">[cache]</span></span><br><span class="line"><span class="attr">backend</span> = oslo_cache.memcache_pool</span><br><span class="line"><span class="attr">enabled</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">memcache_servers</span> = <span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">11211</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">11211</span></span><br><span class="line"><span class="section">[drivers]</span></span><br><span class="line"><span class="attr">transport</span> = wsgi</span><br><span class="line"><span class="attr">message_store</span> = mongodb</span><br><span class="line"><span class="attr">management_store</span> = mongodb</span><br><span class="line"><span class="section">[drivers:management_store:mongodb]</span></span><br><span class="line"><span class="attr">uri</span> = mongodb://<span class="number">172.17</span>.<span class="number">223.21</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">27017</span>/?replicaSet=rs0&amp;w=<span class="number">2</span>&amp;readPreference=secondaryPreferred</span><br><span class="line"><span class="attr">database</span> = zaqarmanagementstore</span><br><span class="line"><span class="attr">partitions</span> = <span class="number">8</span></span><br><span class="line"><span class="comment">;max_attempts = 1000</span></span><br><span class="line"><span class="comment">;max_retry_sleep = 0.1</span></span><br><span class="line"><span class="comment">;max_retry_jitter = 0.005</span></span><br><span class="line"><span class="comment">;gc_interval = 5 * 60</span></span><br><span class="line"><span class="comment">;gc_threshold = 1000</span></span><br><span class="line"><span class="section">[drivers:message_store:mongodb]</span></span><br><span class="line"><span class="attr">database</span> = zaqarmessagestore</span><br><span class="line"><span class="attr">uri</span> = mongodb://<span class="number">172.17</span>.<span class="number">223.21</span>,<span class="number">172.17</span>.<span class="number">223.26</span>:<span class="number">27017</span>/?replicaSet=rs0&amp;w=<span class="number">2</span>&amp;readPreference=secondaryPreferred</span><br><span class="line"><span class="section">[drivers:transport:wsgi]</span></span><br><span class="line"><span class="attr">bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="section">[transport]</span></span><br><span class="line"><span class="attr">max_queues_per_page</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">max_queue_metadata</span> = <span class="number">262144</span></span><br><span class="line"><span class="attr">max_mesages_per_page</span> = <span class="number">10</span></span><br><span class="line"><span class="attr">max_messages_post_size</span> = <span class="number">262144</span></span><br><span class="line"><span class="attr">max_message_ttl</span> = <span class="number">1209600</span></span><br><span class="line"><span class="attr">max_claim_ttl</span> = <span class="number">43200</span></span><br><span class="line"><span class="attr">max_claim_grace</span> = <span class="number">43200</span></span><br><span class="line"><span class="section">[signed_url]</span></span><br><span class="line"><span class="attr">secret_key</span> = SOMELONGSECRETKEY</span><br></pre></td></tr></table></figure></p><p>8.编辑uwsgi服务启动文件<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/systemd/system/zaqar.uwsgi.service</span></span><br><span class="line"><span class="section">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=uWSGI Zaqar</span><br><span class="line"><span class="attr">After</span>=syslog.target</span><br><span class="line"><span class="section">[Service]</span></span><br><span class="line"><span class="attr">ExecStart</span>=/usr/bin/uwsgi --ini /srv/zaqar/uwsgi.ini</span><br><span class="line"><span class="comment"># Requires systemd version 211 or newer</span></span><br><span class="line"><span class="attr">RuntimeDirectory</span>=uwsgi</span><br><span class="line"><span class="attr">Restart</span>=always</span><br><span class="line"><span class="attr">KillSignal</span>=SIGQUIT</span><br><span class="line"><span class="attr">Type</span>=notify</span><br><span class="line"><span class="attr">StandardError</span>=syslog</span><br><span class="line"><span class="attr">NotifyAccess</span>=all</span><br><span class="line"><span class="attr">User</span>=zaqar</span><br><span class="line"><span class="attr">Group</span>=zaqar</span><br><span class="line"><span class="section">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=multi-user.target</span><br></pre></td></tr></table></figure></p><p>启动uwsgi服务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl start zaqar.uwsgi.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> zaqar.uwsgi.service</span></span><br></pre></td></tr></table></figure></p><p>说明，如果报错可以查看日志。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tailf /var/<span class="built_in">log</span>/messages</span></span><br></pre></td></tr></table></figure></p><p>9.配置Pool</p><p>生成一个UUID<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># uuidgen</span><br><span class="line"><span class="number">7289</span>f400<span class="number">-2439</span><span class="number">-4822</span><span class="number">-9e3</span>a<span class="number">-928</span>af262d843</span><br></pre></td></tr></table></figure></p><p>运行cURL命令去请求一个keystone token。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat auth_token.json </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"auth"</span>: &#123;</span><br><span class="line">        <span class="string">"identity"</span>: &#123;</span><br><span class="line">            <span class="string">"methods"</span>: [</span><br><span class="line">                <span class="string">"password"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="string">"password"</span>: &#123;</span><br><span class="line">                <span class="string">"user"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"zaqar"</span>,</span><br><span class="line">                    <span class="string">"domain"</span>: &#123;</span><br><span class="line">                        <span class="string">"id"</span>: <span class="string">"default"</span>,</span><br><span class="line">                        <span class="string">"name"</span>: <span class="string">"Default"</span></span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"password"</span>: <span class="string">"zaqar"</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"scope"</span>: &#123;</span><br><span class="line">            <span class="string">"project"</span>: &#123;</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"service"</span>,</span><br><span class="line">                <span class="string">"domain"</span>: &#123;<span class="string">"id"</span>: <span class="string">"default"</span>&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>比如，这里获取到的token是<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># <span class="selector-tag">curl</span> <span class="selector-tag">-i</span> <span class="selector-tag">-s</span> <span class="selector-tag">-d</span>@<span class="keyword">auth_token</span>.<span class="keyword">json</span> -H <span class="string">"Content-Type: application/json"</span> http://<span class="number">172.17</span>.<span class="number">223.21</span>:<span class="number">5000</span>/v3/auth/tokens  |grep <span class="string">'X-Subject-Token'</span></span><br><span class="line">X-Subject-Token: <span class="number">7212</span>ad333e6d4a1781974f8e203bf555</span><br></pre></td></tr></table></figure></p><p>配置Pool<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># curl -i -X PUT http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/pools/testpool -d '&#123;<span class="string">"weight"</span>: <span class="number">100</span>, <span class="string">"uri"</span>: <span class="string">"mongodb://172.17.223.21,172.17.223.26:27017/?replicaSet=rs0&amp;w=2&amp;readPreference=secondaryPreferred"</span>, <span class="string">"options"</span>: &#123;<span class="string">"partitions"</span>: <span class="number">8</span>&#125;&#125;' -H <span class="string">"Client-ID: 7289f400-2439-4822-9e3a-928af262d843"</span> -H <span class="string">"X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b"</span> -H <span class="string">"Content-type: application/json"</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">201</span> Created</span><br><span class="line"><span class="built_in">content</span>-<span class="built_in">length</span>: <span class="number">0</span></span><br><span class="line"><span class="built_in">content</span>-type: application/json; charset=UTF-<span class="number">8</span></span><br><span class="line">location: http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/pools/testpool</span><br><span class="line">Connection: <span class="built_in">close</span></span><br></pre></td></tr></table></figure></p><ol start="10"><li><p>验证操作</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># curl -i -X POST http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/queues/samplequeue/messages -d '&#123;<span class="string">"messages"</span>: [&#123;<span class="string">"body"</span>: &#123;<span class="string">"event"</span>: <span class="number">1</span>&#125;, <span class="string">"ttl"</span>: <span class="number">600</span>&#125;, &#123;<span class="string">"body"</span>: &#123;<span class="string">"event"</span>: <span class="number">2</span>&#125;, <span class="string">"ttl"</span>: <span class="number">600</span>&#125;]&#125;' -H <span class="string">"Content-type: application/json"</span> -H <span class="string">"Client-ID: 7289f400-2439-4822-9e3a-928af262d843"</span> -H <span class="string">"X-Auth-Token: eb7e4d966aef4648a1f57a91e4abaa6b"</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">201</span> Created</span><br><span class="line"><span class="built_in">content</span>-<span class="built_in">length</span>: <span class="number">135</span></span><br><span class="line"><span class="built_in">content</span>-type: application/json; charset=UTF-<span class="number">8</span></span><br><span class="line">location: http://<span class="number">172.17</span>.223.21:<span class="number">8888</span>/v2/queues/samplequeue/messages?ids=5b0bb7289140d605340bf3a6,5b0bb7289140d605340bf3a7</span><br><span class="line">Connection: <span class="built_in">close</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"resources"</span>: [<span class="string">"/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a6"</span>, <span class="string">"/v2/queues/samplequeue/messages/5b0bb7289140d605340bf3a7"</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>安装Zaqar UI</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/zaqar-ui -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker cp zaqar-ui horizon:/home</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -u root -it horizon bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /home &amp;&amp; pip install -e zaqar-ui/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp zaqar-ui/zaqar_ui/enabled/_1510_project_messaging_group.py /var/lib/kolla/venv/lib/python2.7/site-packages/openstack_dashboard/<span class="built_in">local</span>/enabled</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp zaqar-ui/zaqar_ui/enabled/_1520_project_queues.py /var/lib/kolla/venv/lib/python2.7/site-packages/openstack_dashboard/<span class="built_in">local</span>/enabled</span></span><br></pre></td></tr></table></figure></li></ol><p>最后，你就可以愉快的使用Zaqar服务啦。当然，由于该项目目前尚不成熟，参阅资料很少，希望你能一边使用一边填坑。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于什么是Zaqar，有什么作用。国内已有介绍读者可以自行Google查阅。若在此再阐述，已显多余。由于安装Zaqar服务官方文档还有坑且国内无资料，故这里就写一写吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;依赖服务&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个基本的OpenS
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>从数据删除看备份的重要性</title>
    <link href="http://yoursite.com/2018/05/23/%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E7%9C%8B%E5%A4%87%E4%BB%BD%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7/"/>
    <id>http://yoursite.com/2018/05/23/从数据删除看备份的重要性/</id>
    <published>2018-05-23T14:55:14.000Z</published>
    <updated>2018-05-26T07:25:27.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>所谓，“常在河边走，哪有不湿鞋”。在一个实际的环境中，由于种种原因，可能发生数据被删除的情况。比如，云平台中的数据库、虚拟机、数据卷、镜像或底层存储被删除等，如果数据没有进行备份，则是灾难性的后果。</p><p>在笔者的工作中，经历过2次在生产环境云平台上，客户虚拟机数据被删除的情况，一次是研发部门开发的代码逻辑判断错误导致，另一次是运维同事人为误操作。因此，觉得有必要调研整理下数据备份相关的小文，以此共勉。</p><p>在一个由OpenStack+Ceph架构组成的云平台环境中，有N种数据备份方案。如OpenStack有自带的Karbor、Freezer云服务，Ceph也有相关的备份方案，也有其他商业的备份方案等。实际上，OpenStack云平台本身也提供了一些较好易用的备份功能，比如虚拟机快照/备份、数据卷快照/备份，在使用时也倡导通过将数据卷挂载给虚拟机，从而将数据写入到云盘中，间接的实现数据容灾备份。</p><p>但，如果删除的是底层存储数据，上层的备份操作基本都将报废。那么有什么好的备份方案吗。这里，我们阐述下Ceph相关的备份方案。</p><h2 id="方案1-Snapshot"><a href="#方案1-Snapshot" class="headerlink" title="方案1 Snapshot"></a>方案1 Snapshot</h2><p><strong>原理</strong></p><p>异步备份，基于RBD的snapshot机制。<br> <img src="/images/ceph-01.png" alt="image"></p><p><strong>介绍</strong></p><p>在此方案下，Cluster A &amp; B是独立的Ceph集群，通过RBD的snapshot机制，在Cluster A端，针对image定期通过rbd创建image的快照，然后通过rbd export-diff, rbd import-diff命令来完成image备份到Cluster B。</p><p><strong>命令和步骤</strong></p><p>把 Cluster A 的 pool 中的testimage 异步备份到 Cluster B 的 pool 中。</p><p>1）在Cluster A/B上创建rbd/testimage<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">create</span> -p rbd <span class="comment">--size 10240 testimage</span></span><br></pre></td></tr></table></figure></p><p>2）在准备备份image前，暂停Cluster A端对testimage的IO操作，然后创建个snapshot<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="keyword">create</span> &lt;snap-<span class="keyword">name</span>&gt;</span><br></pre></td></tr></table></figure></p><p>3）导出Cluster A端的testimage数据，不指定from-snap<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="keyword">export</span>-diff &lt;<span class="built_in">image</span>-name&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure></p><p>4）copy上一步中导出的文件到Cluster B，并导入数据到testimage<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="keyword">import</span>-diff &lt;<span class="built_in">path</span>&gt; &lt;image-<span class="keyword">name</span>&gt;</span><br></pre></td></tr></table></figure></p><p>后续需周期性的暂停Cluster A端的testimage的IO，然后创建snapshot，通过 rbd export-diff <image-name> [–from-snap <snap-name>] <path></path>命令导出incremental diff，之后把差异数据文件copy到Cluster B上，然后通过命令rbd import-diff <path></path> <image-name>导入。</image-name></snap-name></image-name></p><p>【注】：也可不暂停Cluster A端的IO，直接take snapshot；这样并不会引起image的数据不一致，只是有可能会使rbd export-diff导出的数据在take snapshot之后。</p><p><strong>此方案优缺点</strong></p><p><strong>优点：</strong></p><ul><li>命令简单，通过定制执行脚本就能实现rbd块设备的跨区备份</li></ul><p><strong>缺点：</strong></p><ul><li>每次同步前都需要在源端take snapshot或暂停IO操作</li><li>持续的snapshots可能导致image的读写性能下降</li><li>还要考虑后续删除不用的snapshots</li><li>snapshot只能保证IO的一致性，并不能保证使用rbd块设备上的系统一致性；</li></ul><h2 id="方案2-RBD-Mirroring"><a href="#方案2-RBD-Mirroring" class="headerlink" title="方案2  RBD Mirroring"></a>方案2  RBD Mirroring</h2><p><strong>原理</strong></p><p>异步备份，Ceph新的rbd mirror功能<br><img src="/images/ceph-02.png" alt="image"></p><p><strong>介绍</strong></p><p>Ceph新的rbd mirror功能支持配置两个Ceph Cluster之间的rbd同步。</p><p>在此方案下，Master Cluster使用性能较高的存储设备，提供给OpenStack的Glance、Cinder（cinder-volume、cinder-backup）和Nova服务使用；而Backup Cluster则使用容量空间大且廉价的存储设备（如SATA盘）来备份Ceph数据。不同的Ceph Cluster集群，可以根据实际需要，选择是否跨物理机房备份。</p><p><strong>优点：</strong></p><ul><li>Ceph新的功能，不需要额外开发</li><li>同步的粒度比较小，为一个块设备的transaction</li><li>保证了Crash consistency</li><li>可配置pool的备份，也可单独指定image备份</li><li>同步备份，不同机房的Ceph集群，底层存储的跨机房容灾</li></ul><h2 id="方案3-ceph备份软件ceph-backup"><a href="#方案3-ceph备份软件ceph-backup" class="headerlink" title="方案3  ceph备份软件ceph-backup"></a>方案3  ceph备份软件ceph-backup</h2><p><strong>介绍</strong></p><p>ceph-backup是一个用来备份ceph RBD块设备的开源软件，提供了两种模式。</p><ul><li>增量：在给定备份时间窗口内基于rbd快照的增量备份</li><li>完全：完整映像导出时不包含快照</li></ul><p><strong>编译安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/teralytics/ceph-backup.git</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> ceph-backup</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> python setup.py install</span></span><br></pre></td></tr></table></figure></p><p>安装过程中会下载一些东西，注意要有网络，需要等待一会<br>准备配置文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/cephbackup/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp ceph-backup.cfg /etc/cephbackup/cephbackup.conf</span></span><br></pre></td></tr></table></figure></p><p>我的配置文件如下，备份rbd存储的zp的镜像，支持多image，images后面用逗号隔开就可以<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/cephbackup/cephbackup.conf </span></span><br><span class="line">[rbd]</span><br><span class="line">window size = 7</span><br><span class="line">window unit = days</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = <span class="literal">yes</span></span><br><span class="line">ceph<span class="built_in"> config </span>= /etc/ceph/ceph.conf</span><br><span class="line">backup mode = full</span><br><span class="line">check mode = <span class="literal">no</span></span><br></pre></td></tr></table></figure></p><p><strong>开始备份</strong></p><p>全量备份配置<br>上面的配置文件已经写好了，直接执行备份命令就可以了<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cephbackup</span></span><br><span class="line"><span class="string">Starting</span> <span class="string">backup</span> <span class="string">for</span> <span class="string">pool</span> <span class="string">rbd</span></span><br><span class="line"><span class="string">Full</span> <span class="string">ceph</span> <span class="string">backup</span></span><br><span class="line"><span class="string">Images</span> <span class="string">to</span> <span class="attr">backup:</span></span><br><span class="line"><span class="string">rbd/zp</span></span><br><span class="line"><span class="string">Backup</span> <span class="attr">folder:</span> <span class="string">/tmp/</span></span><br><span class="line"><span class="attr">Compression:</span> <span class="literal">True</span></span><br><span class="line"><span class="string">Check</span> <span class="attr">mode:</span> <span class="literal">False</span></span><br><span class="line"><span class="string">Taking</span> <span class="string">full</span> <span class="string">backup</span> <span class="string">of</span> <span class="attr">images:</span> <span class="string">zp</span></span><br><span class="line"><span class="string">rbd</span> <span class="string">image</span> <span class="string">'zp'</span><span class="string">:</span></span><br><span class="line"><span class="string">size</span> <span class="number">40960</span> <span class="string">MB</span> <span class="string">in</span> <span class="number">10240</span> <span class="string">objects</span></span><br><span class="line"><span class="string">order</span> <span class="number">22</span> <span class="string">(4096</span> <span class="string">kB</span> <span class="string">objects)</span></span><br><span class="line"><span class="attr">block_name_prefix:</span> <span class="string">rbd_data.25496b8b4567</span></span><br><span class="line"><span class="attr">format:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">features:</span> <span class="string">layering</span></span><br><span class="line"><span class="attr">flags:</span> </span><br><span class="line"><span class="string">Exporting</span> <span class="string">image</span> <span class="string">zp</span> <span class="string">to</span> <span class="string">/tmp/rbd/zp/zp_UTC20170119T092933.full</span></span><br><span class="line"><span class="string">Compress</span> <span class="string">mode</span> <span class="string">activated</span></span><br></pre></td></tr></table></figure></p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">export</span> rbd/zp /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">Exporting image: <span class="number">100</span>% complete...done.</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tar scvfz <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full.tar.gz <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full</span><br><span class="line">tar: Removing leading `<span class="regexp">/' from member names</span></span><br></pre></td></tr></table></figure><p>压缩如果打开了，正好文件也是稀疏文件的话，需要等很久，压缩的效果很好，dd生成的文件可以压缩到很小<br>检查备份生成的文件<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ll /tmp/rbd/zp/zp_UTC20170119T092933.full*</span></span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root<span class="number"> 42949672960 </span>Jan<span class="number"> 19 </span>17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root          <span class="number"> 0 </span>Jan<span class="number"> 19 </span>17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz</span><br></pre></td></tr></table></figure></p><p>全量备份的还原<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span> <span class="regexp">/tmp/</span>rbd<span class="regexp">/zp/</span>zp_UTC20170119T092933.full zpbk</span><br></pre></td></tr></table></figure></p><p>检查数据，没有问题<br>增量备份配置<br>写下增量配置的文件，修改下备份模式的选项<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[rbd]</span><br><span class="line">window size = 7</span><br><span class="line">window unit = day</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = <span class="literal">yes</span></span><br><span class="line">ceph<span class="built_in"> config </span>= /etc/ceph/ceph.conf</span><br><span class="line">backup mode = incremental</span><br><span class="line">check mode = <span class="literal">no</span></span><br></pre></td></tr></table></figure></p><p>执行多次进行增量备份以后是这样的<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ll  /tmp/rbd/zpbk/</span></span><br><span class="line">total 146452</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root<span class="number"> 42949672960 </span>Jan<span class="number"> 19 </span>18:04 zpbk@UTC20170119T100339.full</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root      <span class="number"> 66150 </span>Jan<span class="number"> 19 </span>18:05 zpbk@UTC20170119T100546.diff_from_UTC20170119T100339</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:05 zpbk@UTC20170119T100550.diff_from_UTC20170119T100546</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:06 zpbk@UTC20170119T100606.diff_from_UTC20170119T100550</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root         <span class="number"> 68 </span>Jan<span class="number"> 19 </span>18:06 zpbk@UTC20170119T100638.diff_from_UTC20170119T100606</span><br></pre></td></tr></table></figure></p><p><strong>增量备份的还原</strong></p><p>分成多个步骤进行</p><p>1、进行全量的恢复<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span> config@UTC20161130T170848.full dest_image</span><br></pre></td></tr></table></figure></p><p>2、重新创建基础快照<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rbd snap create dest_image<span class="doctag">@UTC</span>20161130T170848</span></span><br></pre></td></tr></table></figure></p><p>3、还原增量的快照<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rbd <span class="keyword">import</span>-diff config@UTC20161130T170929.diff_from_UTC20161130T170848 dest_image</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综合评估，以上三种方案的优缺点，推荐使用“方案二RBD Mirroring”。</p><p>参考链接<br><a href="http://www.yangguanjun.com/2017/02/22/rbd-data-replication/" target="_blank" rel="noopener">http://www.yangguanjun.com/2017/02/22/rbd-data-replication/</a><br><a href="https://ceph.com/planet/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/" target="_blank" rel="noopener">https://ceph.com/planet/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;所谓，“常在河边走，哪有不湿鞋”。在一个实际的环境中，由于种种原因，可能发生数据被删除的情况。比如，云平台中的数据库、虚拟机、数据卷、镜像或
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
      <category term="Ceph" scheme="http://yoursite.com/categories/OpenStack/Ceph/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>聊聊OpenStack运维架构那些事儿</title>
    <link href="http://yoursite.com/2018/05/09/%E8%81%8A%E8%81%8AOpenStack%E8%BF%90%E7%BB%B4%E6%9E%B6%E6%9E%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/"/>
    <id>http://yoursite.com/2018/05/09/聊聊OpenStack运维架构那些事儿/</id>
    <published>2018-05-09T13:22:43.000Z</published>
    <updated>2018-05-10T14:58:47.155Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>想一想，从事OpenStack杂七杂八的事儿，至今正好三年半了。做过QA测试（手动的、自动的）、CI（gerrit、jenkins、gitlab、harbor）、云产品封装（从系统pxe到openstack代码）、自动化部署开发、运维监控、分布式存储、底层功能调研和实现、开源社区参与、Docker等等。</p><p>一个良好的架构设计和运维保障措施，能为OpenStack云平台的稳定健康运行，产生不可估量的积极影响。</p><p>下面，是笔者从业OpenStack以来，关于OpenStack运维、架构设计、实施的点滴之想。在此，做一个回顾和总结。如有差错，欢迎拍砖。</p><p>OK，咱们言归正传进入话题吧。如果化繁为简，简单的来说，要部署一套生产环境级别的OpenStack云平台，至少会涉及到四个层次的内容，即物理基础设施层、存储层、OpenStack云服务层和用户应用层。如下图所示。</p><p><img src="/images/openstack-ceng.png" alt="image"></p><h2 id="物理基础设施层"><a href="#物理基础设施层" class="headerlink" title="物理基础设施层"></a>物理基础设施层</h2><p>首先，从最底层开始说起，即“物理基础设施层”。一个基本的物理基础设施IT环境，包括了电力设备、空调和防火设备、网络设备（如交换机、路由器、防火墙等）、存储设备和服务器等。由于专业知识的限制，这里，只涉及交换机和服务器方面。一个基本的物理IT环境，如下图所示。</p><p><img src="/images/wangluoceng.png" alt="image"></p><h3 id="交换机设备"><a href="#交换机设备" class="headerlink" title="交换机设备"></a>交换机设备</h3><p>一般地，在OpenStack生产环境上，交换机端口应该做聚合（channel）。也就是将2个或多个物理端口组合在一起成为一条逻辑的链路从而增加交换机和网络节点之间的带宽，将属于这几个端口的带宽合并，给端口提供一个几倍于独立端口的独享的高带宽。Trunk是一种封装技术，它是一条点到点的链路，链路的两端可以都是交换机，也可以是交换机和路由器，还可以是主机和交换机或路由器。</p><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><h4 id="网卡"><a href="#网卡" class="headerlink" title="网卡"></a>网卡</h4><p>OpenStack云平台涉及到的网络有管理网络（用于OpenStack各服务之间通信）、外部网络（提供floating ip）、存储网络（如ceph存储网络）和虚机网络（也称租户网络、业务网络）四种类型。</p><p>对应到每一种网络，服务器都应该做网卡Bond，来提供服务器网络的冗余、高可用和负载均衡的能力，根据实际需求，可以选择模式0或模式1。在网络流量较大的场景下推荐使用bond 0；在可靠性要求较高的场景下推荐使用bond 1。</p><p>二者优劣比较。</p><p><img src="/images/bond.png" alt="image"></p><p>在生产环境中，如果是少于90台OpenStack节点规模的私有云，一般网络类型对应的带宽是（PS：90台只是一个相对值，非绝对值，有洁癖的人请绕过）。</p><ul><li>管理网络：千兆网络</li><li>外部网络：千兆网络</li><li>存储网络：万兆网络</li><li>租户网络：千兆网络</li></ul><p>如果是多于90台OpenStack节点规模的私有云或公有云环境，则推荐尽量都使用万兆网络。</p><h4 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a>硬盘</h4><p>服务器操作系统使用的系统盘，应该用2块硬盘来做RAID 1，以提供系统存储的高可靠性。且推荐使用高性能且成本可控的SAS硬盘，以提高操作系统、MySQL数据库和Docker容器（如果使用kolla部署openstack）的存储性能。</p><h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>OpenStack各计算节点的CPU型号，必须一致，以保证虚拟机的迁移功能正常可用等。</p><h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><p>OpenStack各计算节点的内存大小，应该一致，以保证虚拟机创建管理的均衡调度等。同时，主机的Swap交换分区，应该科学合理的设置，不能使用系统默认创建的。如何设置，请参考此文。<a href="https://xuchao918.github.io/2018/05/07/如何设置OpenStack节点Swap分区/" target="_blank" rel="noopener">如何设置OpenStack节点Swap分区</a>。</p><p>数据中心中少部分机器用于做控制节点，大部分机器都是需要运行虚拟化软件的，虚拟化平台上有大量的vm，而宿主机本身的系统也会跑一些服务，那么这就势必会造成vm之间资源抢占，vm与宿主机系统之间的资源抢占，我们需要通过设定游戏规则，让他们在各自的界限内高效运行，减少冲突抢占。</p><p>我们可以让宿主机运行操作系统时候，更多的选择指定的几个核，这样就不会过多抢占虚拟化中虚机的vcpu调度，通过修改内核启动参数我们可以做到:</p><p>修改 /etc/default/grub文件，让系统只使用前三个核 隔离其余核。<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX_DEFAULT="isolcpus=<span class="number">4,5,6,7</span>,<span class="number">8,9,10,11</span>,<span class="number">12,13,14,15</span>,<span class="number">16,17,18,19</span>,<span class="number">20,21,22,23</span>,<span class="number">24,25,26,27</span>,<span class="number">28,29,30,31</span>"</span><br></pre></td></tr></table></figure></p><p>更新内核参数<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> update-grub</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure></p><p>内存配置方面，网易私有云的实践是关闭 KVM 内存共享，打开透明大页：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="number">0</span> &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/ksm</span><span class="regexp">/pages_shared</span></span><br><span class="line"><span class="regexp">echo 0 &gt; /sys</span><span class="regexp">/kernel/mm</span><span class="regexp">/ksm/pages</span>_sharing</span><br><span class="line">echo always &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/enabled</span><br><span class="line">echo never &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/defrag</span><br><span class="line">echo <span class="number">0</span> &gt; <span class="regexp">/sys/kernel</span><span class="regexp">/mm/transparent</span>_hugepage/khugepaged/defrag</span><br></pre></td></tr></table></figure></p><p>据说，经过 SPEC CPU2006 测试，这些配置对云主机 CPU 性能大概有7%左右的提升。</p><h2 id="OpenStack云平台层"><a href="#OpenStack云平台层" class="headerlink" title="OpenStack云平台层"></a>OpenStack云平台层</h2><h3 id="云平台高可用（HA）"><a href="#云平台高可用（HA）" class="headerlink" title="云平台高可用（HA）"></a>云平台高可用（HA）</h3><h4 id="高可用（HA）介绍"><a href="#高可用（HA）介绍" class="headerlink" title="高可用（HA）介绍"></a>高可用（HA）介绍</h4><p>高可用性是指提供在本地系统单个组件故障情况下，能继续访问应用的能力，无论这个故障是业务流程、物理设施、IT软/硬件的故障。最好的可用性， 就是你的一台机器宕机了，但是使用你的服务的用户完全感觉不到。你的机器宕机了，在该机器上运行的服务肯定得做故障切换（failover），切换有两个维度的成本：RTO （Recovery Time Objective）和 RPO（Recovery Point Objective）。RTO 是服务恢复的时间，最佳的情况是 0，这意味着服务立即恢复；最坏是无穷大意味着服务永远恢复不了；RPO 是切换时向前恢复的数据的时间长度，0 意味着使用同步的数据，大于 0 意味着有数据丢失，比如 ” RPO = 1 天“ 意味着恢复时使用一天前的数据，那么一天之内的数据就丢失了。因此，恢复的最佳结果是 RTO = RPO = 0，但是这个太理想，或者要实现的话成本太高。</p><p>对 HA 来说，往往使用分布式存储，这样的话，RPO =0 ；同时使用 Active/Active （双活集群） HA 模式来使得 RTO 几乎为0，如果使用 Active/Passive  HA模式的话，则需要将 RTO 减少到最小限度。HA 的计算公式是[ 1 - (宕机时间)/（宕机时间 + 运行时间）]，我们常常用几个 9 表示可用性：</p><ul><li>2 个9：99% = 1% <em> 365 = 3.65 </em> 24 小时/年 = 87.6 小时/年的宕机时间</li><li>4 个9: 99.99% = 0.01% <em> 365 </em> 24 * 60 = 52.56 分钟/年</li><li>5 个9：99.999% = 0.001% * 365 = 5.265 分钟/年的宕机时间，也就意味着每次停机时间在一到两分钟。</li><li>11 个 9：几年宕机几分钟。</li></ul><p><strong>服务的分类</strong></p><p>HA 将服务分为两类：</p><ul><li><p>有状态服务：后续对服务的请求依赖于之前对服务的请求。OpenStack中有状态的服务包括MySQL数据库和AMQP消息队列。对于有状态类服务的HA，如neutron-l3-agent、neutron-metadata-agent、nova-compute、cinder-volume等服务，最简便的方法就是多节点部署。比如某一节点上的nova-compute服务挂了，也并不会影响到整个云平台不能创建虚拟机，或者所在节点的虚拟机无法使用（比如ssh等）。 </p></li><li><p>无状态服务：对服务的请求之间没有依赖关系，是完全独立的，基于冗余实例和负载均衡实现HA。OpenStack中无状态的服务包括nova-api、nova-conductor、glance-api、keystone-api、neutron-api、nova-scheduler等。由于API服务，属于无状态类服务，天然支持Active/Active HA模式。因此，一般使用 keepalived +HAProxy方案来做。</p></li></ul><p><strong>HA 的种类</strong></p><p>HA 需要使用冗余的服务器组成集群来运行负载，包括应用和服务。这种冗余性也可以将 HA 分为两类：</p><ul><li><p>Active/Passive HA：集群只包括两个节点简称主备。在这种配置下，系统采用主和备用机器来提供服务，系统只在主设备上提供服务。在主设备故障时，备设备上的服务被启动来替代主设备提供的服务。典型地，可以采用 CRM 软件比如 Pacemaker 来控制主备设备之间的切换，并提供一个虚机 IP 来提供服务。</p></li><li><p>Active/Active HA：集群只包括两个节点时简称双活，包括多节点时成为多主（Multi-master）。在这种配置下，系统在集群内所有服务器上运行同样的负载。以数据库为例，对一个实例的更新，会被同步到所有实例上。这种配置下往往采用负载均衡软件比如 HAProxy 来提供服务的虚拟 IP。</p></li></ul><h4 id="OpenStack云环境高可用（HA）"><a href="#OpenStack云环境高可用（HA）" class="headerlink" title="OpenStack云环境高可用（HA）"></a>OpenStack云环境高可用（HA）</h4><p>云环境是一个广泛的系统，包括了基础设施层、OpenStack云平台服务层、虚拟机和最终用户应用层。</p><p><strong>云环境的 HA 包括：</strong></p><ul><li>用户应用的 HA</li><li>虚拟机的 HA</li><li>OpenStack云平台服务的 HA</li><li>基础设施层的HA：电力、空调和防火设施、网络设备（如交换机、路由器）、服务器设备和存储设备等</li></ul><p>仅就OpenStack云平台服务（如nova-api、nova-scheduler、nova-compute等）而言，少则几十，多则上百个。如果某一个服务挂了，则对应的功能便不能正常使用。因此，如何保障整体云环境的HA高可用，便成为了架构设计和运维的重中之重。</p><p>OpenStack HA高可用架构，如下图所示。</p><p><img src="/images/vmha.png" alt="image"></p><p><strong>OpenStack高可用内容</strong></p><p>如果，从部署层面来划分，OpenStack高可用的内容包括：</p><ul><li>控制节点（Rabbitmq、mariadb、Keystone、nova-api等）</li><li>网络节点（neutron_dhcp_agent、neutron_l3_agent、neutron_openvswitch_agent等）</li><li>计算节点（Nova-Compute、neutron_openvswitch_agent、虚拟机等）</li><li>存储节点（cinder-volume、swift等）</li></ul><h5 id="控制节点HA"><a href="#控制节点HA" class="headerlink" title="控制节点HA"></a>控制节点HA</h5><p>在生产环境中，建议至少部署三台控制节点，其余可做计算节点、网络节点或存储节点。采用Haproxy + KeepAlived方式，代理数据库服务和OpenStack服务，对外暴露VIP提供API访问。</p><p><strong>MySQL数据库HA</strong></p><p>mysql 的HA 方案有很多，这里只讨论openstack 官方推荐的mariadb galara 集群。Galera Cluster 是一套在innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到各个节点上去。特点如下：<br>1）同步复制，（&gt;=3）奇数个节点<br>2）Active-active的多主拓扑结构<br>3）集群任意节点可以读和写<br>4）自动身份控制,失败节点自动脱离集群<br>5）自动节点接入<br>6）真正的基于”行”级别和ID检查的并行复制<br>7）无单点故障,易扩展</p><p>采用MariaDB + Galera方案部署至少三个节点（最好节点数量为奇数），外部访问通过Haproxy的active + backend方式代理。平时主库为A，当A出现故障，则切换到B或C节点。如下图所示。</p><p><img src="/images/mysqlha.jpg" alt="image"></p><p><strong>RabbitMQ 消息队列HA</strong></p><p>RabbitMQ采用原生Cluster集群方案，所有节点同步镜像队列。三台物理机，其中2个Mem节点主要提供服务，1个Disk节点用于持久化消息，客户端根据需求分别配置主从策略。</p><p><strong>OpenStack API服务HA</strong></p><p>OpenStack控制节点上运行的基本上是API 无状态类服务，如nova-api、neutron-server、glance-registry、nova-novncproxy、keystone等。因此，可以由 HAProxy 提供负载均衡，将请求按照一定的算法转到某个节点上的 API 服务，并由KeepAlived提供 VIP。</p><h5 id="网络节点HA"><a href="#网络节点HA" class="headerlink" title="网络节点HA"></a>网络节点HA</h5><p>网络节点上运行的Neutron服务包括很多的组件，比如 L3 Agent，openvswitch Agent，LBaas，VPNaas，FWaas，Metadata Agent 等，其中部分组件提供了原生的HA 支持。</p><ul><li>Openvswitch Agent HA： openvswitch agent 只在所在的网络或者计算节点上提供服务，因此它是不需要HA的</li><li>L3 Agent HA：成熟主流的有VRRP 和DVR两种方案</li><li>DHCP Agent HA：在多个网络节点上部署DHCP Agent，实现HA </li><li>LBaas Agent HA：Pacemaker + 共享存储（放置 /var/lib/neutron/lbaas/ 目录） 的方式来部署 A/P 方式的 LBaas Agent HA</li></ul><h5 id="存储节点HA"><a href="#存储节点HA" class="headerlink" title="存储节点HA"></a>存储节点HA</h5><p>存储节点的HA，主要是针对cinder-volume、cinder-backup服务做HA，最简便的方法就是部署多个存储节点，某一节点上的服务挂了，不至于影响到全局。</p><h5 id="计算节点和虚拟机-HA"><a href="#计算节点和虚拟机-HA" class="headerlink" title="计算节点和虚拟机 HA"></a>计算节点和虚拟机 HA</h5><p>计算节点和虚拟机的HA，社区从2016年9月开始一直致力于一个虚拟机HA的统一方案，<br>详细参考：High Availability for Virtual Machines。目前还处于开发阶段。业界目前使用的方案大致有以下几种：</p><p>1)检查compute计算节点和nova 服务运行状态，对于有问题的节点或服务进行自动修复。该方案的实现是：</p><p>①Pacemaker 监控每个计算节点上的 pacemaker_remote 的连接，来检查该节点是否处于活动状态。发现它不可以连接的话，启动恢复（recovery）过程。</p><ul><li>运行 ‘nova service-disable’</li><li>将该节点关机</li><li>等待 nova 发现该节点失效了</li><li>将该节点开机</li><li>如果节点启动成功，执行 ‘nova service-enable’</li><li>如果节点启动失败，则执行 ‘nova evacuate’ 把该节点上的虚机移到别的可用计算节点上。</li></ul><p>②Pacemaker 监控每个服务的状态，如果状态失效，该服务会被重启，重启失败则触发防护行为（fencing action），即停掉该服务。</p><p>2)分布式健康检查，参考分布式健康检查：实现OpenStack计算节点高可用</p><p>如果使用第一种方案，实现计算节点和虚拟机HA，要做的事情基本有三件，即。</p><p><strong>监控</strong></p><p>监控主要做两个事情，一个是监控计算节点的硬件和软件故障。第二个是触发故障的处理事件，也就是隔离和恢复。</p><p>OpenStack 计算节点高可用，可以用pacemaker和pacemaker_remote来做。使用pacemaker_remote后，我们可以把所有的计算节点都加入到这个集群中，计算节点只需要安装pacemaker_remote即可。pacemaker集群会监控计算节点上的pacemaker_remote是否 “活着”，你可以定义什么是“活着”。在计算节点上可以监控nova-compute、neutron-ovs-agent、libvirt等进程，从而确定计算节点是否活着，甚至我们还可以在该计算节点上启动虚拟机来确定计算节点是否活着。如果监控到某个pacemaker_remote有问题，可以马上触发之后的隔离和恢复事件。</p><p><strong>隔离</strong></p><p>隔离最主要的任务是将不能正常工作的计算节点从OpenStack集群环境中移除，nova-scheduler就不会在把create_instance的message发给该计算节点。</p><p>Pacemaker 已经集成了fence这个功能，因此我们可以使用fence_ipmilan来关闭计算节点。Pacemaker集群中fence_compute 会一直监控这个计算节点是否down了，因为nova只能在计算节点down了之后才可以执行host-evacuate来迁移虚拟机，期间等待的时间稍长。这里有个更好的办法， 就是调用nova service-force-down 命令，直接把计算节点标记为down，方便更快的迁移虚拟机。</p><p><strong>恢复</strong></p><p>恢复就是将状态为down的计算节点上的虚拟机迁移到其他计算节点上。Pacemaker集群会调用host-evacuate API将所有虚拟机迁移。host-evacuate最后是使用rebuild来迁移虚拟机，每个虚拟机都会通过scheduler调度在不同的计算节点上启动。</p><p><strong>虚拟机操作系统故障恢复</strong></p><p>OpenStack 中的 libvirt/KVM 驱动已经能够很好地自动化处理这类问题。具体地，你可以在flavor的 extra_specs 或者镜像的属性中加上 hw:watchdog_action ，这样 一个 watchdog 设备会配置到虚拟机上。如果 hw:watchdog_action 设置为 reset，那么虚拟机的操作系统一旦奔溃，watchdog 会将虚拟机自动重启。</p><h3 id="OpenStack计算资源限制"><a href="#OpenStack计算资源限制" class="headerlink" title="OpenStack计算资源限制"></a>OpenStack计算资源限制</h3><p><strong>设置内存</strong></p><p>#内存分配超售比例，默认是 1.5 倍，生产环境不建议开启超售<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ram_allocation_ratio</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>#内存预留量，这部分内存不能被虚拟机使用，以便保证系统的正常运行<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">reserved_host_memory_mb</span> = <span class="number">10240</span>      //如预留<span class="number">10</span>GB</span><br></pre></td></tr></table></figure></p><p><strong>设置CPU</strong></p><p>在虚拟化资源使用上，我们可以通过nova来控制，OpenStack提供了一些配置，我们可以很容易的做到，修改文件nova.conf。</p><p>#虚拟机 vCPU 的绑定范围，可以防止虚拟机争抢宿主机进程的 CPU 资源，建议值是预留前几个物理 CPU<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">vcpu_pin_set</span> = <span class="number">4</span>-<span class="number">31</span></span><br></pre></td></tr></table></figure></p><p>#物理 CPU 超售比例，默认是 16 倍，超线程也算作一个物理 CPU<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">cpu_allocation_ratio</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure></p><h3 id="使用多Region和AZ"><a href="#使用多Region和AZ" class="headerlink" title="使用多Region和AZ"></a>使用多Region和AZ</h3><p>如果，OpenStack云平台需要跨机房或地区部署，可以使用多Region和 Availability Zone（以下简称AZ）的方案。这样，每个机房之间在地理位置上自然隔离，这对上层的应用来说是天然的容灾方法。</p><p><strong>多区域（Region）部署</strong></p><p>OpenStack支持依据地理位置划分为不同的Region，所有的Regino除了共享Keystone、Horizon服务外，每个Region都是一个完整的OpenStack环境，从整体上看，多个区域之间的部署相对独立，但可通过内网专线实现互通（如BGP-EVPN）。其架构如下图所示。</p><p><img src="/images/region-az.png" alt="image"></p><p>部署时只需要部署一套公共的Keystone和Horizon服务，其它服务按照单Region方式部署即可，通过Endpoint指定Region。用户在请求任何资源时必须指定具体的区域。采用这种方式能够把分布在不同的区域的资源统一管理起来，各个区域之间可以采取不同的部署架构甚至不同的版本。其优点如下：</p><ul><li>部署简单，每个区域部署几乎不需要额外的配置，并且区域很容易实现横向扩展。</li><li>故障域隔离，各个区域之间互不影响。</li><li>灵活自由，各个区域可以使用不同的架构、存储、网络。</li></ul><p>但该方案也存在明显的不足：</p><ul><li>各个区域之间完全隔离，彼此之间不能共享资源。比如在Region A创建的Volume，不能挂载到Region B的虚拟机中。在Region A的资源，也不能分配到Region B中，可能出现Region负载不均衡问题。</li><li>各个区域之间完全独立，不支持跨区域迁移，其中一个区域集群发生故障，虚拟机不能疏散到另一个区域集群中。</li><li>Keystone成为最主要的性能瓶颈，必须保证Keystone的可用性，否则将影响所有区域的服务。该问题可以通过部署多Keystone节点解决。</li></ul><p>OpenStack多Region方案通过把一个大的集群划分为多个小集群统一管理起来，从而实现了大规模物理资源的统一管理，它特别适合跨数据中心并且分布在不同区域的场景，此时根据区域位置划分Region，比如北京和上海。而对于用户来说，还有以下好处:</p><ul><li>用户能根据自己的位置选择离自己最近的区域，从而减少网络延迟,加快访问速度。</li><li>用户可以选择在不同的Region间实现异地容灾。当其中一个Region发生重大故障时，能够快速把业务迁移到另一个Region中。</li></ul><p><strong>多Availability Zone部署</strong></p><p>如果，只是想在一个机房中部署OpenStack云环境。则只需要使用AZ方案即可。每个AZ有自己独立供电的机架，以及OpenStack计算节点。</p><p>Availability Zone</p><p>一个Region可以被细分为一个或多个物理隔离或逻辑隔离的availability zones（AZ）。启动虚拟机时，可以指定特定的AZ甚至特定AZ中的某一个节点来启动该虚拟机。AZ可以简单理解为一组节点的集合，这组节点具有独立的电力供应设备，比如一个个独立供电的机房，或一个个独立供电的机架都可以被划分成AZ。</p><p>然后将应用的多个虚拟机分别部署在Region的多个AZ上，提高虚拟机的容灾性和可用性。由于，AZ是物理隔离的，所以一个AZ挂了不会影响到其他的AZ。同时，还可以将挂了的AZ上的虚拟机，迁移到其他正常可用的AZ上，类似于异地双活。</p><p>Host Aggregate</p><p>除了AZ，计算节点也可以在逻辑上划分为主机聚合（Host Aggregates简称HA）。主机聚合使用元数据去标记计算节点组。一个计算节点可以同时属于一个主机聚合以及AZ而不会有冲突，它也可以属于多个主机聚合。</p><p>主机聚合的节点具有共同的属性，比如：cpu是特定类型的一组节点，disks是ssd的一组节点，os是linux或windows的一组节点等等。需要注意的是，Host Aggregates是用户不可见的概念，主要用来给nova-scheduler通过某一属性来进行instance的调度，比如讲数据库服务的 instances都调度到具有ssd属性的Host Aggregate中，又或者让某个flavor或某个image的instance调度到同一个Host Aggregates中。</p><p>简单的来说，Region、Availability Zone和Host Aggregate这三者是从大范围到小范围的关系，即前者包含了后者。一个地理区域Region包含多个可用区AZ (availability zone)，同一个AZ中的计算节点又可以根据某种规则逻辑上的组合成一个组。例如在北京有一个Region，成都有一个Region，做容灾之用。同时，在北京Region下，有2个AZ可用区（如酒仙桥机房和石景山机房），每个AZ都有自己独立的网络和供电设备，以及OpenStack计算节点等，如果用户是在北京，那么用户在部署VM的时候选择北京，可以提高用户的访问速度和较好的SLA（服务等级协议）。</p><h3 id="备份你的数据"><a href="#备份你的数据" class="headerlink" title="备份你的数据"></a>备份你的数据</h3><p>如果因为某些原因，没有跨物理机房或地区的Region和AZ。那么OpenStack云平台相关的数据备份，则是必须要做的。比如MySQL数据库等，可以根据实际需求，每隔几小时进行一次备份。而备份的数据，建议存放到其他机器上。</p><h3 id="使用合适的Docker存储"><a href="#使用合适的Docker存储" class="headerlink" title="使用合适的Docker存储"></a>使用合适的Docker存储</h3><p>如果，OpenStack云平台是用kolla容器化部署和管理的。那么选择一个正确、合适的Docker存储，关乎你的平台稳定和性能。</p><p>Docker 使用存储驱动来管理镜像每层内容及可读写的容器层，存储驱动有 devicemapper、aufs、overlay、overlay2、btrfs、zfs 等，不同的存储驱动实现方式有差异，镜像组织形式可能也稍有不同，但都采用栈式存储，并采用 Copy-on-Write(CoW) 策略。且存储驱动采用热插拔架构，可动态调整。那么，存储驱动那么多，该如何选择合适的呢？大致可从以下几方面考虑：</p><ul><li>若内核支持多种存储驱动，且没有显式配置，Docker 会根据它内部设置的优先级来选择。优先级为 aufs &gt; btrfs/zfs &gt; overlay2 &gt; overlay &gt; devicemapper。若使用 devicemapper 的话，在生产环境，一定要选择 direct-lvm, loopback-lvm 性能非常差。</li><li>选择会受限于 Docker 版本、操作系统、系统版本等。例如，aufs 只能用于 Ubuntu 或 Debian 系统，btrfs 只能用于 SLES （SUSE Linux Enterprise Server, 仅 Docker EE 支持）。</li><li>有些存储驱动依赖于后端的文件系统。例如，btrfs 只能运行于后端文件系统 btrfs 上。</li><li>不同的存储驱动在不同的应用场景下性能不同。例如，aufs、overlay、overlay2 操作在文件级别，内存使用相对更高效，但大文件读写时，容器层会变得很大；devicemapper、btrfs、zfs 操作在块级别，适合工作在写负载高的场景；容器层数多，且写小文件频繁时，overlay2 效率比 overlay更高；btrfs、zfs 更耗内存。</li></ul><p>Docker 容器其实是在镜像的最上层加了一层读写层，通常也称为容器层。在运行中的容器里做的所有改动，如写新文件、修改已有文件、删除文件等操作其实都写到了容器层。存储驱动决定了镜像及容器在文件系统中的存储方式及组织形式。</p><p>在我们的生产环境中，使用的是Centos 7.4系统及其4.15内核版本+Docker 1.13.1版本。因此使用的是overlay2存储。下面对overlay2作一些简单介绍。</p><p><strong>Overlay介绍</strong></p><p>OverlayFS 是一种类似 AUFS 的联合文件系统，但实现更简单，性能更优。OverlayFS 严格来说是 Linux 内核的一种文件系统，对应的 Docker 存储驱动为 overlay 或者 overlay2，overlay2 需 Linux 内核 4.0 及以上，overlay 需内核 3.18 及以上。且目前仅 Docker 社区版支持。条件许可的话，尽量使用 overlay2，与 overlay 相比，它的 inode 利用率更高。</p><p>和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的容器层和镜像层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示：</p><p><img src="/images/overlay2.png" alt="image"></p><p><strong>分析</strong></p><ul><li>从kernel 3.18进入主流Linux内核。设计简单，速度快，比AUFS和Device mapper速度快。在某些情况下，也比Btrfs速度快。是Docker存储方式选择的未来。因为OverlayFS只有两层，不是多层，所以OverlayFS “copy-up”操作快于AUFS。以此可以减少操作延时。</li><li>OverlayFS支持页缓存共享，多个容器访问同一个文件能共享一个页缓存，以此提高内存使用率。</li><li>OverlayFS消耗inode，随着镜像和容器增加，inode会遇到瓶颈。Overlay2能解决这个问题。在Overlay下，为了解决inode问题，可以考虑将/var/lib/docker挂在单独的文件系统上，或者增加系统inode设置。</li></ul><h2 id="使用分布式存储"><a href="#使用分布式存储" class="headerlink" title="使用分布式存储"></a>使用分布式存储</h2><p>如果OpenStack云平台使用开源的分布式存储系统，如Ceph、GlusterFS等。如何保证存储系统的冗余容灾性、可靠性、安全性和性能，便显得尤为重要。这里，以Ceph开源分布式存储为例进行讲解。</p><h3 id="Mon节点和OSD节点部署"><a href="#Mon节点和OSD节点部署" class="headerlink" title="Mon节点和OSD节点部署"></a>Mon节点和OSD节点部署</h3><p>一般地，在生产环境中，至少需要部署有3个Ceph Mon节点（数量最好为奇数）以及多个OSD节点。</p><h3 id="开启CephX认证"><a href="#开启CephX认证" class="headerlink" title="开启CephX认证"></a>开启CephX认证</h3><p>同时，开启CephX认证方式，以提高数据存储的安全性，防范被攻击。如下所示。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/ceph/ceph.conf </span></span><br><span class="line">[global]</span><br><span class="line">fsid = e10d7336-23e8-4dac-a07a-d012d9208ae1</span><br><span class="line">mon_initial_members = computer1, computer2, computer3</span><br><span class="line">mon_host = 172.17.51.54,172.17.51.55,172.17.51.56</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">………</span><br></pre></td></tr></table></figure></p><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>如果Ceph节点少于90台，建议Ceph公共网络（即Public Network）使用千兆网络，集群网络（即Cluster Network）使用万兆网络。如果Ceph节点多于90台，且业务负载较高，则尽量都使用万兆网络，在重要的环境上，Ceph公共网络和集群网络，都应该单独分开。需要注意的是，Ceph存储节点使用的网卡，必须要做网卡Bond，防止网卡因故障而导致网络中断。</p><h3 id="使用Cache-Tier"><a href="#使用Cache-Tier" class="headerlink" title="使用Cache Tier"></a>使用Cache Tier</h3><p>在一个云存储环境中，出于成本的考虑，基本会少量使用SSD硬盘，大量使用SATA硬盘。在OpenStack集成Ceph的云环境中，如何使用SSD和SATA硬盘。一般有两种使用方法。</p><p>第一种：分别创建独立的SSD和SATA存储资源集群。然后，Cinder块存储服务对接这两套Ceph后端存储，这样云平台便可以同时创建和使用SSD介质和SATA介质的云硬盘。</p><p>第二种：使用SSD硬盘创建容量相对较小但性能高的缓存池（Cache tier），SATA硬盘创建容量大的但性能较低的存储池（Storage tier）。</p><p>以第二种方式为例进行讲解。当客户端访问操作数据时，会优先读写cache tier数据(当然要根据cache mode来决定)，如果数据在storage tier 则会提升到cache tier中，在cache tier中会有请求命中算法、缓存刷写算法、缓存淘汰算法等，将热数据提升到cache tier中，将冷数据下放到storage tier中。</p><p>缓存层代理自动处理缓存层和后端存储之间的数据迁移。在使用过程中，我们可以根据自己的需要，来配置迁移规则，主要有两种场景：</p><ul><li>回写模式： 管理员把缓存层配置为 writeback 模式时， Ceph 客户端们会把数据写入缓存层、并收到缓存层发来的 ACK ；写入缓存层的数据会被迁移到存储层、然后从缓存层刷掉。直观地看，缓存层位于后端存储层的“前面”，当 Ceph 客户端要读取的数据位于存储层时，缓存层代理会把这些数据迁移到缓存层，然后再发往 Ceph 客户端。从此， Ceph 客户端将与缓存层进行 I/O 操作，直到数据不再被读写。此模式对于易变数据来说较理想（如照片/视频编辑、事务数据等）。</li><li>只读模式： 管理员把缓存层配置为 readonly 模式时， Ceph 直接把数据写入后端。读取时， Ceph 把相应对象从后端复制到缓存层，根据已定义策略、脏对象会被缓存层踢出。此模式适合不变数据（如社交网络上展示的图片/视频、 DNA 数据、 X-Ray 照片等），因为从缓存层读出的数据可能包含过期数据，即一致性较差。对易变数据不要用 readonly 模式。</li></ul><h3 id="独立使用Pool"><a href="#独立使用Pool" class="headerlink" title="独立使用Pool"></a>独立使用Pool</h3><p>Ceph可以统一OpenStack Cinder块存储服务（cinder-volume、cinder-backup）、Nova计算服务和Glance镜像服务的后端存储。在生产环境上，建议单独创建4个存储资源池（Pool）以分别对应OpenStack的4种服务存储。同时，每个Pool的副本数建议设置为3份，如下表所示。</p><table><thead><tr><th>Openstack服务</th><th style="text-align:center">Ceph存储池</th><th style="text-align:right">认证用户 </th></tr></thead><tbody><tr><td>Cinder-volumes</td><td style="text-align:center">volumes</td><td style="text-align:right">cinder </td></tr><tr><td>Cinder-backups</td><td style="text-align:center">backups</td><td style="text-align:right">cinder </td></tr><tr><td>Nova</td><td style="text-align:center">vms</td><td style="text-align:right">cinder</td></tr><tr><td>Glance</td><td style="text-align:center">images</td><td style="text-align:right">cinder、glance</td></tr></tbody></table><p>最后，Ceph分布式存储部署架构，如下图所示。</p><p><img src="/images/ceph.png" alt="image"></p><h2 id="用户应用层"><a href="#用户应用层" class="headerlink" title="用户应用层"></a>用户应用层</h2><p>在相当多的业务中，都会涉及到服务高可用。而一般的高可用的实现都是通过VIP(Vitrual IP)实现。VIP不像IP一样，对应一个实际的网络接口（网卡），它是虚拟出来的IP地址，所以利用其特性可以实现服务的容错和迁移工作。</p><p>在常见节点中VIP配置非常简单，没有多大的限制。但OpenStack实例中，一个IP对应一个Port设备。并且Neutron 有“Allowed address pairs”限制，该功能要求 Port 的MAC/IP 相互对应，那么该IP才能连通。对Port设备的进行操作可以实现下面几个功能：</p><ul><li>一个Port设备添加多组Allowed address Pairs，允许多个IP通过该Port连通。</li><li>一个IP对应多组MAC地址。</li><li>一个MAC地址对应多个IP</li></ul><p>另外在OpenStack创建的实例中建立VIP并使其能正常工作可以使用下面方法：</p><ul><li>创建VIP的Port设备(防止该VIP被再次分配)</li><li>更新Port设备的Allowed address pairs</li></ul><p>第一步，创建Port设备<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#source</span> admin-openrc<span class="selector-class">.sh</span>   <span class="comment">//导入租户环境变量</span></span><br><span class="line"><span class="selector-id">#openstack</span> network list    <span class="comment">//查看现有网络，从中选择创建port设备的网络</span></span><br><span class="line"><span class="selector-id">#openstack</span> subnet list     <span class="comment">//查看网络下存在子网，从中选择port设备所处子网</span></span><br><span class="line"><span class="selector-id">#openstack</span> port create --network NetWork_Name --fixed-ip subnet=SubNet_Name,\</span><br><span class="line">ip-address=IP Port_Name</span><br><span class="line"><span class="selector-id">#openstack</span> port show Port_Name</span><br></pre></td></tr></table></figure></p><p>此时Port设备已经创建，但该Port设备与需要建立VIP的实例没有任何关系，在该实例上创建VIP也是不能工作的。原因在于下面<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#neutron port-list |grep Instance-IP        <span class="comment">//找到需要配置VIP的实例的Port ID</span></span></span><br></pre></td></tr></table></figure></p><p>查看该Port的详细信息<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-show <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985</span><br></pre></td></tr></table></figure></p><p>此时的allowed_address_pairs为空，就算在该实例中创建VIP，其MAC/IP也是不对应，不能工作的。那么就要进行第二步，即更新Port的allowed_address_pairs信息<br><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-update Port-ID --allowed_address_pair list-<span class="keyword">true</span> <span class="class"><span class="keyword">type</span></span>=dict ip_address=IP</span><br></pre></td></tr></table></figure></p><p>例如<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-update <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985 \</span><br><span class="line">--allowed_address_pairs <span class="type">list</span>=true type=dict ip_address=<span class="number">172.24</span><span class="number">.1</span><span class="number">.202</span></span><br></pre></td></tr></table></figure></p><p>现在再来查看实例Port信息<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#neutron port-show <span class="number">17</span>b580e8<span class="number">-1733</span><span class="number">-4e2</span>e-b248-cde4863f4985</span><br></pre></td></tr></table></figure></p><p>此时在虚拟机中创建VIP，就能够正常工作了。</p><h2 id="运维平台建设"><a href="#运维平台建设" class="headerlink" title="运维平台建设"></a>运维平台建设</h2><p>监控是整个运维乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供详实的数据用于追查定位问题。目前业界有很多不错的开源产品可供选择。选择一些开源的监控系统，是一个省时省力，效率最高的方案。</p><p>使用Kolla容器化部署和管理OpenStack云平台，已成为主流趋势。这里，我们以容器化部署和管理OpenStack云平台为背景，聊聊云平台相关的运维平台建设。</p><h3 id="监控目标"><a href="#监控目标" class="headerlink" title="监控目标"></a>监控目标</h3><p>我们先来了解什么是监控、监控的重要性以及监控的目标，当然每个人所在的行业不同、公司不同、业务不同、岗位不同，对监控的理解也不同，但是我们需要注意，监控是需要站在公司的业务角度去考虑，而不是针对某个监控技术的使用。</p><p><strong>监控的目标，包括：</strong></p><p>1）对系统不间断实时监控：实际上是对系统不间断的实时监控(这就是监控)；<br>2）实时反馈系统当前状态：我们监控某个硬件、或者某个系统，都是需要能实时看到当前系统的状态，是正常、异常、或者故障；<br>3）保证服务可靠性安全性：我们监控的目的就是要保证系统、服务、业务正常运行；<br>4）保证业务持续稳定运行：如果我们的监控做得很完善，即使出现故障，能第一时间接收到故障报警，在第一时间处理解决，从而保证业务持续性的稳定运行；</p><p><strong>监控体系分层</strong></p><p>监控有赖于运维各专业条线协同完善，通过将监控体系进行分层、分类，各专业条线再去有重点的丰富监控指标。监控的对象，主要有基础设施硬件类和应用软件类等，如下图所示：</p><p><img src="/images/monitor.png" alt="image"></p><ul><li>硬件设施层：交换机、路由器、负载均衡设备、防火墙、服务器（硬盘、CPU、内存和网卡）等。</li><li>云平台层：日志、数据库、消息队列、操作系统、OpenStack服务、Ceph存储、Docker容器、系统和应用负载等。</li><li>应用层：虚拟机、数据卷、虚拟网卡等。</li></ul><h3 id="监控手段"><a href="#监控手段" class="headerlink" title="监控手段"></a>监控手段</h3><p>通常情况下，随着系统的运行，操作系统会产生系统日志，应用程序会产生应用程序的访问日志、错误日志、运行日志、网络日志，我们可以使用 EFK 来进行日志监控。对于日志监控来说，最常见的需求就是收集、存储、查询、展示。</p><p>除了对日志进行监控外，我们还需要对系统和应用的运行状况进行实时监控。不同的监控目标，有不同的监控手段。OpenStack云资源的监控，如虚拟机、镜像、数据卷、虚拟网卡等，天然的可以由OpenStack自带的Ceilometer+Gnocchi+Aodh等服务来做（PS：ceilometer可以将采集数据交给gnocchi做数据聚合，最后用grafana来出图）。</p><p>如果，OpenStack云平台是基于Kolla容器化部署和运行管理的。那么诸如Docker容器、操作系统负载、存储空间等，又该使用什么来运维监控并告警呢。自然，TPG栈便呼之欲出了（不要问我为啥不用Zabbix）。</p><p>什么是TPIG栈。即由Telegraf + Influxdb + Grafana + Prometheus组合成的一套运维监控工具集合。它们之间的关系是。<br>Prometheus/Telegraf(收集数据) —-&gt; Influxdb(保存数据) —-&gt; Grafana(显示数据)</p><p>说明：<br>Prometheus和Telegraf不是必须同时部署使用的，你可以根据自己的需要，选择二者都部署，也可以二者选其一。</p><p>如下几种开源工具或方案，Kolla社区都是默认支持的。最重要的是，如何去使用、完善它们。</p><ul><li>日志收集和分析处理的开源方案有EFK栈：fluentd/filebeat + elasticsearch +kibana</li><li>性能采集和分析处理的开源方案有TPIG栈：telegraf + influxdb + grafana + Prometheus</li></ul><p><strong>监控方法</strong></p><p>了解监控对象：我们要监控的对象你是否了解呢？比如硬盘的IOPS？<br>对象性能指标：我们要监控这个东西的什么属性？比如 CPU 的使用率、负载、用户态、内核态、上下文切换。<br>报警阈值定义：怎么样才算是故障，要报警呢？比如 CPU 的负载到底多少算高，用户态、内核态分别跑多少算高？<br>故障处理流程：收到了故障报警，我们怎么处理呢？有什么更高效的处理流程吗？</p><p><strong>监控流程</strong></p><ul><li>数据采集：通过telegraf/Prometheus等对系统和应用进行数据采集；</li><li>数据存储：监控数据存储在MySQL、influxdb上，也可以存储在其他数据库中；</li><li>数据分析：当我们事后需要分析故障时，EFK栈 能给我们提供图形以及时间等相关信息，方面我们确定故障所在；</li><li>数据展示：web 界面展示；</li><li>监控报警：电话报警、邮件报警、微信报警、短信报警、报警升级机制等（无论什么报警都可以）；</li><li>报警处理：当接收到报警，我们需要根据故障的级别进行处理，比如:重要紧急、重要不紧急等。根据故障的级别，配合相关的人员进行快速处理；</li></ul><h3 id="监控告警"><a href="#监控告警" class="headerlink" title="监控告警"></a>监控告警</h3><p>当监控的对象超过了某一阈值或者某一服务出现了异常时，便自动发送邮件、短信或微信给相关人员进行告警。</p><p><strong>建立集中监控平台</strong></p><p>在一体化运维体系中，监控平台贯穿所有环节，它起到了生产系统涉及的软硬件环境实时运行状况的“监”，监控平台事件驱动的特性也为一体化运维体系起到神经网络驱动的作用，进而进行了“控”，另外，监控平台优质的运维数据可以作为运维大数据分析的数据源，实现运维数据采集的角色。为了提高投入效率，减少重复投入，需要建立集中监控平台实现统一展示、统一管理，支持两地三中心建设，具备灵活的扩展性，支持运维大数据分析。</p><p><strong>指标权重与阀值分级</strong></p><p>需要重点强调一下监控指标的指标权重、阀值分级与上升机制问题，做监控的人知道“监”的最重要目标是不漏报，为了不漏报在实际实施过程中会出现监控告警过多的困难。如何让运维人员在不漏处理监控事件，又能快速解决风险最高的事件，则需要监控的指标需要进行指标权重、阀值分级与上升机制：</p><p>1）指标权重：</p><p>监控指标的权重是为了定义此项监控指标是否为必须配置，比如应用软件服务、端口监听是一个应用可用性的重要指标，权重定义为一级指标；对于批量状态，则由于不少应用系统并没有批量状态，则定义为二级指标。通常来说一级指标将作为监控覆盖面的底线，通过设置好权重，一是为了让运维人员知道哪些监控指标必须确保覆盖，同时加以引入KPI考核；二是为了让监控平台建设人员有侧重的优化，实现一级指标的自动配置，无需运维人员手工配置。</p><p>2）阀值分级与上升机制：</p><p>有监控指标，就需要针对监控指标定义阀值，监控阀值的设立需要有分级机制，以分通知、预警、告警三级为例：通知需要运维人员关注，比如“交易系统登录数2000，登录成功率95%，平时登录数基线500，登录成功率96%”，由于登录成功率并未明显下降，可能是由于业务作了业务推广，运维人员只需关注当前应用运行状态再做判断；预警代表监控事件需要运维人员处理，但重要性略低，比如“CPU使用率71%，增长趋势非突增”，管理员受理到这个预警可以先设置为一个维护期，待当天找个时间集中处理；告警则必须马上处理的事件，比如“交易成功率为10%，平时为90%”这类监控事件己反映出交易运行问题。</p><p>对于升级，是指一个预警当长时间未处理时，需要有一个上升机制，转化为告警，以督办运维人员完成监控事件的处理。</p><p><strong>事件分级标准</strong></p><p>前面提到了事件分级的问题，分级是将事件当前紧急程度进行标识显示，事件升级是对于低级的事件当达到一定的程度，比如处理时间过长，则需要进行升级。我们将监控事件等级事件级别分为通知、预警、故障三种：</p><ul><li>通知：指一般的通知信息类事件。</li><li>预警：指已经出现异常，即将要引起生产故障的事件。</li><li>故障：指已经发生问题，并且已经影响到生产流程的事件，如果需要进一步细化故障级别，可以分为一般故障和紧急故障：一般故障不需要紧急处理的故障，紧急故障需要管理员紧急处理的故障。事件细分的粒度需根据各企业团队的管理要求而定。</li></ul><p><strong>事件应急响应</strong></p><p>运维最基本的指标就是保证系统的可用性，应急恢复的时效性是系统可用性的关键指标。通常来讲应急恢复的方法有不少，比如：</p><ul><li>服务整体性能下降或异常，可以考虑重启服务；</li><li>应用做过变更，可以考虑是否需要回切变更；</li><li>资源不足，可以考虑应急扩容；</li><li>应用性能问题，可以考虑调整应用参数、日志参数；</li><li>数据库繁忙，可以考虑通过数据库快照分析，优化SQL；</li><li>应用功能设计有误，可以考虑紧急关闭功能菜单；</li><li>还有很多……</li></ul><h3 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h3><p>上面，我们了解到了监控目标、监控手段、监控告警、监控方法和流程之后，我们也更需要知道监控的核心是什么。即<br>1）发现问题：当系统发生故障报警，我们会收到故障报警的信息 ；<br>2）定位问题：故障邮件一般都会写某某主机故障、具体故障的内容，我们需要对报警内容进行分析，比如一台服务器连不上：我们就需要考虑是网络问题、还是负载太高导致长时间无法连接，又或者某开发触发了防火墙禁止的相关策略等等，我们就需要去分析故障具体原因；<br>3）解决问题：当然我们了解到故障的原因后，就需要通过故障解决的优先级去解决该故障；<br>4）总结问题：当我们解决完重大故障后，需要对故障原因以及防范进行总结归纳，避免以后重复出现；</p><p><strong>最后</strong></p><p>关于，如何具体的使用EFK栈和TPG栈监控和采集OpenStack云平台的Log日志和性能数据实现一体化的运维监控告警，将在后面进行专题分享。</p><p>参考链接：<br><a href="http://www.yunweipai.com/archives/13243.html" target="_blank" rel="noopener">http://www.yunweipai.com/archives/13243.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;想一想，从事OpenStack杂七杂八的事儿，至今正好三年半了。做过QA测试（手动的、自动的）、CI（gerrit、jenkins、gitl
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
  </entry>
  
  <entry>
    <title>如何获取Kolla的OpenStack镜像</title>
    <link href="http://yoursite.com/2018/05/07/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96Kolla%E7%9A%84OpenStack%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2018/05/07/如何获取Kolla的OpenStack镜像/</id>
    <published>2018-05-07T15:16:22.000Z</published>
    <updated>2018-06-14T14:25:56.334Z</updated>
    
    <content type="html"><![CDATA[<p>由于，OpenStack社区自Queens版本起，便不再提供将打包好的kolla openstack镜像放在<br><a href="http://tarballs.openstack.org/kolla/images/" target="_blank" rel="noopener">该链接上</a></p><p>所以，我们要获取Kolla的OpenStack镜像，就只能依靠自己手动获取。有如下几种方法。</p><h2 id="从官方源下载镜像"><a href="#从官方源下载镜像" class="headerlink" title="从官方源下载镜像"></a>从官方源下载镜像</h2><p>安装kolla-ansible<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install kolla-ansible/</span></span><br></pre></td></tr></table></figure></p><p>编辑globals.yml文件，设置相关参数<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># vim /etc/kolla/globals.yml</span></span><br><span class="line"><span class="symbol">kolla_install_type:</span> <span class="string">"source"</span></span><br><span class="line"><span class="symbol">kolla_base_distro:</span> <span class="string">"centos"</span></span><br><span class="line"><span class="symbol">openstack_release:</span> <span class="string">"queens"</span></span><br></pre></td></tr></table></figure></p><p>下载镜像<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible -i all-in-one bootstrap-servers</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible pull</span></span><br></pre></td></tr></table></figure></p><h2 id="手动构建镜像"><a href="#手动构建镜像" class="headerlink" title="手动构建镜像"></a>手动构建镜像</h2><p>下载kolla项目<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla.git -b stable/queens</span></span><br></pre></td></tr></table></figure></p><p>生成kolla-build.conf文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pip install tox</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> kolla/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tox -e genconfig</span></span><br></pre></td></tr></table></figure></p><p>构建openstack镜像<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//构建基于centos系统的source源码安装的openstack镜像</span><br><span class="line"># kolla-<span class="keyword">build </span>-t source -<span class="keyword">b </span>centos</span><br><span class="line"></span><br><span class="line">//或者，构建基于centos系统的<span class="keyword">binary二进制包安装的openstack镜像</span></span><br><span class="line"><span class="keyword"># </span>kolla-<span class="keyword">build </span>-t <span class="keyword">binary </span>-<span class="keyword">b </span>centos</span><br></pre></td></tr></table></figure></p><h2 id="自动化拉取kolla镜像"><a href="#自动化拉取kolla镜像" class="headerlink" title="自动化拉取kolla镜像"></a>自动化拉取kolla镜像</h2><p>由于OpenStack社区，已经开始正式将kolla镜像托管在DockerHub上。所以，我们还可以从Docker Hub上直接拉取kolla镜像，由于openstack镜像少则几十，多则上百，因此，这里我编写了一个bash脚本，用于自动化拉取queens版本的kolla镜像。</p><p>为了加快从docker hub上拉取镜像，这里，配置上阿里云的镜像加速器。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># cat /etc/docker/daemon.json </span></span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"registry-mirrors"</span>: [<span class="string">"https://a5aghnme.mirror.aliyuncs.com"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>重启docker服务，生效<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload &amp;&amp; systemctl restart docker</span></span><br></pre></td></tr></table></figure></p><p><strong>拉取queens版本的kolla镜像</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 01_pull_kolla.sh </span></span><br><span class="line"><span class="meta">#!/usr/bin/bash</span></span><br><span class="line"></span><br><span class="line">image_tag=queens    <span class="comment">#该变量，你可以根据自己需要进行修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete images</span></span><br><span class="line">docker images | awk <span class="string">'&#123;print $3&#125;'</span> | xargs docker rmi -f</span><br><span class="line"></span><br><span class="line"><span class="comment"># pull public images</span></span><br><span class="line"><span class="keyword">for</span> public_images <span class="keyword">in</span> memcached kolla-toolbox cron mongodb mariadb rabbitmq keepalived haproxy chrony iscsid tgtd</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$public_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull monitor manage images</span></span><br><span class="line"><span class="comment"># it is recommended to use telegraf + influxdb + grafana + collectd + Prometheus</span></span><br><span class="line"><span class="keyword">for</span> monitor_images <span class="keyword">in</span> collectd telegraf grafana influxdb prometheus-server prometheus-haproxy-exporter prometheus-node-exporter prometheus-mysqld-exporter prometheus-memcached-exporter prometheus-cadvisor</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$monitor_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull log manage images</span></span><br><span class="line"><span class="keyword">for</span> log_images <span class="keyword">in</span> fluentd elasticsearch kibana</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$log_images</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull nova</span></span><br><span class="line"><span class="keyword">for</span> nova <span class="keyword">in</span> nova-compute nova-consoleauth nova-ssh nova-placement-api nova-api nova-compute-ironic nova-consoleauth nova-serialproxy nova-scheduler nova-novncproxy nova-conductor nova-libvirt</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$nova</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull keystone</span></span><br><span class="line"><span class="keyword">for</span> keystone <span class="keyword">in</span> keystone keystone-fernet keystone-ssh</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$keystone</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull freezer</span></span><br><span class="line">docker pull kolla/centos-source-freezer-api:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull glance </span></span><br><span class="line"><span class="keyword">for</span> glance <span class="keyword">in</span> glance-api glance-registry</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$glance</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull cinder</span></span><br><span class="line"><span class="keyword">for</span> cinder <span class="keyword">in</span> cinder-volume cinder-api cinder-backup cinder-scheduler</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$cinder</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull neutron</span></span><br><span class="line"><span class="keyword">for</span> neutron <span class="keyword">in</span> neutron-server neutron-lbaas-agent neutron-dhcp-agent neutron-l3-agent neutron-openvswitch-agent neutron-metadata-agent neutron-server-opendaylight opendaylight</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$neutron</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull openvswitch</span></span><br><span class="line"><span class="keyword">for</span> openvswitch <span class="keyword">in</span> openvswitch-db-server openvswitch-vswitchd neutron-openvswitch-agent</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$openvswitch</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull ceilometer</span></span><br><span class="line"><span class="keyword">for</span> ceilometer <span class="keyword">in</span> ceilometer-api ceilometer-compute ceilometer-notification ceilometer-central</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$ceilometer</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull gnocchi</span></span><br><span class="line"><span class="keyword">for</span> gnocchi <span class="keyword">in</span> gnocchi-metricd gnocchi-api gnocchi-statsd</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$gnocchi</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull aodh</span></span><br><span class="line"><span class="keyword">for</span> aodh <span class="keyword">in</span> aodh-evaluator aodh-api aodh-listener aodh-notifier</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$aodh</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull heat</span></span><br><span class="line"><span class="keyword">for</span> heat <span class="keyword">in</span> heat-api heat-api-cfn heat-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$heat</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull horizon</span></span><br><span class="line">docker pull kolla/centos-source-horizon:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull murano</span></span><br><span class="line"><span class="keyword">for</span> murano <span class="keyword">in</span> murano-api murano-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$murano</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull magnum</span></span><br><span class="line"><span class="keyword">for</span> magnum <span class="keyword">in</span> magnum-api magnum-conductor</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$magnum</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull senlin</span></span><br><span class="line"><span class="keyword">for</span> senlin <span class="keyword">in</span> senlin-api senlin-engine</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$senlin</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull sahara</span></span><br><span class="line"><span class="keyword">for</span> sahara <span class="keyword">in</span> sahara-engine sahara-api</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$sahara</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull trove</span></span><br><span class="line"><span class="keyword">for</span> trove <span class="keyword">in</span> trove-api trove-taskmanager trove-conductor</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$trove</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull swift</span></span><br><span class="line"><span class="keyword">for</span> swift <span class="keyword">in</span> swift-rsyncd swift-proxy-server swift-object-expirer swift-object swift-account swift-container swift-base</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$swift</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull ironic</span></span><br><span class="line"><span class="keyword">for</span> ironic <span class="keyword">in</span> ironic-conductor ironic-pxe ironic-api ironic-inspector</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$ironic</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">docker pull kolla/centos-source-dnsmasq:pike</span><br><span class="line">docker tag kolla/centos-source-dnsmasq:pike kolla/centos-source-dnsmasq:queens</span><br><span class="line">docker rmi -f kolla/centos-source-dnsmasq:pike</span><br><span class="line"></span><br><span class="line"><span class="comment"># pull cloudkitty</span></span><br><span class="line"><span class="keyword">for</span> cloudkitty <span class="keyword">in</span> cloudkitty-api cloudkitty-processor panko-api</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker pull kolla/centos-source-<span class="variable">$cloudkitty</span>:<span class="variable">$image_tag</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pull kuryr</span></span><br><span class="line">docker pull kolla/centos-source-kuryr-libnetwork:<span class="variable">$image_tag</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># save images</span></span><br><span class="line">images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line">docker save -o kolla_queens_images.tar <span class="variable">$images</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clean pull's images</span></span><br><span class="line">docker images | awk <span class="string">'&#123;print $3&#125;'</span> | xargs docker rmi -f</span><br></pre></td></tr></table></figure></p><p><strong>将镜像push到本地Registry</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 02_push_kolla.sh </span></span><br><span class="line"><span class="meta">#!/usr/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load images</span></span><br><span class="line">docker load --input kolla_queens_images.tar</span><br><span class="line"></span><br><span class="line">registry=172.17.51.27:4000    <span class="comment">#请将该registry地址，改为你自己环境的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tag images</span></span><br><span class="line">images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> images_tag <span class="keyword">in</span> <span class="variable">$images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker tag <span class="variable">$images_tag</span>:queens <span class="variable">$registry</span>/<span class="variable">$images_tag</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete old's images</span></span><br><span class="line">delete_images=`docker images | grep <span class="string">'^kolla'</span> | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> delete_images1 <span class="keyword">in</span> <span class="variable">$delete_images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker rmi <span class="variable">$delete_images1</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># push images</span></span><br><span class="line">push_images=`docker images | grep queens | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> push_images1 <span class="keyword">in</span> <span class="variable">$push_images</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  docker push <span class="variable">$push_images1</span>:queens</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>综合，比较以上三种方法的优缺点。这里，推荐使用第三种方法，速度更快，也更便捷。<br>PS：脚本写得有点搓，不喜可喷。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于，OpenStack社区自Queens版本起，便不再提供将打包好的kolla openstack镜像放在&lt;br&gt;&lt;a href=&quot;http://tarballs.openstack.org/kolla/images/&quot; target=&quot;_blank&quot; rel=&quot;noop
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/categories/OpenStack/Kolla/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>如何设置OpenStack节点Swap分区</title>
    <link href="http://yoursite.com/2018/05/07/%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEOpenStack%E8%8A%82%E7%82%B9Swap%E5%88%86%E5%8C%BA/"/>
    <id>http://yoursite.com/2018/05/07/如何设置OpenStack节点Swap分区/</id>
    <published>2018-05-07T13:02:51.000Z</published>
    <updated>2018-05-07T14:09:46.724Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Swap分区介绍"><a href="#Swap分区介绍" class="headerlink" title="Swap分区介绍"></a>Swap分区介绍</h2><p>Swap分区，即交换分区。它的功能就是在物理内存不够的情况下，操作系统先把内存中暂时不用的数据，存到硬盘的交换空间，腾出内存来让别的程序运行，当程序需要用到交换空间内的数据的时候，操作系统再将数据从交换分区恢复到物理内存中。这样，系统总是在物理内存不够时，才进行Swap交换。                </p><h2 id="如何设置Swap分区大小"><a href="#如何设置Swap分区大小" class="headerlink" title="如何设置Swap分区大小"></a>如何设置Swap分区大小</h2><p>以上是SWAP 交换分区的作用。 实际上，我们更关注的应该是SWAP分区的大小问题。 设置多大才是最优的。如下，提供了两种方案。</p><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><p>在Linux系统中，我们可以参照Red Hat公司为RHEL 7推荐的SWAP空间的大小划分原则，在你没有其他特别需求时，可以作为很好的参考依据。</p><ul><li>内存小于2GB，推荐2倍于内存的swap空间；</li><li>内存2GB~8GB，推荐和内存大小一样的swap空间；</li><li>内存8GB~64GB，推荐至少4GB的swap空间；</li><li>内存大于64GB，推荐至少4GB的swap空间。</li></ul><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-swapspace" target="_blank" rel="noopener">原文链接</a></p><p>实际上，系统中交换分区的大小并不取决于物理内存的量，而是取决于系统中内存的负荷，所以在安装系统时要根据具体的业务来设置SWAP的值。</p><p>在OpenStack中，默认的CPU超配比例是1:16，内存超配比例是1:1.5。当宿主机使用swap交换分区来为虚拟机分配内存的时候，则虚拟机的性能将急速下降。生产环境上不建议开启内存超售（建议配置比例1:1）。另外，建议设置nova.conf文件中的reserved_host_memory_mb 参数，即内存预留量（建议至少预留4GB），保证该部分内存不能被虚拟机使用。</p><h3 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h3><p>系统在什么情况下才会使用Swap？实际上，并不是等所有的物理内存都消耗完毕之后，才去使用swap的空间，什么时候使用是由swappiness 参数值控制的。<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat <span class="regexp">/proc/</span>sys<span class="regexp">/vm/</span>swappiness</span><br><span class="line"><span class="number">60</span></span><br></pre></td></tr></table></figure></p><p>该值默认值是60。</p><ul><li>swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间。</li><li>swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。</li></ul><p>由于，现在服务器的内存一般是上百GB，所以我们可以把这个参数值设置的低一些（如10-30之间），让操作系统尽可能的使用物理内存，降低系统对swap的使用，从而提高宿主机系统和虚拟机的性能。</p><p>永久性修改<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'vm.swappiness=10'</span> &gt;&gt;/etc/sysctl.conf</span></span><br></pre></td></tr></table></figure></p><p>保存，重启就生效了。</p><p>查看系统当前SWAP 空间大小<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> free –h</span></span><br></pre></td></tr></table></figure></p><p><strong>小结</strong></p><p>为了保证主机系统和应用程序的稳定运行（内存不足或泄露，易导致系统或应用崩溃），建议在实际使用过程中，服务器仍然需要创建一定的Swap分区。其分区大小可以结合以上两种方案进行设置，已达到最佳效果。</p><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><p><strong>1.释放SWAP 空间</strong></p><p>假设我们的系统出现了性能问题，我们通过vmstat命令看到有大量的swap，而我们的物理内存又很充足，那么我们可以手工把swap 空间释放出来。让进程去使用物理内存，从而提高性能。</p><p>我们对swap 空间的释放，可以通过关闭swap分区，再启动swap 分区来实现。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vmstat 1 5      // 1表示每隔1秒采集一次服务器状态，5表示只采集5次</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> free -h</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> swapon -s      //显示交换分区的使用状况</span></span><br></pre></td></tr></table></figure></p><p>关闭swap 交换分区：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapoff /dev/sda2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> swapon -s</span></span><br></pre></td></tr></table></figure></p><p>启用swap分区：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapon /dev/sda2</span></span><br></pre></td></tr></table></figure></p><p>验证状态：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapon -s</span></span><br></pre></td></tr></table></figure></p><p>Swap分区的拓展和缩小：<a href="https://www.e-learn.cn/content/linux/339010" target="_blank" rel="noopener">https://www.e-learn.cn/content/linux/339010</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Swap分区介绍&quot;&gt;&lt;a href=&quot;#Swap分区介绍&quot; class=&quot;headerlink&quot; title=&quot;Swap分区介绍&quot;&gt;&lt;/a&gt;Swap分区介绍&lt;/h2&gt;&lt;p&gt;Swap分区，即交换分区。它的功能就是在物理内存不够的情况下，操作系统先把内存中暂时不用的数
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Kolla中配置OpenStack虚机网络vxlan和vlan共存</title>
    <link href="http://yoursite.com/2018/05/03/Kolla%E4%B8%AD%E9%85%8D%E7%BD%AEOpenStack%E8%99%9A%E6%9C%BA%E7%BD%91%E7%BB%9Cvxlan%E5%92%8Cvlan%E5%85%B1%E5%AD%98/"/>
    <id>http://yoursite.com/2018/05/03/Kolla中配置OpenStack虚机网络vxlan和vlan共存/</id>
    <published>2018-05-03T12:59:32.000Z</published>
    <updated>2018-05-07T12:55:48.347Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack Neutron网络服务定义了四种网络模式：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = <span class="built_in">local</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = vlan   </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = gre</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tenant_network_type = vxlan</span></span><br></pre></td></tr></table></figure></p><p>这里，本文以vlan、vxlan为例，阐述如何实现OpenStack虚机网络（亦称租户网络、业务网络）同时支持vxlan和vlan两种网络。</p><p><strong>说明</strong></p><ul><li>环境：Openstack queens版本</li><li>部署工具：kolla-ansible</li></ul><p>在kolla-ansible部署节点的/etc/kolla/globals.yml文件中，配置网卡。如下所示。</p><p><img src="/images/globals.png" alt="image"></p><ul><li>eth0：openstack管理网络；vlan 51，交换机端口设置为Access模式</li><li>eth1：虚机网络(vxlan)；vlan 52，交换机端口设置为Access模式</li><li>eth2：外部网络兼虚机网络(vlan)；vlan网段53-54，交换机端口设置为trunk模式，主机不配置IP地址</li></ul><h2 id="在所有网络节点上，操作如下"><a href="#在所有网络节点上，操作如下" class="headerlink" title="在所有网络节点上，操作如下"></a>在所有网络节点上，操作如下</h2><p>修改文件/etc/kolla/neutron-server/ml2_conf.ini</p><p><img src="/images/network1.png" alt="image"> </p><p>修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini</p><p><img src="/images/network2.png" alt="image"> </p><p>重启neutron容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker restart neutron_server neutron_openvswitch_agent</span></span><br></pre></td></tr></table></figure></p><p>在网络节点上，查看br-ex网桥设置情况，如下。</p><p><img src="/images/networkbrex.png" alt="image"> </p><h2 id="在所有计算节点上，操作如下"><a href="#在所有计算节点上，操作如下" class="headerlink" title="在所有计算节点上，操作如下"></a>在所有计算节点上，操作如下</h2><p>修改文件/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini</p><p><img src="/images/compute.png" alt="image"></p><p>创建一个br-ex外部网桥，并关联到主机的eth2物理网卡上。这样，当计算节点上的虚拟机使用vlan网络时，便可以直接通过qbr-&gt;br-int-&gt;br-ex-&gt;eth2连接到外网。（vlan网络的三层路由，建议使用物理路由器，这样性能和稳定性更好，而不需要通过网络节点上的L3 vRouter虚拟路由）。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># docker exec -<span class="keyword">u</span> root -it neutron_openvswitch_agent ovs-vsctl <span class="built_in">add</span>-<span class="keyword">br</span> <span class="keyword">br</span>-<span class="keyword">ex</span></span><br><span class="line"># docker exec -<span class="keyword">u</span> root -it neutron_openvswitch_agent ovs-vsctl <span class="built_in">add</span>-port <span class="keyword">br</span>-<span class="keyword">ex</span> eth2</span><br></pre></td></tr></table></figure></p><p>最后，重启相关容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker restart neutron_openvswitch_agent</span></span><br></pre></td></tr></table></figure></p><p>在计算节点上，查看br-ex网桥设置情况，如下。</p><p><img src="/images/computebrex.png" alt="image"></p><p>创建一个vlan id为53的网段<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span> <span class="comment">neutron</span> <span class="comment">net</span><span class="literal">-</span><span class="comment">create</span> <span class="comment">vlan</span><span class="literal">-</span><span class="comment">53</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">shared</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:physical_network</span> <span class="comment">physnet1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:network_type</span> <span class="comment">vlan</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">provider:segmentation_id</span> <span class="comment">53</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#</span> <span class="comment">neutron</span> <span class="comment">subnet</span><span class="literal">-</span><span class="comment">create</span> <span class="comment">vlan</span><span class="literal">-</span><span class="comment">53</span> <span class="comment">172</span><span class="string">.</span><span class="comment">17</span><span class="string">.</span><span class="comment">53</span><span class="string">.</span><span class="comment">0/24</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">name</span> <span class="comment">provider</span><span class="literal">-</span><span class="comment">53</span><span class="literal">-</span><span class="comment">subnet</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gateway</span> <span class="comment">172</span><span class="string">.</span><span class="comment">17</span><span class="string">.</span><span class="comment">53</span><span class="string">.</span><span class="comment">1</span></span><br></pre></td></tr></table></figure></p><p>查看创建的网络，如下。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># neutron net-<span class="type">list</span></span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br><span class="line">| id                                   | name           | tenant_id                        | subnets                                             |</span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br><span class="line">| <span class="number">5</span>d9c4874-e03b<span class="number">-4</span>bde-aee0<span class="number">-947</span>d7dde4860 | vlan<span class="number">-53</span>        | <span class="number">48</span>fbadff0ab84229b429166babbe488f | <span class="number">9</span>bade37c-ff44<span class="number">-4004</span><span class="number">-8e82</span><span class="number">-20</span>d61348fdc0 <span class="number">172.17</span><span class="number">.53</span><span class="number">.0</span>/<span class="number">24</span> |</span><br><span class="line">| <span class="number">7</span>b0152da-a975<span class="number">-4</span>dbf-b35b<span class="number">-437951</span>c66efa | tenant_network | <span class="number">48</span>fbadff0ab84229b429166babbe488f | a45516a4<span class="number">-4</span>ce9<span class="number">-4</span>c2e<span class="number">-8052</span><span class="number">-8</span>c71eae0e219 <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span>    |</span><br><span class="line">| <span class="number">9630</span>cf8b<span class="number">-4072</span><span class="number">-415</span>b-a9a9<span class="number">-99</span>ff815748f8 | public_network | <span class="number">48</span>fbadff0ab84229b429166babbe488f | a98f8c80<span class="number">-78</span>de<span class="number">-43</span>ba-af52-d86c19fc59ef <span class="number">172.17</span><span class="number">.54</span><span class="number">.0</span>/<span class="number">24</span> |</span><br><span class="line">+--------------------------------------+----------------+----------------------------------+-----------------------------------------------------+</span><br></pre></td></tr></table></figure></p><p>最后，创建一个虚拟机并使用该vlan网络。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># nova boot --flavor <span class="number">1</span>Gmem_1cpu --image centos7 --nic net-id=<span class="number">5</span>d9c4874-e03b<span class="number">-4</span>bde-aee0<span class="number">-947</span>d7dde4860 test_vm</span><br><span class="line"></span><br><span class="line"># nova <span class="type">list</span> | grep test_vm</span><br><span class="line">| f506129b<span class="number">-610</span>f<span class="number">-4e2</span>d<span class="number">-886</span>b<span class="number">-5</span>d791cdcb282 | test_vm | <span class="literal">ACTIVE</span> | -  | Running | vlan<span class="number">-53</span>=<span class="number">172.17</span><span class="number">.53</span><span class="number">.7</span></span><br></pre></td></tr></table></figure></p><p>测试虚拟机网络通信<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -c 4 172.17.53.7</span></span><br><span class="line">PING 172.17.53.7 (172.17.53.7) 56(84) bytes of data.</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=1 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.421 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=2 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.503 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=3 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.543 ms</span><br><span class="line">64 bytes <span class="keyword">from</span> 172.17.53.7: <span class="attribute">icmp_seq</span>=4 <span class="attribute">ttl</span>=63 <span class="attribute">time</span>=0.469 ms</span><br></pre></td></tr></table></figure></p><p><strong>br-int和br-ex说明</strong></p><ul><li>br-int</li></ul><p>br-int是OpenVswitch中的集成网桥，类似于一个二层的交换机。上面挂载了大量的agent来提供各种网络服务，另外负责对发往br-ex的流量，实现local vlan转化为外部vlan。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ovs-ofctl dump-flows br-int  </span></span><br><span class="line">NXST_FLOW reply (<span class="attribute">xid</span>=0x4):  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=147294.121s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=224, <span class="attribute">n_bytes</span>=33961, <span class="attribute">idle_age</span>=13, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=3,in_port=4,dl_vlan=1 <span class="attribute">actions</span>=mod_vlan_vid:101,NORMAL  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=603538.84s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=19, <span class="attribute">n_bytes</span>=2234, <span class="attribute">idle_age</span>=18963, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=2,in_port=4 <span class="attribute">actions</span>=drop  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=603547.134s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=31901, <span class="attribute">n_bytes</span>=6419756, <span class="attribute">idle_age</span>=13, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=1 <span class="attribute">actions</span>=NORMAL</span><br></pre></td></tr></table></figure></p><ul><li>br-ex</li></ul><p>br-ex是OpenVswitch中的一个外部网桥，要做的事情很简单，只需要正常转发数据流量即可。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ovs-ofctl dump-flows br-ex  </span></span><br><span class="line">NXST_FLOW reply (<span class="attribute">xid</span>=0x4):  </span><br><span class="line"> <span class="attribute">cookie</span>=0x0, <span class="attribute">duration</span>=6770.969s, <span class="attribute">table</span>=0, <span class="attribute">n_packets</span>=5411, <span class="attribute">n_bytes</span>=306944, <span class="attribute">idle_age</span>=0, <span class="attribute">hard_age</span>=65534, <span class="attribute">priority</span>=0 <span class="attribute">actions</span>=NORMAL</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;OpenStack Neutron网络服务定义了四种网络模式：&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;spa
      
    
    </summary>
    
      <category term="OpenStack" scheme="http://yoursite.com/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
  </entry>
  
  <entry>
    <title>如何删除Registry中kolla-ansible的镜像</title>
    <link href="http://yoursite.com/2018/04/30/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4Registry%E4%B8%ADkolla-ansible%E7%9A%84%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2018/04/30/如何删除Registry中kolla-ansible的镜像/</id>
    <published>2018-04-30T15:16:19.000Z</published>
    <updated>2018-05-03T13:16:53.453Z</updated>
    
    <content type="html"><![CDATA[<p>出于某些情况，如释放磁盘空间、旧镜像删除等原因，需要我们删除本地Registry仓库中的镜像。本篇文章，将讲解如何在OpenStack环境的kolla-ansible中，删除本地Registry中的镜像。</p><h2 id="Registry中的镜像管理"><a href="#Registry中的镜像管理" class="headerlink" title="Registry中的镜像管理"></a>Registry中的镜像管理</h2><p>查看Registry仓库中现有的镜像：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> -XGET http:<span class="comment">//172.17.51.51:4000/v2/_catalog</span></span><br></pre></td></tr></table></figure></p><p>查看Registry仓库中指定的镜像，如这里的centos-source-magnum-conductor。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> -XGET http:<span class="comment">//172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/tags/list</span></span><br></pre></td></tr></table></figure></p><h2 id="如何删除私有-registry-中的镜像"><a href="#如何删除私有-registry-中的镜像" class="headerlink" title="如何删除私有 registry 中的镜像"></a>如何删除私有 registry 中的镜像</h2><p>首先，在默认情况下，docker registry 是不允许删除镜像的，需要在配置文件config.yml中启用。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#vim /etc/docker/registry/config.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">log:</span></span><br><span class="line"><span class="attr">  fields:</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line"><span class="attr">  cache:</span></span><br><span class="line"><span class="attr">    blobdescriptor:</span> <span class="string">inmemory</span></span><br><span class="line"><span class="attr">  filesystem:</span></span><br><span class="line"><span class="attr">    rootdirectory:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">  delete:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="attr">  addr:</span> <span class="string">:5000</span></span><br><span class="line"><span class="attr">  headers:</span></span><br><span class="line"><span class="attr">    X-Content-Type-Options:</span> <span class="string">[nosniff]</span></span><br><span class="line"><span class="attr">health:</span></span><br><span class="line"><span class="attr">  storagedriver:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    interval:</span> <span class="number">10</span><span class="string">s</span></span><br><span class="line"><span class="attr">    threshold:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>修改后，需要重启registry容器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">docker restart registry</span></span><br></pre></td></tr></table></figure></p><p>使用API接口 GET /v2/&lt;镜像名&gt;/manifests/<tag> 来取得要删除的镜像:Tag所对应的 digest。比如，要删除kolla/centos-source-magnum-conductor:queens镜像，那么取得 digest 的命令是：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#curl</span> --<span class="selector-tag">header</span> <span class="string">"Accept: application/vnd.docker.distribution.manifest.v2+json"</span> -I -X HEAD http:<span class="comment">//172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/queens</span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">Content-Length: <span class="number">8666</span></span><br><span class="line">Content-Type: application/vnd<span class="selector-class">.docker</span><span class="selector-class">.distribution</span><span class="selector-class">.manifest</span><span class="selector-class">.v2</span>+json</span><br><span class="line">Docker-Content-Digest: sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868</span><br><span class="line">Docker-Distribution-Api-Version: registry/<span class="number">2.0</span></span><br><span class="line">Etag: <span class="string">"sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868"</span></span><br><span class="line">X-Content-Type-Options: nosniff</span><br><span class="line">Date: Sat, <span class="number">28</span> Apr <span class="number">2018</span> <span class="number">02</span>:<span class="number">44</span>:<span class="number">46</span> GMT</span><br></pre></td></tr></table></figure></tag></p><p>得到 Docker-Content-Digest:<br><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sha<span class="number">256</span>:e<span class="number">94</span><span class="keyword">c</span><span class="number">4</span>d<span class="number">08520</span>a<span class="number">7</span>f<span class="number">77</span>cbfa<span class="number">0</span><span class="keyword">c</span><span class="number">2</span>d<span class="number">314</span>bc<span class="number">9281</span>d<span class="number">07874</span>b<span class="number">8</span><span class="keyword">c</span><span class="number">7</span>d<span class="number">9337</span>ad<span class="number">5</span>f<span class="number">541832</span>f<span class="number">7</span>d<span class="number">868</span></span><br></pre></td></tr></table></figure></p><p>然后调用API接口 DELETE /v2/&lt;镜像名&gt;/manifests/<digest> 来删除镜像。比如：<br><figure class="highlight ldif"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#curl -I -X DELETE http://172.17.51.51:4000/v2/kolla/centos-source-magnum-conductor/manifests/sha256:e94c4d08520a7f77cbfa0c2d314bc9281d07874b8c7d9337ad5f541832f7d868</span></span><br><span class="line"><span class="attribute">HTTP/1.1 202 Accepted</span></span><br><span class="line"><span class="attribute">Docker-Distribution-Api-Version</span>: registry/2.0</span><br><span class="line"><span class="attribute">X-Content-Type-Options</span>: nosniff</span><br><span class="line"><span class="attribute">Date</span>: Sat, 28 Apr 2018 03:34:31 GMT</span><br><span class="line"><span class="attribute">Content-Length</span>: 0</span><br><span class="line"><span class="attribute">Content-Type</span>: text/plain; charset=utf-8</span><br></pre></td></tr></table></figure></digest></p><p>至此，镜像已从 registry 中标记删除，外界访问 pull 不到了。但是 registry 的本地空间并未释放，需要垃圾收集才会释放。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">docker <span class="built_in">exec</span> registry bin/registry garbage-collect /etc/docker/registry/config.yml</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;出于某些情况，如释放磁盘空间、旧镜像删除等原因，需要我们删除本地Registry仓库中的镜像。本篇文章，将讲解如何在OpenStack环境的kolla-ansible中，删除本地Registry中的镜像。&lt;/p&gt;
&lt;h2 id=&quot;Registry中的镜像管理&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Kolla" scheme="http://yoursite.com/tags/Kolla/"/>
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
</feed>
